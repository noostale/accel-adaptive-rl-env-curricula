{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACCEL IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DANGER: ACCEL does not use a teacher, but a single student that learns from levels selected by a curator, in this impmenetation the curator is a sampler that gives te studemt with a probabilistic manner, the highest regret-based score level, but instaed of discarding the levels during training, it makes small edits to the most diffucult ones, gibing the change to rely on already good but hard leves without the need of a teacher.\n",
    "# TODO: the start_pos and goal_pos of edited levels are uncorrectly marked, if I mutate the level no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "from gymnasium.spaces import Box\n",
    "\n",
    "from minigrid.core.mission import MissionSpace\n",
    "from minigrid.core.world_object import Goal, Wall\n",
    "from minigrid.minigrid_env import MiniGridEnv, Grid\n",
    "\n",
    "from minigrid.wrappers import ImgObsWrapper\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# ====================================================\n",
    "# 1. Custom MiniGrid Environment that returns only the image\n",
    "#    for SB3's PPO (which expects a Box space).\n",
    "# ====================================================\n",
    "class MyCustomGrid(MiniGridEnv):\n",
    "    \"\"\"\n",
    "    Simple MiniGrid environment that places random wall tiles\n",
    "    according to a config dict, returning only the 'image' observation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config=None, **kwargs):\n",
    "        if config is None:\n",
    "            config = {}\n",
    "        self.config = config\n",
    "\n",
    "        # Extract parameters from config\n",
    "        self.width = config.get(\"width\")\n",
    "        self.height = config.get(\"height\")\n",
    "        self.num_blocks = config.get(\"num_blocks\")\n",
    "        self.custom_seed = config.get(\"seed_val\")\n",
    "        \n",
    "        \n",
    "        # Create a random number generator with the custom seed\n",
    "        self.rng = np.random.default_rng(seed=self.custom_seed)\n",
    "\n",
    "        grid_size = max(self.width, self.height)\n",
    "\n",
    "        mission_space = MissionSpace(mission_func=lambda: \"get to the green goal square\")\n",
    "\n",
    "        super().__init__(\n",
    "            grid_size=grid_size,\n",
    "            max_steps=self.width * self.height * 2, # max_steps is typically 2x the grid size\n",
    "            see_through_walls=False,\n",
    "            agent_view_size=5,                      # Size of the agent's view square\n",
    "            mission_space=mission_space,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # Manually define our observation_space as a single Box (the image).\n",
    "        # By default, MiniGrid's image shape is (view_size, view_size, 3) if using partial obs,\n",
    "        # or (height, width, 3) if using full-grid observation. We'll do full-grid here:\n",
    "        # We'll define (self.height, self.width, 3) as the shape.\n",
    "        # In practice, \"image\" shape can vary if partial observations are used.\n",
    "        self.observation_space = Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=(self.agent_view_size, self.agent_view_size, 3),\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "\n",
    "    def _gen_grid(self, width, height):\n",
    "        \"\"\"\n",
    "        Generate the grid layout for a new episode.\n",
    "        We use self.width and self.height from config, even though the underlying\n",
    "        MiniGrid environment might use grid_size for some of its operations.\n",
    "        \"\"\"    \n",
    "        \n",
    "        # Create an empty grid of the \"true\" width x height from config\n",
    "        self.grid = Grid(self.width, self.height)\n",
    "        # Surround the grid with walls\n",
    "        self.grid.wall_rect(0, 0, self.width, self.height)\n",
    "        \n",
    "        # Place random walls inside using the custom seed. Only place a wall if the cell is empty.\n",
    "        for _ in range(self.num_blocks):\n",
    "            r = self.rng.integers(1, self.height - 1)\n",
    "            c = self.rng.integers(1, self.width - 1)\n",
    "            if self.grid.get(c, r) is None: #and (c, r) != self.config[\"start_pos\"] and (c, r) != self.config[\"goal_pos\"]:\n",
    "                self.put_obj(Wall(), c, r)\n",
    "        \n",
    "        # Place the goal object in a random position not occupied by any wall\n",
    "        \"\"\"if self.config[\"goal_pos\"] is None:\"\"\"\n",
    "        while True:\n",
    "            r = self.rng.integers(1, self.height - 1)\n",
    "            c = self.rng.integers(1, self.width - 1)\n",
    "            if self.grid.get(c, r) is None: # and (c, r) != self.config[\"start_pos\"]:\n",
    "                self.put_obj(Goal(), c, r)\n",
    "                self.config[\"goal_pos\"] = (c, r)\n",
    "                break\n",
    "        \"\"\"elif self.config[\"goal_pos\"] is not None:\n",
    "            c, r = self.config[\"goal_pos\"]\n",
    "            self.put_obj(Goal(), c, r)\"\"\"\n",
    "\n",
    "        # Place the agent in a random position not occupied by any wall and not on the goal\n",
    "        \n",
    "        \"\"\"if self.config[\"start_pos\"] is None:\"\"\"\n",
    "        while True:\n",
    "            r = self.rng.integers(1, self.height - 1)\n",
    "            c = self.rng.integers(1, self.width - 1)\n",
    "            if self.grid.get(c, r) is None: # and (c, r) != self.config[\"goal_pos\"]:\n",
    "                self.place_agent(top=(c, r), rand_dir=True)\n",
    "                self.config[\"start_pos\"] = (c, r)\n",
    "                break  \n",
    "        \"\"\"elif self.config[\"start_pos\"] is not None:\n",
    "            c, r = self.config[\"start_pos\"]\n",
    "            self.place_agent(top=(c, r), rand_dir=True)\"\"\"\n",
    "     \n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Override reset to ensure we only return the 'image' array\n",
    "        instead of a dict with 'image' and 'mission'.\n",
    "        \"\"\"\n",
    "        obs, info = super().reset(**kwargs)\n",
    "        obs = self._convert_obs(obs)\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Same for step: override to convert the dict observation into an image only.\n",
    "        \"\"\"\n",
    "        obs, reward, done, truncated, info = super().step(action)\n",
    "        obs = self._convert_obs(obs)\n",
    "        return obs, reward, done, truncated, info\n",
    "\n",
    "    def _convert_obs(self, original_obs):\n",
    "        \"\"\"\n",
    "        original_obs is typically {'image':..., 'mission':...}.\n",
    "        We'll just return original_obs['image'] to get a Box(low=0,high=255) shape.\n",
    "        \"\"\"\n",
    "        #print(original_obs[\"image\"].shape)\n",
    "        return original_obs[\"image\"]\n",
    "        #return np.transpose(original_obs[\"image\"], (2, 0, 1))\n",
    "\n",
    "\n",
    "\n",
    "def random_config(grid_size, num_blocks=None):\n",
    "    max_blocks = int(((grid_size - 1) * (grid_size - 1)) / 2)\n",
    "    \n",
    "    if num_blocks is None:\n",
    "        num_blocks = np.random.randint(1, max_blocks)\n",
    "    else:\n",
    "        num_blocks = min(num_blocks, max_blocks)\n",
    "    \n",
    "    return {\n",
    "        \"width\": grid_size,\n",
    "        \"height\": grid_size,\n",
    "        \"num_blocks\": num_blocks,\n",
    "        \"start_pos\": None,\n",
    "        \"goal_pos\": None,\n",
    "        \"edited\": False,\n",
    "        \"seed_val\": np.random.randint(0, 999999),\n",
    "    }\n",
    "    \n",
    "# Modify an existing configuration, adding randomness.\n",
    "def edit_config(old_config):\n",
    "    max_blocks = int(((old_config[\"width\"] - 1) * (old_config[\"height\"] - 1)) / 2)\n",
    "    \n",
    "    new_config = dict(old_config)\n",
    "    \n",
    "    # Randomly change the number of blocks\n",
    "    new_number_blocks = old_config[\"num_blocks\"] + np.random.choice([1])\n",
    "    \n",
    "    # Ensure the number of blocks is within bounds\n",
    "    new_config[\"num_blocks\"] = max(1, min(new_number_blocks, max_blocks))    \n",
    "    \n",
    "    # Mark the config as edited\n",
    "    new_config[\"edited\"] = True\n",
    "    \n",
    "    return new_config\n",
    "\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# 2. Simple “level buffer” \n",
    "# ====================================================\n",
    "# class to memorize generated levels and score\n",
    "class LevelBuffer: \n",
    "    def __init__(self, max_size=50):\n",
    "        self.max_size = max_size\n",
    "        self.data = []  # will store (config_dict, score)\n",
    "\n",
    "    def add(self, config, score):\n",
    "        self.data.append((config, score))\n",
    "        if len(self.data) > self.max_size:\n",
    "            self.data.sort(key=lambda x: x[1], reverse=True)\n",
    "            self.data = self.data[: self.max_size]\n",
    "            #it memorize only the highest score for each level\n",
    "\n",
    "    def sample_config(self): \n",
    "        # Samples a level from the buffer, weighting the probabilities \n",
    "        # based on the scores.\n",
    "        if len(self.data) == 0:\n",
    "            return None\n",
    "        scores = [item[1] for item in self.data]\n",
    "        total = sum(scores)\n",
    "        if total <= 1e-9:\n",
    "            # fallback to uniform\n",
    "            idx = np.random.randint(len(self.data))\n",
    "            return self.data[idx][0]\n",
    "        probs = [s / total for s in scores]\n",
    "        idx = np.random.choice(len(self.data), p=probs)\n",
    "        return self.data[idx][0]\n",
    "\n",
    "# ====================================================\n",
    "# 3. Regret Calculation with Generalized Advantage Estimation (GAE)\n",
    "# ====================================================\n",
    "\n",
    "# Calculate regret using Generalized Advantage Estimation (GAE) with Stable-Baselines3's PPO model.\n",
    "# PLR approximates regret using a score function such as the positive value loss.\n",
    "def calculate_regret_gae(env, model, max_steps, gamma, lam):\n",
    "    \"\"\"\n",
    "    Calculate regret using Generalized Advantage Estimation (GAE)\n",
    "    with Stable-Baselines3's PPO model.\n",
    "    \"\"\"\n",
    "    obs, _ = env.reset()\n",
    "    regrets = []\n",
    "    rewards = []\n",
    "    dones = []\n",
    "    values = []\n",
    "\n",
    "    for t in range(max_steps):\n",
    "        # Add batch dimension to the observation tensor\n",
    "        #print(\"I am here\")\n",
    "        # Transform obs to pytorch tensor\n",
    "        obs_transposed = np.transpose(obs, (2, 0, 1))\n",
    "        obs = np.expand_dims(obs_transposed, axis=0)\n",
    "        obs_tensor = torch.as_tensor(obs).float().to(device)\n",
    "        \n",
    "        # Use the model's policy to get the value and action.\n",
    "        # For actions, model.predict handles single observations well.\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        \n",
    "        #print(\"Action is:\", action)\n",
    "        \n",
    "        # Compute the value from the policy.\n",
    "        value_t = model.policy.predict_values(obs_tensor).item()\n",
    "        values.append(value_t)\n",
    "        \n",
    "        #print(\"Value is:\", value_t) # THE CODE STOPS HERE\n",
    "        \n",
    "        # Perform the step in the environment\n",
    "        obs, reward, done, truncated, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "    #print(\"I am here, outside the loop\")\n",
    "    # Add value of the terminal state (0 if done/truncated)\n",
    "    if done or truncated:\n",
    "        terminal_value = 0.0\n",
    "    else:\n",
    "        terminal_obs_tensor = torch.as_tensor(obs).float().unsqueeze(0).to(device)\n",
    "        terminal_value = model.policy.predict_values(terminal_obs_tensor).item()\n",
    "    values.append(terminal_value)\n",
    "\n",
    "    # Compute TD-errors and GAE-like regret score\n",
    "    for t in range(len(rewards)):\n",
    "        delta_t = rewards[t] + gamma * values[t + 1] * (1 - dones[t]) - values[t]\n",
    "        discounted_error = (gamma * lam) ** t * delta_t\n",
    "        regrets.append(max(0, discounted_error))\n",
    "\n",
    "    # Return the maximum positive regret score (or 0 if empty)\n",
    "    return max(regrets) if regrets else 0.0\n",
    "\n",
    "# ====================================================\n",
    "# 3. Regret Calculation with Generalized Advantage Estimation (GAE) in Parallel\n",
    "# ====================================================\n",
    "\n",
    "def calculate_regret_gae_parallel(\n",
    "    env_config, \n",
    "    model, \n",
    "    max_steps=1000, \n",
    "    gamma=0.99, \n",
    "    lam=0.95, \n",
    "    n_envs=4\n",
    "):\n",
    "    \"\"\"\n",
    "    Roll out n_envs copies of MyCustomGrid(env_config) in parallel,\n",
    "    compute GAE-based 'regret' for each environment, and return the max.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create vectorized env with n_envs copies\n",
    "    vec_env = create_vectorized_env(env_config, n_envs=n_envs)\n",
    "    obs_array = vec_env.reset()  # shape: (n_envs, height, width, 3)\n",
    "\n",
    "    # For each environment, we will store transitions to later compute GAE\n",
    "    # We'll keep them in lists, one per environment.\n",
    "    # Alternatively, we can store them in big arrays (n_envs, max_steps), etc.\n",
    "    rewards_list = [[] for _ in range(n_envs)]\n",
    "    values_list = [[] for _ in range(n_envs)]\n",
    "    dones_list = [[] for _ in range(n_envs)]\n",
    "\n",
    "    for t in range(max_steps):\n",
    "        # Model’s predict can handle multiple obs in a single forward pass\n",
    "        actions, _ = model.predict(obs_array, deterministic=True)\n",
    "        \n",
    "        # Also compute the values in one batch\n",
    "        # Convert obs_array to torch tensor\n",
    "        obs_tensor = torch.as_tensor(np.transpose(obs_array, (0, 3, 1, 2))).float().to(device)\n",
    "        with torch.no_grad():\n",
    "            # shape: (n_envs, 1)\n",
    "            value_t = model.policy.predict_values(obs_tensor).cpu().numpy().flatten()\n",
    "\n",
    "        # Step all envs in parallel\n",
    "        next_obs_array, rewards, dones, truncs = vec_env.step(actions)\n",
    "\n",
    "        # Store the results\n",
    "        for i in range(n_envs):\n",
    "            rewards_list[i].append(rewards[i])\n",
    "            values_list[i].append(value_t[i])\n",
    "            dones_list[i].append(bool(dones[i]) or bool(truncs[i]))\n",
    "\n",
    "        obs_array = next_obs_array\n",
    "\n",
    "        # If all envs are done or truncated, we can break early\n",
    "        if all(dones) or all(truncs):\n",
    "            break\n",
    "\n",
    "    # We also need the terminal value for each env\n",
    "    # (0 if done, otherwise model's value at final obs)\n",
    "    obs_tensor = torch.as_tensor(np.transpose(obs_array, (0, 3, 1, 2))).float().to(device)\n",
    "    with torch.no_grad():\n",
    "        final_values = model.policy.predict_values(obs_tensor).cpu().numpy().flatten()\n",
    "\n",
    "    # Now, compute GAE-based \"regret\" for each of the n_envs\n",
    "    regrets = []\n",
    "    for i in range(n_envs):\n",
    "        # If the env ended with done or truncated, terminal value = 0\n",
    "        if dones_list[i][-1]:\n",
    "            values_list[i].append(0.0)\n",
    "        else:\n",
    "            values_list[i].append(final_values[i])\n",
    "\n",
    "        # Compute delta_t and approximate GAE-like metric\n",
    "        env_rewards = rewards_list[i]\n",
    "        env_values = values_list[i]\n",
    "        env_dones = dones_list[i]\n",
    "\n",
    "        env_regrets = []\n",
    "        for t in range(len(env_rewards)):\n",
    "            #print(\"env_rewards[t]:\", env_rewards[t])\n",
    "            #print(\"gamma\", gamma)\n",
    "            #print(\"env_values[t + 1]:\", env_values[t + 1])\n",
    "            #print(\"env_dones[t]:\", env_dones[t].get('TimeLimit.truncated', False))\n",
    "            #print(\"env_values[t]:\", env_values[t])\n",
    "            delta_t = env_rewards[t] + gamma * env_values[t + 1] * (1 - env_dones[t]) - env_values[t]\n",
    "            # accumulate discounted error\n",
    "            discounted_error = (gamma * lam) ** t * delta_t\n",
    "            env_regrets.append(max(0, discounted_error))\n",
    "\n",
    "        # The environment's \"regret\" is the max of its positive GAE deltas\n",
    "        regrets.append(max(env_regrets) if env_regrets else 0.0)\n",
    "\n",
    "    # Return the maximum regret across the parallel envs\n",
    "    return max(regrets) if regrets else 0.0\n",
    "\n",
    "# ====================================================\n",
    "# 4. Custom Features Extractor for MiniGrid\n",
    "# ====================================================\n",
    "\n",
    "class MinigridFeaturesExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: gym.Space, features_dim: int = 512, normalized_image: bool = False) -> None:\n",
    "        super().__init__(observation_space, features_dim)\n",
    "        #print(\"The observation space given to the features extractor is:\", observation_space)\n",
    "        n_input_channels = observation_space.shape[0]\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, 16, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        # Compute shape by doing one forward pass\n",
    "        with torch.no_grad():\n",
    "            n_flatten = self.cnn(torch.as_tensor(observation_space.sample()[None]).float()).shape[1]\n",
    "\n",
    "        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
    "\n",
    "    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear(self.cnn(observations))\n",
    "\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=MinigridFeaturesExtractor,\n",
    "    features_extractor_kwargs=dict(features_dim=128),\n",
    ")\n",
    "\n",
    "def initialize_ppo(env, learning_rate=1e-4):\n",
    "    return PPO(\n",
    "        \"CnnPolicy\",                    # Convolutional neural network policy (For GPU)\n",
    "        env,                            # environment to learn from\n",
    "        verbose=0,                      # Display training output\n",
    "        n_steps=256,                    # Number of steps to run for each environment per update\n",
    "        batch_size=256,                  # Minibatch size for each gradient update\n",
    "        learning_rate=learning_rate,\n",
    "        policy_kwargs=policy_kwargs,    # Custom policy arguments\n",
    "        device=device                   # Use GPU if available\n",
    "    )\n",
    "\n",
    "def print_level_from_config(config):\n",
    "    env = MyCustomGrid(config, render_mode='rgb_array')\n",
    "    env.reset()\n",
    "    full_level_image = env.render()  # This should return an RGB image of the full grid\n",
    "\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(full_level_image)\n",
    "    plt.title(\"Level Configuration: \" + str(config))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "# Use vectorized environment\n",
    "def create_vectorized_env(config, n_envs=4):\n",
    "    \"\"\"\n",
    "    Create a vectorized environment with n parallel environments.\n",
    "    \"\"\"\n",
    "    env_fns = [lambda: MyCustomGrid(config) for _ in range(n_envs)]\n",
    "    return make_vec_env(lambda: MyCustomGrid(config), n_envs=n_envs)\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "def main_accel(total_iterations, replay_prob, train_steps, level_buffer_size,\n",
    "                    initial_fill_size, grid_size, n_envs, edit_levels, regret_threshold,\n",
    "                    easy_start):\n",
    "    # Create a level buffer\n",
    "    level_buffer = LevelBuffer(max_size=level_buffer_size)\n",
    "    iteration_regrets = []\n",
    "    \n",
    "    # Use a random config to create a vectorized environment\n",
    "    dummy_config = random_config(grid_size)\n",
    "    vectorized_env = create_vectorized_env(dummy_config, n_envs=n_envs)\n",
    "    print(\"Vectorized environment created.\")\n",
    "  \n",
    "    # Initialize PPO with vectorized environment\n",
    "    print(\"Initializing student model PPO...\")\n",
    "    student_model = initialize_ppo(vectorized_env)\n",
    "\n",
    "    # Populate buffer with initial levels\n",
    "    print(f\"Populating buffer with {initial_fill_size} initial levels with regret > {regret_threshold}...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    while len(level_buffer.data) < initial_fill_size:\n",
    "        if easy_start:\n",
    "            cfg = random_config(grid_size, num_blocks=2)\n",
    "        else:\n",
    "            cfg = random_config(grid_size)\n",
    "            \n",
    "        regret = calculate_regret_gae_parallel(cfg, student_model, max_steps=1000, gamma=0.99, lam=0.95, n_envs=n_envs)\n",
    "        #regret = calculate_regret_gae(MyCustomGrid(cfg), student_model, max_steps=500, gamma=0.99, lam=0.95)\n",
    "        \n",
    "        #print(f\"Regret for initial level: {regret}, {len(level_buffer.data)}\")\n",
    "\n",
    "        # Skip levels with low regret\n",
    "        if regret < regret_threshold:\n",
    "            continue\n",
    "\n",
    "        level_buffer.add(cfg, regret)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"Initial buffer fill took {end_time - start_time:.2f} seconds.\")\n",
    "    \n",
    "\n",
    "    # Main ACCEL loop\n",
    "    iteration, skipped = 0, 0\n",
    "    print(\"\\nMain training loop...\")\n",
    "    while iteration < total_iterations + skipped:\n",
    "        print(f\"\\n=== ITERATION {iteration + 1}/{total_iterations + skipped} SKIPPED: {skipped} ===\")\n",
    "        \n",
    "        iteration += 1\n",
    "        \n",
    "        # Decide whether to replay or generate a new level\n",
    "        use_replay = np.random.rand() < replay_prob\n",
    "\n",
    "        if not use_replay or len(level_buffer.data) == 0:\n",
    "            # Create a new random level\n",
    "            cfg = random_config(grid_size)\n",
    "            print(\"Generated new random level:\", cfg)\n",
    "        else:\n",
    "            # Sample a level from the buffer\n",
    "            cfg = level_buffer.sample_config()\n",
    "            print(\"Sampled level from buffer:\", cfg)\n",
    "        \n",
    "                \n",
    "        # Update the vectorized environment with the selected config and train the model\n",
    "        vectorized_env = create_vectorized_env(cfg, n_envs=n_envs)\n",
    "        student_model.set_env(vectorized_env)\n",
    "        student_model.learn(total_timesteps=train_steps)\n",
    "\n",
    "\n",
    "\n",
    "        if not use_replay or not edit_levels:\n",
    "            regret = calculate_regret_gae_parallel(cfg, student_model, max_steps=1000, gamma=0.99, lam=0.95, n_envs=n_envs)\n",
    "        else:\n",
    "            # Edit the level and calculate regret\n",
    "            cfg = edit_config(cfg)\n",
    "            print(\"Edited level to:\", cfg)\n",
    "            regret = calculate_regret_gae_parallel(cfg, student_model, max_steps=1000, gamma=0.99, lam=0.95, n_envs=n_envs)\n",
    "        \n",
    "        \n",
    "        if regret <= regret_threshold:\n",
    "            print(f\"Regret for current level is {regret:.5f} <= threshold {regret_threshold}. Skipping...\")\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        \n",
    "        print(f\"Regret for current level: {regret}\")\n",
    "        level_buffer.add(cfg, regret)\n",
    "        iteration_regrets.append(regret)\n",
    "        \n",
    "    \n",
    "\n",
    "    # Plot and display the progress\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(iteration_regrets, marker='o')\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Regret\")\n",
    "    plt.title(\"Regret Progress during ACCEL Training\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nDone. Final buffer size:\", len(level_buffer.data))\n",
    "    print(\"Top-5 hardest levels (config, regret):\")\n",
    "    level_buffer.data.sort(key=lambda x: x[1], reverse=True)\n",
    "    for i, (cfg, sc) in enumerate(level_buffer.data[:5]):\n",
    "        print(f\"{i + 1}. regret={sc:.5f}, config={cfg}\")\n",
    "        #print_level_from_config(cfg)\n",
    "        \n",
    "    print(\"Top-5 easiest levels (config, regret):\")\n",
    "    level_buffer.data.sort(key=lambda x: x[1])\n",
    "    for i, (cfg, sc) in enumerate(level_buffer.data[:5]):\n",
    "        print(f\"{i + 1}. regret={sc:.5f}, config={cfg}\")\n",
    "        #print_level_from_config(cfg)\n",
    "    \n",
    "    return student_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ACCEL with easy start with config: {'grid_size': 8, 'total_iterations': 15, 'train_steps': 500, 'replay_prob': 0.7, 'level_buffer_size': 128, 'initial_fill_size': 64, 'regret_threshold': 0.0, 'n_envs': 3, 'edit_levels': True, 'easy_start': True}\n",
      "Vectorized environment created.\n",
      "Initializing student model PPO...\n",
      "Populating buffer with 64 initial levels with regret > 0.0...\n",
      "Initial buffer fill took 0.79 seconds.\n",
      "\n",
      "Main training loop...\n",
      "\n",
      "=== ITERATION 1/15 SKIPPED: 0 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 1, 'start_pos': None, 'goal_pos': None, 'edited': False, 'seed_val': 48984}\n",
      "Regret for current level is 0.00000 <= threshold 0.0. Skipping...\n",
      "\n",
      "=== ITERATION 2/16 SKIPPED: 1 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 2, 'start_pos': (2, 6), 'goal_pos': (6, 5), 'edited': False, 'seed_val': 256840}\n",
      "Edited level to: {'width': 8, 'height': 8, 'num_blocks': 3, 'start_pos': (6, 2), 'goal_pos': (1, 6), 'edited': True, 'seed_val': 256840}\n",
      "Regret for current level: 0.9170742928981781\n",
      "\n",
      "=== ITERATION 3/16 SKIPPED: 1 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 2, 'start_pos': (2, 1), 'goal_pos': (4, 5), 'edited': False, 'seed_val': 223165}\n",
      "Edited level to: {'width': 8, 'height': 8, 'num_blocks': 3, 'start_pos': (1, 2), 'goal_pos': (4, 2), 'edited': True, 'seed_val': 223165}\n",
      "Regret for current level is 0.00000 <= threshold 0.0. Skipping...\n",
      "\n",
      "=== ITERATION 4/17 SKIPPED: 2 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 12, 'start_pos': None, 'goal_pos': None, 'edited': False, 'seed_val': 836232}\n",
      "Regret for current level is 0.00000 <= threshold 0.0. Skipping...\n",
      "\n",
      "=== ITERATION 5/18 SKIPPED: 3 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 3, 'start_pos': (2, 2), 'goal_pos': (1, 4), 'edited': True, 'seed_val': 256840}\n",
      "Edited level to: {'width': 8, 'height': 8, 'num_blocks': 4, 'start_pos': (4, 2), 'goal_pos': (3, 6), 'edited': True, 'seed_val': 256840}\n",
      "Regret for current level is 0.00000 <= threshold 0.0. Skipping...\n",
      "\n",
      "=== ITERATION 6/19 SKIPPED: 4 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 20, 'start_pos': None, 'goal_pos': None, 'edited': False, 'seed_val': 206885}\n",
      "Regret for current level is 0.00000 <= threshold 0.0. Skipping...\n",
      "\n",
      "=== ITERATION 7/20 SKIPPED: 5 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 3, 'start_pos': (4, 2), 'goal_pos': (3, 6), 'edited': True, 'seed_val': 256840}\n",
      "Edited level to: {'width': 8, 'height': 8, 'num_blocks': 4, 'start_pos': (4, 2), 'goal_pos': (3, 6), 'edited': True, 'seed_val': 256840}\n",
      "Regret for current level is 0.00000 <= threshold 0.0. Skipping...\n",
      "\n",
      "=== ITERATION 8/21 SKIPPED: 6 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 2, 'start_pos': (1, 4), 'goal_pos': (1, 1), 'edited': False, 'seed_val': 365838}\n",
      "Edited level to: {'width': 8, 'height': 8, 'num_blocks': 3, 'start_pos': (1, 4), 'goal_pos': (5, 5), 'edited': True, 'seed_val': 365838}\n",
      "Regret for current level is 0.00000 <= threshold 0.0. Skipping...\n",
      "\n",
      "=== ITERATION 9/22 SKIPPED: 7 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 2, 'start_pos': (1, 2), 'goal_pos': (4, 2), 'edited': False, 'seed_val': 223165}\n",
      "Edited level to: {'width': 8, 'height': 8, 'num_blocks': 3, 'start_pos': (3, 1), 'goal_pos': (3, 6), 'edited': True, 'seed_val': 223165}\n",
      "Regret for current level is 0.00000 <= threshold 0.0. Skipping...\n",
      "\n",
      "=== ITERATION 10/23 SKIPPED: 8 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 2, 'start_pos': (2, 2), 'goal_pos': (1, 5), 'edited': False, 'seed_val': 149503}\n",
      "Edited level to: {'width': 8, 'height': 8, 'num_blocks': 3, 'start_pos': (6, 2), 'goal_pos': (1, 3), 'edited': True, 'seed_val': 149503}\n",
      "Regret for current level is 0.00000 <= threshold 0.0. Skipping...\n",
      "\n",
      "=== ITERATION 11/24 SKIPPED: 9 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 12, 'start_pos': None, 'goal_pos': None, 'edited': False, 'seed_val': 36815}\n",
      "Regret for current level is 0.00000 <= threshold 0.0. Skipping...\n",
      "\n",
      "=== ITERATION 12/25 SKIPPED: 10 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 2, 'start_pos': (6, 2), 'goal_pos': (1, 6), 'edited': False, 'seed_val': 256840}\n",
      "Edited level to: {'width': 8, 'height': 8, 'num_blocks': 3, 'start_pos': (6, 2), 'goal_pos': (1, 6), 'edited': True, 'seed_val': 256840}\n",
      "Regret for current level: 0.7688450813293457\n",
      "\n",
      "=== ITERATION 13/25 SKIPPED: 10 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 3, 'start_pos': (2, 2), 'goal_pos': (1, 4), 'edited': True, 'seed_val': 256840}\n",
      "Edited level to: {'width': 8, 'height': 8, 'num_blocks': 4, 'start_pos': (4, 2), 'goal_pos': (3, 6), 'edited': True, 'seed_val': 256840}\n",
      "Regret for current level: 0.7896319478750229\n",
      "\n",
      "=== ITERATION 14/25 SKIPPED: 10 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 10, 'start_pos': None, 'goal_pos': None, 'edited': False, 'seed_val': 135388}\n",
      "Regret for current level is 0.00000 <= threshold 0.0. Skipping...\n",
      "\n",
      "=== ITERATION 15/26 SKIPPED: 11 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 3, 'start_pos': (4, 2), 'goal_pos': (3, 6), 'edited': True, 'seed_val': 256840}\n",
      "Edited level to: {'width': 8, 'height': 8, 'num_blocks': 4, 'start_pos': (1, 4), 'goal_pos': (5, 3), 'edited': True, 'seed_val': 256840}\n",
      "Regret for current level is 0.00000 <= threshold 0.0. Skipping...\n",
      "\n",
      "=== ITERATION 16/27 SKIPPED: 12 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 4, 'start_pos': (6, 2), 'goal_pos': (1, 6), 'edited': True, 'seed_val': 256840}\n",
      "Edited level to: {'width': 8, 'height': 8, 'num_blocks': 5, 'start_pos': (5, 5), 'goal_pos': (3, 4), 'edited': True, 'seed_val': 256840}\n",
      "Regret for current level is 0.00000 <= threshold 0.0. Skipping...\n",
      "\n",
      "=== ITERATION 17/28 SKIPPED: 13 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 2, 'start_pos': (3, 1), 'goal_pos': (3, 6), 'edited': False, 'seed_val': 223165}\n",
      "Edited level to: {'width': 8, 'height': 8, 'num_blocks': 3, 'start_pos': (1, 2), 'goal_pos': (4, 2), 'edited': True, 'seed_val': 223165}\n",
      "Regret for current level is 0.00000 <= threshold 0.0. Skipping...\n",
      "\n",
      "=== ITERATION 18/29 SKIPPED: 14 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 20, 'start_pos': None, 'goal_pos': None, 'edited': False, 'seed_val': 699497}\n",
      "Regret for current level is 0.00000 <= threshold 0.0. Skipping...\n",
      "\n",
      "=== ITERATION 19/30 SKIPPED: 15 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 19, 'start_pos': None, 'goal_pos': None, 'edited': False, 'seed_val': 835181}\n",
      "Regret for current level is 0.00000 <= threshold 0.0. Skipping...\n",
      "\n",
      "=== ITERATION 20/31 SKIPPED: 16 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 2, 'start_pos': (6, 2), 'goal_pos': (1, 3), 'edited': False, 'seed_val': 149503}\n",
      "Edited level to: {'width': 8, 'height': 8, 'num_blocks': 3, 'start_pos': (6, 2), 'goal_pos': (1, 3), 'edited': True, 'seed_val': 149503}\n",
      "Regret for current level is 0.00000 <= threshold 0.0. Skipping...\n",
      "\n",
      "=== ITERATION 21/32 SKIPPED: 17 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 4, 'start_pos': (5, 5), 'goal_pos': (3, 4), 'edited': True, 'seed_val': 256840}\n",
      "Edited level to: {'width': 8, 'height': 8, 'num_blocks': 5, 'start_pos': (5, 5), 'goal_pos': (3, 4), 'edited': True, 'seed_val': 256840}\n",
      "Regret for current level is 0.00000 <= threshold 0.0. Skipping...\n",
      "\n",
      "=== ITERATION 22/33 SKIPPED: 18 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 3, 'start_pos': (1, 4), 'goal_pos': (5, 3), 'edited': True, 'seed_val': 256840}\n",
      "Edited level to: {'width': 8, 'height': 8, 'num_blocks': 4, 'start_pos': (1, 4), 'goal_pos': (5, 3), 'edited': True, 'seed_val': 256840}\n",
      "Regret for current level: 0.618484228849411\n",
      "\n",
      "=== ITERATION 23/33 SKIPPED: 18 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 3, 'start_pos': (1, 4), 'goal_pos': (5, 3), 'edited': True, 'seed_val': 256840}\n",
      "Edited level to: {'width': 8, 'height': 8, 'num_blocks': 4, 'start_pos': (1, 4), 'goal_pos': (5, 3), 'edited': True, 'seed_val': 256840}\n",
      "Regret for current level is 0.00000 <= threshold 0.0. Skipping...\n",
      "\n",
      "=== ITERATION 24/34 SKIPPED: 19 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 10, 'start_pos': None, 'goal_pos': None, 'edited': False, 'seed_val': 102194}\n",
      "Regret for current level is 0.00000 <= threshold 0.0. Skipping...\n",
      "\n",
      "=== ITERATION 25/35 SKIPPED: 20 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 14, 'start_pos': None, 'goal_pos': None, 'edited': False, 'seed_val': 248933}\n",
      "Regret for current level is 0.00000 <= threshold 0.0. Skipping...\n",
      "\n",
      "=== ITERATION 26/36 SKIPPED: 21 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 3, 'start_pos': (1, 4), 'goal_pos': (5, 3), 'edited': True, 'seed_val': 256840}\n",
      "Edited level to: {'width': 8, 'height': 8, 'num_blocks': 4, 'start_pos': (1, 4), 'goal_pos': (5, 3), 'edited': True, 'seed_val': 256840}\n",
      "Regret for current level is 0.00000 <= threshold 0.0. Skipping...\n",
      "\n",
      "=== ITERATION 27/37 SKIPPED: 22 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 3, 'start_pos': None, 'goal_pos': None, 'edited': False, 'seed_val': 875049}\n",
      "Regret for current level is 0.00000 <= threshold 0.0. Skipping...\n",
      "\n",
      "=== ITERATION 28/38 SKIPPED: 23 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 3, 'start_pos': (4, 2), 'goal_pos': (3, 6), 'edited': True, 'seed_val': 256840}\n",
      "Edited level to: {'width': 8, 'height': 8, 'num_blocks': 4, 'start_pos': (1, 4), 'goal_pos': (5, 3), 'edited': True, 'seed_val': 256840}\n",
      "Regret for current level is 0.00000 <= threshold 0.0. Skipping...\n",
      "\n",
      "=== ITERATION 29/39 SKIPPED: 24 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 21, 'start_pos': None, 'goal_pos': None, 'edited': False, 'seed_val': 186745}\n",
      "Regret for current level is 0.00000 <= threshold 0.0. Skipping...\n",
      "\n",
      "=== ITERATION 30/40 SKIPPED: 25 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 2, 'start_pos': (6, 2), 'goal_pos': (1, 3), 'edited': False, 'seed_val': 149503}\n",
      "Edited level to: {'width': 8, 'height': 8, 'num_blocks': 3, 'start_pos': (6, 2), 'goal_pos': (1, 3), 'edited': True, 'seed_val': 149503}\n",
      "Regret for current level is 0.00000 <= threshold 0.0. Skipping...\n",
      "\n",
      "=== ITERATION 31/41 SKIPPED: 26 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 2, 'start_pos': (6, 2), 'goal_pos': (1, 3), 'edited': False, 'seed_val': 149503}\n",
      "Edited level to: {'width': 8, 'height': 8, 'num_blocks': 3, 'start_pos': (6, 2), 'goal_pos': (1, 3), 'edited': True, 'seed_val': 149503}\n",
      "Regret for current level: 0.5509995818138123\n",
      "\n",
      "=== ITERATION 32/41 SKIPPED: 26 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 4, 'start_pos': (5, 5), 'goal_pos': (3, 4), 'edited': True, 'seed_val': 256840}\n",
      "Edited level to: {'width': 8, 'height': 8, 'num_blocks': 5, 'start_pos': (5, 5), 'goal_pos': (3, 4), 'edited': True, 'seed_val': 256840}\n",
      "Regret for current level is 0.00000 <= threshold 0.0. Skipping...\n",
      "\n",
      "=== ITERATION 33/42 SKIPPED: 27 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 1, 'start_pos': None, 'goal_pos': None, 'edited': False, 'seed_val': 221982}\n",
      "Regret for current level is 0.00000 <= threshold 0.0. Skipping...\n",
      "\n",
      "=== ITERATION 34/43 SKIPPED: 28 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 2, 'start_pos': (1, 2), 'goal_pos': (4, 2), 'edited': False, 'seed_val': 223165}\n",
      "Edited level to: {'width': 8, 'height': 8, 'num_blocks': 3, 'start_pos': (1, 2), 'goal_pos': (4, 2), 'edited': True, 'seed_val': 223165}\n",
      "Regret for current level is 0.00000 <= threshold 0.0. Skipping...\n",
      "\n",
      "=== ITERATION 35/44 SKIPPED: 29 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 3, 'start_pos': (1, 4), 'goal_pos': (5, 3), 'edited': True, 'seed_val': 256840}\n",
      "Edited level to: {'width': 8, 'height': 8, 'num_blocks': 4, 'start_pos': (4, 2), 'goal_pos': (3, 6), 'edited': True, 'seed_val': 256840}\n",
      "Regret for current level is 0.00000 <= threshold 0.0. Skipping...\n",
      "\n",
      "=== ITERATION 36/45 SKIPPED: 30 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 12, 'start_pos': None, 'goal_pos': None, 'edited': False, 'seed_val': 928578}\n",
      "Regret for current level is 0.00000 <= threshold 0.0. Skipping...\n",
      "\n",
      "=== ITERATION 37/46 SKIPPED: 31 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 3, 'start_pos': (2, 4), 'goal_pos': (3, 5), 'edited': True, 'seed_val': 149503}\n",
      "Edited level to: {'width': 8, 'height': 8, 'num_blocks': 4, 'start_pos': (4, 1), 'goal_pos': (1, 4), 'edited': True, 'seed_val': 149503}\n",
      "Regret for current level is 0.00000 <= threshold 0.0. Skipping...\n",
      "\n",
      "=== ITERATION 38/47 SKIPPED: 32 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 2, 'start_pos': (1, 3), 'goal_pos': (3, 3), 'edited': False, 'seed_val': 533556}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprun\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-s cumulative\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# set all possible seeds    \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mtorch.manual_seed(42)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mnp.random.seed(42)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mconfig = \u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrid_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: 8,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtotal_iterations\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: 15,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_steps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: 500,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreplay_prob\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: 0.7,           # Probability of replaying a level and editing it vs. generating a new one\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlevel_buffer_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: 128,     # Maximum number of levels to store in the buffer\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minitial_fill_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: 64,      # Number of levels to pre-fill the buffer with\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mregret_threshold\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: 0.0,      # Minimum regret threshold to consider a level for the buffer\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn_envs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: 3,                  # Number of parallel environments to use for training\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43medit_levels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: True,          # Whether to edit levels during training i.e. ACCEL or PLR\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43measy_start\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: True            # Whether to fill the buffer with easy levels first i.e. minimum number of blocks\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    }\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mconfig[\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43medit_levels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m] = False\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mconfig[\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43measy_start\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m] = False\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mprint(f\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRunning PLR with config: \u001b[39;49m\u001b[38;5;132;43;01m{config}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel_plr = main_accel(**config)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mprint(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mn============================================\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mconfig[\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43medit_levels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m] = True\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mconfig[\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43measy_start\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m] = False\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mprint(f\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRunning ACCEL with config: \u001b[39;49m\u001b[38;5;132;43;01m{config}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel_accel = main_accel(**config)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mprint(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mn============================================\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mconfig[\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43medit_levels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m] = True\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mconfig[\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43measy_start\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m] = True\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mprint(f\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRunning ACCEL with easy start with config: \u001b[39;49m\u001b[38;5;132;43;01m{config}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel_accel_easy = main_accel(**config)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Emanuele\\miniconda3\\envs\\accel_env_cuda\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:2541\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2539\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[0;32m   2540\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[1;32m-> 2541\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2543\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[0;32m   2544\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[0;32m   2545\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[0;32m   2546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\Emanuele\\miniconda3\\envs\\accel_env_cuda\\Lib\\site-packages\\IPython\\core\\magics\\execution.py:317\u001b[0m, in \u001b[0;36mExecutionMagics.prun\u001b[1;34m(self, parameter_s, cell)\u001b[0m\n\u001b[0;32m    315\u001b[0m     arg_str \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m cell\n\u001b[0;32m    316\u001b[0m arg_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39mtransform_cell(arg_str)\n\u001b[1;32m--> 317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_with_profiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_ns\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Emanuele\\miniconda3\\envs\\accel_env_cuda\\Lib\\site-packages\\IPython\\core\\magics\\execution.py:339\u001b[0m, in \u001b[0;36mExecutionMagics._run_with_profiler\u001b[1;34m(self, code, opts, namespace)\u001b[0m\n\u001b[0;32m    337\u001b[0m prof \u001b[38;5;241m=\u001b[39m profile\u001b[38;5;241m.\u001b[39mProfile()\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 339\u001b[0m     prof \u001b[38;5;241m=\u001b[39m \u001b[43mprof\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunctx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    340\u001b[0m     sys_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mSystemExit\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Emanuele\\miniconda3\\envs\\accel_env_cuda\\Lib\\cProfile.py:101\u001b[0m, in \u001b[0;36mProfile.runctx\u001b[1;34m(self, cmd, globals, locals)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable()\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 101\u001b[0m     \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisable()\n",
      "File \u001b[1;32m<string>:45\u001b[0m\n",
      "Cell \u001b[1;32mIn[5], line 508\u001b[0m, in \u001b[0;36mmain_accel\u001b[1;34m(total_iterations, replay_prob, train_steps, level_buffer_size, initial_fill_size, grid_size, n_envs, edit_levels, regret_threshold, easy_start)\u001b[0m\n\u001b[0;32m    506\u001b[0m vectorized_env \u001b[38;5;241m=\u001b[39m create_vectorized_env(cfg, n_envs\u001b[38;5;241m=\u001b[39mn_envs)\n\u001b[0;32m    507\u001b[0m student_model\u001b[38;5;241m.\u001b[39mset_env(vectorized_env)\n\u001b[1;32m--> 508\u001b[0m \u001b[43mstudent_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_replay \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m edit_levels:\n\u001b[0;32m    513\u001b[0m     regret \u001b[38;5;241m=\u001b[39m calculate_regret_gae_parallel(cfg, student_model, max_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.99\u001b[39m, lam\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m, n_envs\u001b[38;5;241m=\u001b[39mn_envs)\n",
      "File \u001b[1;32mc:\\Users\\Emanuele\\miniconda3\\envs\\accel_env_cuda\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Emanuele\\miniconda3\\envs\\accel_env_cuda\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:323\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 323\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[0;32m    326\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Emanuele\\miniconda3\\envs\\accel_env_cuda\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:247\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    244\u001b[0m             terminal_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mpredict_values(terminal_obs)[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    245\u001b[0m         rewards[idx] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m terminal_value\n\u001b[1;32m--> 247\u001b[0m \u001b[43mrollout_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_last_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_last_episode_starts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs \u001b[38;5;241m=\u001b[39m new_obs  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_episode_starts \u001b[38;5;241m=\u001b[39m dones\n",
      "File \u001b[1;32mc:\\Users\\Emanuele\\miniconda3\\envs\\accel_env_cuda\\Lib\\site-packages\\stable_baselines3\\common\\buffers.py:475\u001b[0m, in \u001b[0;36mRolloutBuffer.add\u001b[1;34m(self, obs, action, reward, episode_start, value, log_prob)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_starts[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(episode_start)\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos] \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m--> 475\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_probs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos] \u001b[38;5;241m=\u001b[39m \u001b[43mlog_prob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_size:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%prun -s cumulative\n",
    "\n",
    "\n",
    "# set all possible seeds    \n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "config = {\n",
    "        \"grid_size\": 8,\n",
    "        \n",
    "        \"total_iterations\": 15,\n",
    "        \"train_steps\": 500,\n",
    "\n",
    "        \"replay_prob\": 0.7,           # Probability of replaying a level and editing it vs. generating a new one\n",
    "        \"level_buffer_size\": 128,     # Maximum number of levels to store in the buffer\n",
    "        \"initial_fill_size\": 64,      # Number of levels to pre-fill the buffer with\n",
    "        \"regret_threshold\": 0.0,      # Minimum regret threshold to consider a level for the buffer\n",
    "        \n",
    "        \"n_envs\": 3,                  # Number of parallel environments to use for training\n",
    "        \n",
    "        \"edit_levels\": True,          # Whether to edit levels during training i.e. ACCEL or PLR\n",
    "        \"easy_start\": True            # Whether to fill the buffer with easy levels first i.e. minimum number of blocks\n",
    "    }\n",
    "\n",
    "\n",
    "'''\n",
    "config[\"edit_levels\"] = False\n",
    "config[\"easy_start\"] = False\n",
    "print(f\"Running PLR with config: {config}\")\n",
    "model_plr = main_accel(**config)\n",
    "\n",
    "print(\"\\n\\n============================================\\n\\n\")\n",
    "\n",
    "config[\"edit_levels\"] = True\n",
    "config[\"easy_start\"] = False\n",
    "print(f\"Running ACCEL with config: {config}\")\n",
    "model_accel = main_accel(**config)\n",
    "\n",
    "print(\"\\n\\n============================================\\n\\n\")\n",
    "'''\n",
    "\n",
    "config[\"edit_levels\"] = True\n",
    "config[\"easy_start\"] = True\n",
    "print(f\"Running ACCEL with easy start with config: {config}\")\n",
    "model_accel_easy = main_accel(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'PLR': model_plr, 'ACCEL': model_accel, 'ACCEL-EasyStart': model_accel_easy}\n",
    "models = {'ACCEL-EasyStart': model_accel_easy}\n",
    "difficulties = 3\n",
    "# Generate n levels with increasing complexity, for each level generate 10 configs\n",
    "levels = []\n",
    "for i in range(difficulties):\n",
    "    level = []\n",
    "    for _ in range(10):\n",
    "        cfg = random_config(config[\"grid_size\"], num_blocks=i*i*3)\n",
    "        #print_level_from_config(cfg)\n",
    "        level.append(cfg)\n",
    "    levels.append(level)\n",
    "\n",
    "# Evaluate the model on the generated levels\n",
    "results = {}\n",
    "for model_name, model in models.items():\n",
    "    results[model_name] = []\n",
    "    for i, level in enumerate(levels):\n",
    "        print(f\"Evaluating level {i + 1} with {4 + i*i} blocks for model {model_name}...\")\n",
    "        r = []\n",
    "        for j, cfg in enumerate(level):\n",
    "            # Create vectorized environment\n",
    "            env = create_vectorized_env(cfg, n_envs=8)\n",
    "            mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=20, deterministic=True)\n",
    "            r.append(mean_reward)\n",
    "        results[model_name].append(r)\n",
    "    print()\n",
    "    \n",
    "# Print mean rewards for each level\n",
    "for model_name in models.keys():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    for i, level in enumerate(levels):\n",
    "        print(f\"Level {i + 1} - Complexity {4 + i*i}: {np.mean(results[model_name][i]):.2f}\")\n",
    "    print()\n",
    "\n",
    "# Boxplot of results, a plot for each level complexity comparing models\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i, level in enumerate(levels):\n",
    "    plt.subplot(1, difficulties, i + 1)\n",
    "    plt.boxplot([results[model_name][i] for model_name in models.keys()])\n",
    "    plt.xticks([1, 2, 3], [model_name for model_name in models.keys()])\n",
    "    plt.title(f\"Level {i + 1} - Complexity {4 + i*3}\")\n",
    "    plt.ylabel(\"Mean Reward\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, config):\n",
    "    env = MyCustomGrid(config, render_mode='human')\n",
    "    obs, _ = env.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    total_reward = 0\n",
    "    i = 0\n",
    "    while not terminated or not truncated:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        #print(terminated, truncated)\n",
    "        total_reward += reward\n",
    "        i += 1\n",
    "        if i > 20:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "# Test the models on a few levels\n",
    "for i in range(30):\n",
    "    test_model(model_accel_easy, random_config(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level Editor Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random level and visualize it\n",
    "random_cnf = random_config(8)\n",
    "print_level_from_config(random_cnf)\n",
    "print(\"random_cnf:\", random_cnf)\n",
    "\n",
    "# Edit the random level and visualize it\n",
    "edited_config = edit_config(random_cnf)\n",
    "print_level_from_config(edited_config)\n",
    "\n",
    "# Edit the random level and visualize it\n",
    "edited_config = edit_config(random_cnf)\n",
    "print_level_from_config(edited_config)\n",
    "\n",
    "# Edit the random level and visualize it\n",
    "edited_config = edit_config(random_cnf)\n",
    "print_level_from_config(edited_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maze generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_maze(width, height):\n",
    "    maze = [[1 for _ in range(width)] for _ in range(height)]  # 1 for walls\n",
    "    stack = []\n",
    "    directions = [(0, 2), (0, -2), (2, 0), (-2, 0)]\n",
    "\n",
    "    def is_valid(x, y):\n",
    "        return 0 < x < height - 1 and 0 < y < width - 1 and maze[x][y] == 1\n",
    "\n",
    "    def carve(x, y):\n",
    "        maze[x][y] = 0\n",
    "        random.shuffle(directions)\n",
    "        for dx, dy in directions:\n",
    "            nx, ny = x + dx, y + dy\n",
    "            if is_valid(nx, ny):\n",
    "                maze[x + dx // 2][y + dy // 2] = 0\n",
    "                carve(nx, ny)\n",
    "\n",
    "    carve(1, 1)  # Start at (1, 1)\n",
    "\n",
    "    return maze\n",
    "\n",
    "# Display the maze\n",
    "def print_maze(maze):\n",
    "    for row in maze:\n",
    "        print(\"\".join(\"█\" if cell == 1 else \" \" for cell in row))\n",
    "\n",
    "maze = generate_maze(21, 21)\n",
    "print_maze(maze)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomMaze(MiniGridEnv):\n",
    "    \"\"\"\n",
    "    Simple MiniGrid environment that places random wall tiles\n",
    "    according to a config dict, returning only the 'image' observation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config=None, **kwargs):\n",
    "        if config is None:\n",
    "            config = {}\n",
    "        self.config = config\n",
    "\n",
    "        # Extract parameters from config\n",
    "        self.width = config.get(\"width\")\n",
    "        self.height = config.get(\"height\")\n",
    "        self.num_blocks = config.get(\"num_blocks\")\n",
    "        self.custom_seed = config.get(\"seed_val\")\n",
    "        \n",
    "        \n",
    "        # Create a random number generator with the custom seed\n",
    "        self.rng = np.random.default_rng(seed=self.custom_seed)\n",
    "\n",
    "        grid_size = max(self.width, self.height)\n",
    "\n",
    "        mission_space = MissionSpace(mission_func=lambda: \"get to the green goal square\")\n",
    "\n",
    "        super().__init__(\n",
    "            grid_size=grid_size,\n",
    "            max_steps=self.width * self.height * 2, # max_steps is typically 2x the grid size\n",
    "            see_through_walls=False,\n",
    "            agent_view_size=5,                      # Size of the agent's view square\n",
    "            mission_space=mission_space,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # Manually define our observation_space as a single Box (the image).\n",
    "        # By default, MiniGrid's image shape is (view_size, view_size, 3) if using partial obs,\n",
    "        # or (height, width, 3) if using full-grid observation. We'll do full-grid here:\n",
    "        # We'll define (self.height, self.width, 3) as the shape.\n",
    "        # In practice, \"image\" shape can vary if partial observations are used.\n",
    "        self.observation_space = Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=(self.agent_view_size, self.agent_view_size, 3),\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "\n",
    "    \n",
    "    def _gen_grid(self, width, height):\n",
    "        \"\"\"\n",
    "        Generate the grid layout for a new episode using the DFS Maze Generation Algorithm.\n",
    "        \"\"\"\n",
    "        # Create an empty grid of the \"true\" width x height from config\n",
    "        self.grid = Grid(self.width, self.height)\n",
    "        # Surround the grid with walls\n",
    "        self.grid.wall_rect(0, 0, self.width, self.height)\n",
    "\n",
    "        # Initialize the maze as walls\n",
    "        maze = [[1 for _ in range(self.width)] for _ in range(self.height)]\n",
    "\n",
    "        # Define directions for DFS\n",
    "        directions = [(0, 2), (0, -2), (2, 0), (-2, 0)]\n",
    "\n",
    "        def is_valid(x, y):\n",
    "            \"\"\"Check if a cell is valid for carving.\"\"\"\n",
    "            return 0 < x < self.height - 1 and 0 < y < self.width - 1 and maze[x][y] == 1\n",
    "\n",
    "        def carve(x, y):\n",
    "            \"\"\"Carve passages in the maze using DFS.\"\"\"\n",
    "            maze[x][y] = 0  # Mark the cell as part of the maze\n",
    "            self.grid.set(x, y, None)  # Clear the wall in the grid\n",
    "            self.rng.shuffle(directions)\n",
    "            for dx, dy in directions:\n",
    "                nx, ny = x + dx, y + dy\n",
    "                if is_valid(nx, ny):\n",
    "                    # Remove the wall between cells\n",
    "                    maze[x + dx // 2][y + dy // 2] = 0\n",
    "                    self.grid.set(x + dx // 2, y + dy // 2, None)\n",
    "                    carve(nx, ny)\n",
    "\n",
    "        # Start carving from the top-left corner\n",
    "        carve(1, 1)\n",
    "\n",
    "        # Place the goal object in a random position not occupied by a wall\n",
    "        while True:\n",
    "            r = self.rng.integers(1, self.height - 1)\n",
    "            c = self.rng.integers(1, self.width - 1)\n",
    "            if self.grid.get(c, r) is None:\n",
    "                self.put_obj(Goal(), c, r)\n",
    "                break\n",
    "\n",
    "        # Place the agent in a random position not occupied by a wall and not on the goal\n",
    "        while True:\n",
    "            r = self.rng.integers(1, self.height - 1)\n",
    "            c = self.rng.integers(1, self.width - 1)\n",
    "            if self.grid.get(c, r) is None:\n",
    "                self.place_agent(top=(c, r), rand_dir=True)\n",
    "                break\n",
    "    \n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Override reset to ensure we only return the 'image' array\n",
    "        instead of a dict with 'image' and 'mission'.\n",
    "        \"\"\"\n",
    "        obs, info = super().reset(**kwargs)\n",
    "        obs = self._convert_obs(obs)\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Same for step: override to convert the dict observation into an image only.\n",
    "        \"\"\"\n",
    "        obs, reward, done, truncated, info = super().step(action)\n",
    "        obs = self._convert_obs(obs)\n",
    "        return obs, reward, done, truncated, info\n",
    "\n",
    "    def _convert_obs(self, original_obs):\n",
    "        \"\"\"\n",
    "        original_obs is typically {'image':..., 'mission':...}.\n",
    "        We'll just return original_obs['image'] to get a Box(low=0,high=255) shape.\n",
    "        \"\"\"\n",
    "        return original_obs[\"image\"]\n",
    "        #return np.transpose(original_obs[\"image\"], (2, 0, 1))\n",
    "\n",
    "\n",
    "\n",
    "def print_maze_from_config(config):\n",
    "    env = MyCustomMaze(config, render_mode='rgb_array')\n",
    "    env.reset()\n",
    "    full_level_image = env.render()  # This should return an RGB image of the full grid\n",
    "\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(full_level_image)\n",
    "    plt.title(\"Maze Configuration: \" + str(config))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# Generate a random maze and visualize it\n",
    "random_maze = random_config(6)\n",
    "print_maze_from_config(random_maze)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD CODE, NOT VECORIZED, REDUNDANT CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# ====================================================\n",
    "# 4. Main ACCEL Loop\n",
    "# ====================================================\n",
    "\n",
    "def main_accel_demo(total_iterations, replay_prob, train_steps, level_buffer_size,\n",
    "                    initial_fill_size, grid_size):\n",
    "    \n",
    "    \n",
    "    # Create a level buffer to store generated levels and their scores\n",
    "    level_buffer = LevelBuffer(max_size=level_buffer_size)\n",
    "    iteration_regrets = []\n",
    "        \n",
    "    #Create a dummy environment to initialize the model\n",
    "    dummy_env = MyCustomGrid(random_config(grid_size))\n",
    "    vectorized_env = create_vectorized_env(dummy_env, n_envs=4)\n",
    "\n",
    "    # Initialize student model with PPO\n",
    "    print(\"Initializing student model PPO...\")\n",
    "    student_model = initialize_ppo(dummy_env)\n",
    "\n",
    "    skipped = 0\n",
    "\n",
    "    # Populate buffer with initial levels\n",
    "    print(f\"Populating buffer with {initial_fill_size} initial levels with regret != 0...\")\n",
    "    for _ in range(initial_fill_size + skipped):\n",
    "        cfg = random_config(grid_size)\n",
    "        regret = calculate_regret_gae(MyCustomGrid(cfg), student_model, max_steps=1000, gamma=0.99, lam=0.95)\n",
    "        \n",
    "        # Skip levels with 0 regret\n",
    "        if regret == 0:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        level_buffer.add(cfg, regret)\n",
    "        \n",
    "    \n",
    "    print(\"\\nDone. Number of skipped levels with zero regret:\", skipped)\n",
    "    \n",
    "    skipped = 0\n",
    "    iteration = 0\n",
    "    # Main ACCEL loop\n",
    "    print(\"\\nMain ACCEL loop...\")\n",
    "    while iteration < total_iterations + skipped:\n",
    "        print(f\"\\n=== ITERATION {iteration + 1}/{total_iterations} SKIPPED {skipped} ===\")\n",
    "        \n",
    "        iteration += 1\n",
    "        \n",
    "        # Decide whether to use replay or generate a new level\n",
    "        use_replay = np.random.rand() < replay_prob\n",
    "        \n",
    "        # Generates new random levels if you don't use replay\n",
    "        if not use_replay or len(level_buffer.data) == 0:\n",
    "            cfg = random_config(grid_size)\n",
    "            regret = calculate_regret_gae(MyCustomGrid(cfg), student_model, max_steps=100, gamma=0.99, lam=0.95)\n",
    "            \n",
    "            if regret == 0:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            level_buffer.add(cfg, regret)\n",
    "            print(f\"  Sampled new config, regret={regret:.3f}\")\n",
    "        else:\n",
    "            # Replays an existing layer, edits it, and evaluates the new layer\n",
    "            old_cfg = level_buffer.sample_config()\n",
    "            env = MyCustomGrid(old_cfg)\n",
    "            \n",
    "            student_model.set_env(env)\n",
    "            student_model.learn(total_timesteps=train_steps)\n",
    "\n",
    "            new_cfg = edit_config(old_cfg)\n",
    "            regret = calculate_regret_gae(MyCustomGrid(new_cfg), student_model, max_steps=100, gamma=0.99, lam=0.95)\n",
    "            \n",
    "            if regret == 0:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            level_buffer.add(new_cfg, regret)\n",
    "            print(f\"  Replayed + mutated config, regret={regret:.3f}\")\n",
    "        \n",
    "        iteration_regrets.append(regret)\n",
    "    \n",
    "    print(\"\\nDone. Number of skipped levels with zero regret:\", skipped)\n",
    "\n",
    "    # Visualize progress of the regret over iterations.\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(iteration_regrets, marker='o')\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Regret\")\n",
    "    plt.title(\"Regret Progress during ACCEL Training\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    \n",
    "    config = {\n",
    "        \"grid_size\": 8,\n",
    "        \n",
    "        \"total_iterations\": 64,\n",
    "        \"train_steps\": 1024,\n",
    "\n",
    "        \"replay_prob\": 0.7,           # Probability of replaying a level and editing it vs. generating a new one\n",
    "        \"level_buffer_size\": 128,     # Maximum number of levels to store in the buffer\n",
    "        \"initial_fill_size\": 64,      # Number of levels to pre-fill the buffer with\n",
    "        \"regret_threshold\": 0.0,      # Minimum regret threshold to consider a level for the buffer\n",
    "        \n",
    "        \"n_envs\": 8,                  # Number of parallel environments to use for training\n",
    "        \n",
    "        \"edit_levels\": True,          # Whether to edit levels during training i.e. ACCEL or PLR\n",
    "        \"easy_start\": True            # Whether to fill the buffer with easy levels first i.e. minimum number of blocks\n",
    "    }\n",
    "    \n",
    "    print(\"Running ACCEL with config:\")\n",
    "    print(config, \"\\n\")\n",
    "    \n",
    "    main_accel(**config)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Box\n",
    "\n",
    "from minigrid.core.mission import MissionSpace\n",
    "from minigrid.core.world_object import Goal, Wall\n",
    "from minigrid.minigrid_env import MiniGridEnv, Grid\n",
    "\n",
    "from minigrid.wrappers import ImgObsWrapper\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# 1. Custom MiniGrid Environment that returns only the image\n",
    "#    for SB3's PPO (which expects a Box space).\n",
    "# ====================================================\n",
    "class MyCustomGrid(MiniGridEnv):\n",
    "    \"\"\"\n",
    "    Simple MiniGrid environment that places random wall tiles\n",
    "    according to a config dict, returning only the 'image' observation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config=None, **kwargs):\n",
    "        if config is None:\n",
    "            config = {}\n",
    "        self.config = config\n",
    "\n",
    "        # Extract parameters from config\n",
    "        self.width = config.get(\"width\")\n",
    "        self.height = config.get(\"height\")\n",
    "        self.num_blocks = config.get(\"num_blocks\")\n",
    "        self.custom_seed = config.get(\"seed_val\")\n",
    "\n",
    "        # Create a random number generator with the custom seed\n",
    "        self.rng = np.random.default_rng(seed=self.custom_seed)\n",
    "\n",
    "        grid_size = max(self.width, self.height)\n",
    "\n",
    "        mission_space = MissionSpace(mission_func=lambda: \"get to the green goal square\")\n",
    "\n",
    "        super().__init__(\n",
    "            grid_size=grid_size,\n",
    "            max_steps=self.width * self.height * 2, # typical 2x the grid size\n",
    "            see_through_walls=False,\n",
    "            agent_view_size=5,\n",
    "            mission_space=mission_space,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # Manually define our observation_space as a single Box (the image).\n",
    "        self.observation_space = Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=(self.agent_view_size, self.agent_view_size, 3),\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "\n",
    "    def _gen_grid(self, width, height):\n",
    "        \"\"\"\n",
    "        Generate the grid layout for a new episode.\n",
    "        \"\"\"\n",
    "\n",
    "        # Create an empty grid of the \"true\" width x height from config\n",
    "        self.grid = Grid(self.width, self.height)\n",
    "        # Surround the grid with walls\n",
    "        self.grid.wall_rect(0, 0, self.width, self.height)\n",
    "\n",
    "        # Place random walls inside using the custom seed. \n",
    "        for _ in range(self.num_blocks):\n",
    "            r = self.rng.integers(1, self.height - 1)\n",
    "            c = self.rng.integers(1, self.width - 1)\n",
    "            if self.grid.get(c, r) is None:\n",
    "                self.put_obj(Wall(), c, r)\n",
    "\n",
    "        # Place the goal object in a random position not occupied by any wall\n",
    "        while True:\n",
    "            r = self.rng.integers(1, self.height - 1)\n",
    "            c = self.rng.integers(1, self.width - 1)\n",
    "            if self.grid.get(c, r) is None:\n",
    "                self.put_obj(Goal(), c, r)\n",
    "                self.config[\"goal_pos\"] = (c, r)\n",
    "                break\n",
    "\n",
    "        # Place the agent in a random position not occupied by any wall and not on the goal\n",
    "        while True:\n",
    "            r = self.rng.integers(1, self.height - 1)\n",
    "            c = self.rng.integers(1, self.width - 1)\n",
    "            if self.grid.get(c, r) is None:\n",
    "                self.place_agent(top=(c, r), rand_dir=True)\n",
    "                self.config[\"start_pos\"] = (c, r)\n",
    "                break\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = super().reset(**kwargs)\n",
    "        obs = self._convert_obs(obs)\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, truncated, info = super().step(action)\n",
    "        obs = self._convert_obs(obs)\n",
    "        return obs, reward, done, truncated, info\n",
    "\n",
    "    def _convert_obs(self, original_obs):\n",
    "        return original_obs[\"image\"]\n",
    "\n",
    "\n",
    "def random_config(grid_size, num_blocks=None):\n",
    "    max_blocks = int(((grid_size - 1) * (grid_size - 1)) / 2)\n",
    "    \n",
    "    if num_blocks is None:\n",
    "        num_blocks = np.random.randint(1, max_blocks)\n",
    "    else:\n",
    "        num_blocks = min(num_blocks, max_blocks)\n",
    "    \n",
    "    return {\n",
    "        \"width\": grid_size,\n",
    "        \"height\": grid_size,\n",
    "        \"num_blocks\": num_blocks,\n",
    "        \"start_pos\": None,\n",
    "        \"goal_pos\": None,\n",
    "        \"edited\": False,\n",
    "        \"seed_val\": np.random.randint(0, 999999),\n",
    "    }\n",
    "    \n",
    "def edit_config(old_config):\n",
    "    max_blocks = int(((old_config[\"width\"] - 1) * (old_config[\"height\"] - 1)) / 2)\n",
    "    \n",
    "    new_config = dict(old_config)\n",
    "    \n",
    "    # Randomly increase number of blocks by 1\n",
    "    new_number_blocks = old_config[\"num_blocks\"] + np.random.choice([1])\n",
    "    new_config[\"num_blocks\"] = max(1, min(new_number_blocks, max_blocks))    \n",
    "\n",
    "    new_config[\"edited\"] = True\n",
    "    return new_config\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# 2. Simple “level buffer” \n",
    "# ====================================================\n",
    "class LevelBuffer: \n",
    "    def __init__(self, max_size=50):\n",
    "        self.max_size = max_size\n",
    "        self.data = []  # will store (config_dict, score)\n",
    "\n",
    "    def add(self, config, score):\n",
    "        self.data.append((config, score))\n",
    "        if len(self.data) > self.max_size:\n",
    "            self.data.sort(key=lambda x: x[1], reverse=True)\n",
    "            self.data = self.data[: self.max_size]\n",
    "\n",
    "    def sample_config(self): \n",
    "        if len(self.data) == 0:\n",
    "            return None\n",
    "        scores = [item[1] for item in self.data]\n",
    "        total = sum(scores)\n",
    "        if total <= 1e-9:\n",
    "            # fallback to uniform\n",
    "            idx = np.random.randint(len(self.data))\n",
    "            return self.data[idx][0]\n",
    "        probs = [s / total for s in scores]\n",
    "        idx = np.random.choice(len(self.data), p=probs)\n",
    "        return self.data[idx][0]\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# 3. Utility Functions\n",
    "# ====================================================\n",
    "class MinigridFeaturesExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: gym.Space, features_dim: int = 512, normalized_image: bool = False) -> None:\n",
    "        super().__init__(observation_space, features_dim)\n",
    "        n_input_channels = observation_space.shape[0]\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, 16, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, (2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        # Compute shape by doing one forward pass\n",
    "        with torch.no_grad():\n",
    "            sample_input = torch.as_tensor(observation_space.sample()[None]).float()\n",
    "            n_flatten = self.cnn(sample_input).shape[1]\n",
    "\n",
    "        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
    "\n",
    "    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear(self.cnn(observations))\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=MinigridFeaturesExtractor,\n",
    "    features_extractor_kwargs=dict(features_dim=128),\n",
    ")\n",
    "\n",
    "\n",
    "def initialize_ppo(env, learning_rate=1e-4):\n",
    "    return PPO(\n",
    "        \"CnnPolicy\",                    \n",
    "        env,                            \n",
    "        verbose=0,                      \n",
    "        n_steps=256,                    \n",
    "        batch_size=64,                  \n",
    "        learning_rate=learning_rate,\n",
    "        policy_kwargs=policy_kwargs,    \n",
    "        device=device                   \n",
    "    )\n",
    "\n",
    "def print_level_from_config(config):\n",
    "    env = MyCustomGrid(config, render_mode='rgb_array')\n",
    "    env.reset()\n",
    "    full_level_image = env.render()  # This should return an RGB image of the full grid\n",
    "\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(full_level_image)\n",
    "    plt.title(\"Level Configuration: \" + str(config))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "def create_vectorized_env(config, n_envs=4):\n",
    "    env_fns = [lambda: MyCustomGrid(config) for _ in range(n_envs)]\n",
    "    return make_vec_env(lambda: MyCustomGrid(config), n_envs=n_envs)\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# 4. Parallel regret calculation (NEW)\n",
    "# ====================================================\n",
    "def calculate_regret_gae_parallel(\n",
    "    env_config, \n",
    "    model, \n",
    "    max_steps=1000, \n",
    "    gamma=0.99, \n",
    "    lam=0.95, \n",
    "    n_envs=8\n",
    "):\n",
    "    \"\"\"\n",
    "    Roll out n_envs copies of MyCustomGrid(env_config) in parallel,\n",
    "    compute GAE-based 'regret' for each environment, and return the max.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create vectorized env with n_envs copies\n",
    "    vec_env = make_vec_env(lambda: MyCustomGrid(env_config), n_envs=n_envs)\n",
    "    obs_array = vec_env.reset()  # shape: (n_envs, height, width, 3)\n",
    "\n",
    "    # For each environment, we will store transitions to later compute GAE\n",
    "    # We'll keep them in lists, one per environment.\n",
    "    # Alternatively, we can store them in big arrays (n_envs, max_steps), etc.\n",
    "    rewards_list = [[] for _ in range(n_envs)]\n",
    "    values_list = [[] for _ in range(n_envs)]\n",
    "    dones_list = [[] for _ in range(n_envs)]\n",
    "\n",
    "    for t in range(max_steps):\n",
    "        # Model’s predict can handle multiple obs in a single forward pass\n",
    "        actions, _ = model.predict(obs_array, deterministic=True)\n",
    "        \n",
    "        # Also compute the values in one batch\n",
    "        # Convert obs_array to torch tensor\n",
    "        obs_tensor = torch.as_tensor(np.transpose(obs_array, (0, 3, 1, 2))).float().to(device)\n",
    "        with torch.no_grad():\n",
    "            # shape: (n_envs, 1)\n",
    "            value_t = model.policy.predict_values(obs_tensor).cpu().numpy().flatten()\n",
    "\n",
    "        # Step all envs in parallel\n",
    "        next_obs_array, rewards, dones, truncs = vec_env.step(actions)\n",
    "\n",
    "        # Store the results\n",
    "        for i in range(n_envs):\n",
    "            rewards_list[i].append(rewards[i])\n",
    "            values_list[i].append(value_t[i])\n",
    "            dones_list[i].append(dones[i] or truncs[i])\n",
    "\n",
    "        obs_array = next_obs_array\n",
    "\n",
    "        # If all envs are done or truncated, we can break early\n",
    "        if all(dones) or all(truncs):\n",
    "            break\n",
    "\n",
    "    # We also need the terminal value for each env\n",
    "    # (0 if done, otherwise model's value at final obs)\n",
    "    obs_tensor = torch.as_tensor(np.transpose(obs_array, (0, 3, 1, 2))).float().to(device)\n",
    "    with torch.no_grad():\n",
    "        final_values = model.policy.predict_values(obs_tensor).cpu().numpy().flatten()\n",
    "\n",
    "    # Now, compute GAE-based \"regret\" for each of the n_envs\n",
    "    regrets = []\n",
    "    for i in range(n_envs):\n",
    "        # If the env ended with done or truncated, terminal value = 0\n",
    "        if dones_list[i][-1]:\n",
    "            values_list[i].append(0.0)\n",
    "        else:\n",
    "            values_list[i].append(final_values[i])\n",
    "\n",
    "        # Compute delta_t and approximate GAE-like metric\n",
    "        env_rewards = rewards_list[i]\n",
    "        env_values = values_list[i]\n",
    "        env_dones = dones_list[i]\n",
    "\n",
    "        env_regrets = []\n",
    "        for t in range(len(env_rewards)):\n",
    "            delta_t = (\n",
    "                env_rewards[t] \n",
    "                + gamma * env_values[t + 1] * (1 - env_dones[t]) \n",
    "                - env_values[t]\n",
    "            )\n",
    "            # accumulate discounted error\n",
    "            discounted_error = (gamma * lam) ** t * delta_t\n",
    "            env_regrets.append(max(0, discounted_error))\n",
    "\n",
    "        # The environment's \"regret\" is the max of its positive GAE deltas\n",
    "        regrets.append(max(env_regrets) if env_regrets else 0.0)\n",
    "\n",
    "    # Return the maximum regret across the parallel envs\n",
    "    return max(regrets) if regrets else 0.0\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# 5. Main ACCEL loop, now using parallel regret\n",
    "# ====================================================\n",
    "def main_accel(\n",
    "    total_iterations, \n",
    "    replay_prob, \n",
    "    train_steps, \n",
    "    level_buffer_size,\n",
    "    initial_fill_size, \n",
    "    grid_size, \n",
    "    n_envs, \n",
    "    edit_levels, \n",
    "    regret_threshold,\n",
    "    easy_start\n",
    "):\n",
    "    # Create a level buffer\n",
    "    level_buffer = LevelBuffer(max_size=level_buffer_size)\n",
    "    iteration_regrets = []\n",
    "\n",
    "    # Use a random config to create a vectorized environment\n",
    "    dummy_config = random_config(grid_size)\n",
    "    vectorized_env = create_vectorized_env(dummy_config, n_envs=n_envs)\n",
    "\n",
    "    # Initialize PPO with vectorized environment\n",
    "    print(\"Initializing student model PPO...\")\n",
    "    student_model = initialize_ppo(vectorized_env)\n",
    "\n",
    "    # Populate buffer with initial levels\n",
    "    print(f\"Populating buffer with {initial_fill_size} initial levels (regret > {regret_threshold})...\")\n",
    "    while len(level_buffer.data) < initial_fill_size:\n",
    "        if easy_start:\n",
    "            cfg = random_config(grid_size, num_blocks=2)\n",
    "        else:\n",
    "            cfg = random_config(grid_size)\n",
    "\n",
    "        # *** Use parallel regret calc here (1 env is also valid if you prefer) ***\n",
    "        regret = calculate_regret_gae_parallel(\n",
    "            cfg, student_model, \n",
    "            max_steps=1000, gamma=0.99, lam=0.95, \n",
    "            n_envs=n_envs  # you can tune this\n",
    "        )\n",
    "\n",
    "        if regret < regret_threshold:\n",
    "            continue\n",
    "\n",
    "        level_buffer.add(cfg, regret)\n",
    "\n",
    "    # Main ACCEL loop\n",
    "    iteration, skipped = 0, 0\n",
    "    print(\"\\nMain training loop...\")\n",
    "    while iteration < total_iterations + skipped:\n",
    "        print(f\"\\n=== ITERATION {iteration + 1}/{total_iterations + skipped} SKIPPED: {skipped} ===\")\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "        # Decide whether to replay or generate a new level\n",
    "        use_replay = np.random.rand() < replay_prob\n",
    "\n",
    "        if not use_replay or len(level_buffer.data) == 0:\n",
    "            cfg = random_config(grid_size)\n",
    "            print(\"Generated new random level:\", cfg)\n",
    "        else:\n",
    "            cfg = level_buffer.sample_config()\n",
    "            print(\"Sampled level from buffer:\", cfg)\n",
    "\n",
    "        # Update the vectorized environment with the selected config\n",
    "        vectorized_env = create_vectorized_env(cfg, n_envs=n_envs)\n",
    "        student_model.set_env(vectorized_env)\n",
    "\n",
    "        # Train the student model\n",
    "        student_model.learn(total_timesteps=train_steps)\n",
    "\n",
    "        if not use_replay or not edit_levels:\n",
    "            regret = calculate_regret_gae_parallel(\n",
    "                cfg, student_model, \n",
    "                max_steps=1000, gamma=0.99, lam=0.95,\n",
    "                n_envs=n_envs  # parallel\n",
    "            )\n",
    "        else:\n",
    "            # Edit the level and calculate regret\n",
    "            cfg = edit_config(cfg)\n",
    "            print(\"Edited level to:\", cfg)\n",
    "            regret = calculate_regret_gae_parallel(\n",
    "                cfg, student_model, \n",
    "                max_steps=1000, gamma=0.99, lam=0.95,\n",
    "                n_envs=n_envs\n",
    "            )\n",
    "\n",
    "        if regret <= regret_threshold:\n",
    "            print(f\"Regret={regret:.5f} <= threshold={regret_threshold}. Skipping...\")\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        print(f\"Regret for current level: {regret}\")\n",
    "        level_buffer.add(cfg, regret)\n",
    "        iteration_regrets.append(regret)\n",
    "\n",
    "    # Plot the progress\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(iteration_regrets, marker='o')\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Regret\")\n",
    "    plt.title(\"Regret Progress during ACCEL Training\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nDone. Final buffer size:\", len(level_buffer.data))\n",
    "    print(\"Top-5 hardest levels (config, regret):\")\n",
    "    level_buffer.data.sort(key=lambda x: x[1], reverse=True)\n",
    "    for i, (cfg, sc) in enumerate(level_buffer.data[:5]):\n",
    "        print(f\"{i + 1}. regret={sc:.5f}, config={cfg}\")\n",
    "\n",
    "    print(\"Top-5 easiest levels (config, regret):\")\n",
    "    level_buffer.data.sort(key=lambda x: x[1])\n",
    "    for i, (cfg, sc) in enumerate(level_buffer.data[:5]):\n",
    "        print(f\"{i + 1}. regret={sc:.5f}, config={cfg}\")\n",
    "\n",
    "    return student_model\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Example of how you might run:\n",
    "# ============================\n",
    "if __name__ == \"__main__\":\n",
    "    trained_model = main_accel(\n",
    "        total_iterations=5,\n",
    "        replay_prob=0.5,\n",
    "        train_steps=5000,\n",
    "        level_buffer_size=10,\n",
    "        initial_fill_size=5,\n",
    "        grid_size=8,\n",
    "        n_envs=4,             # we run 4 environments in parallel\n",
    "        edit_levels=True,\n",
    "        regret_threshold=0.1,\n",
    "        easy_start=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "accel_env_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
