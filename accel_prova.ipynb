{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACCEL IMPLEMENTATION TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "import gymnasium.spaces as spaces\n",
    "\n",
    "from minigrid.core.mission import MissionSpace\n",
    "from minigrid.core.world_object import Goal, Wall\n",
    "from minigrid.minigrid_env import MiniGridEnv, Grid\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 32.7     |\n",
      "|    ep_rew_mean     | 0.379    |\n",
      "| time/              |          |\n",
      "|    fps             | 499      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 128      |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 37.7          |\n",
      "|    ep_rew_mean          | 0.272         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 393           |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 0             |\n",
      "|    total_timesteps      | 256           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4635734e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.95         |\n",
      "|    explained_variance   | -0.0436       |\n",
      "|    learning_rate        | 0.0001        |\n",
      "|    loss                 | 0.0109        |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | -0.000228     |\n",
      "|    value_loss           | 0.0348        |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 41.1          |\n",
      "|    ep_rew_mean          | 0.216         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 397           |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 0             |\n",
      "|    total_timesteps      | 384           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.8272549e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.95         |\n",
      "|    explained_variance   | -0.0987       |\n",
      "|    learning_rate        | 0.0001        |\n",
      "|    loss                 | 0.0057        |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.000175     |\n",
      "|    value_loss           | 0.0129        |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 42.7         |\n",
      "|    ep_rew_mean          | 0.176        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 389          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 1            |\n",
      "|    total_timesteps      | 512          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.047047e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.95        |\n",
      "|    explained_variance   | 0.119        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | -5.4e-05     |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.000201    |\n",
      "|    value_loss           | 0.000724     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 44.3          |\n",
      "|    ep_rew_mean          | 0.139         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 401           |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 1             |\n",
      "|    total_timesteps      | 640           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4207326e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.95         |\n",
      "|    explained_variance   | -1.05         |\n",
      "|    learning_rate        | 0.0001        |\n",
      "|    loss                 | -0.00032      |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -0.00013      |\n",
      "|    value_loss           | 0.000126      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 44.9          |\n",
      "|    ep_rew_mean          | 0.126         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 400           |\n",
      "|    iterations           | 6             |\n",
      "|    time_elapsed         | 1             |\n",
      "|    total_timesteps      | 768           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.0931475e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.95         |\n",
      "|    explained_variance   | 0.096         |\n",
      "|    learning_rate        | 0.0001        |\n",
      "|    loss                 | -1.74e-06     |\n",
      "|    n_updates            | 50            |\n",
      "|    policy_gradient_loss | -0.000139     |\n",
      "|    value_loss           | 0.000144      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 42.6         |\n",
      "|    ep_rew_mean          | 0.171        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 404          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 896          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.662426e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.95        |\n",
      "|    explained_variance   | -0.193       |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | -0.000453    |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.000317    |\n",
      "|    value_loss           | 0.00115      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 43.3          |\n",
      "|    ep_rew_mean          | 0.156         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 410           |\n",
      "|    iterations           | 8             |\n",
      "|    time_elapsed         | 2             |\n",
      "|    total_timesteps      | 1024          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7792918e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.95         |\n",
      "|    explained_variance   | 0.0117        |\n",
      "|    learning_rate        | 0.0001        |\n",
      "|    loss                 | 0.0191        |\n",
      "|    n_updates            | 70            |\n",
      "|    policy_gradient_loss | -0.000103     |\n",
      "|    value_loss           | 0.0407        |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 42.2          |\n",
      "|    ep_rew_mean          | 0.177         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 404           |\n",
      "|    iterations           | 9             |\n",
      "|    time_elapsed         | 2             |\n",
      "|    total_timesteps      | 1152          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3667159e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.95         |\n",
      "|    explained_variance   | -0.141        |\n",
      "|    learning_rate        | 0.0001        |\n",
      "|    loss                 | -0.000174     |\n",
      "|    n_updates            | 80            |\n",
      "|    policy_gradient_loss | -0.000125     |\n",
      "|    value_loss           | 0.000707      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 252\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. regret=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, config=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 252\u001b[0m     \u001b[43mmain_accel_demo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 211\u001b[0m, in \u001b[0;36mmain_accel_demo\u001b[1;34m(total_iterations, replay_prob, train_steps)\u001b[0m\n\u001b[0;32m    209\u001b[0m     env \u001b[38;5;241m=\u001b[39m MyCustomGrid(cfg)\n\u001b[0;32m    210\u001b[0m     teacher_model\u001b[38;5;241m.\u001b[39mset_env(env)\n\u001b[1;32m--> 211\u001b[0m     \u001b[43mteacher_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;66;03m# Populate buffer with initial levels\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\asus\\miniconda3\\envs\\accel\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\asus\\miniconda3\\envs\\accel\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:323\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 323\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[0;32m    326\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\asus\\miniconda3\\envs\\accel\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:218\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    214\u001b[0m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[0;32m    216\u001b[0m         clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[1;32m--> 218\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[0;32m    222\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\asus\\miniconda3\\envs\\accel\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \n\u001b[0;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\asus\\miniconda3\\envs\\accel\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\vec_transpose.py:97\u001b[0m, in \u001b[0;36mVecTransposeImage.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m---> 97\u001b[0m     observations, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;66;03m# Transpose the terminal observations\u001b[39;00m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, done \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dones):\n",
      "File \u001b[1;32mc:\\Users\\asus\\miniconda3\\envs\\accel\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:70\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx]:\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;66;03m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterminal_observation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m obs\n\u001b[1;32m---> 70\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_obs(env_idx, obs)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obs_from_buf(), np\u001b[38;5;241m.\u001b[39mcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews), np\u001b[38;5;241m.\u001b[39mcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones), deepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos))\n",
      "File \u001b[1;32mc:\\Users\\asus\\miniconda3\\envs\\accel\\Lib\\site-packages\\stable_baselines3\\common\\monitor.py:83\u001b[0m, in \u001b[0;36mMonitor.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected you to pass keyword argument \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m into reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_reset_info[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 83\u001b[0m, in \u001b[0;36mMyCustomGrid.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     79\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;124;03m    Override reset to ensure we only return the 'image' array\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;124;03m    instead of a dict with 'image' and 'mission'.\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m     obs, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m     obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_obs(obs)\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obs, info\n",
      "File \u001b[1;32mc:\\Users\\asus\\miniconda3\\envs\\accel\\Lib\\site-packages\\minigrid\\minigrid_env.py:132\u001b[0m, in \u001b[0;36mMiniGridEnv.reset\u001b[1;34m(self, seed, options)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;66;03m# Generate a new random grid at the start of each episode\u001b[39;00m\n\u001b[1;32m--> 132\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gen_grid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# These fields should be defined by _gen_grid\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_pos \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_pos, \u001b[38;5;28mtuple\u001b[39m)\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_pos \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_dir \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    139\u001b[0m )\n",
      "Cell \u001b[1;32mIn[10], line 76\u001b[0m, in \u001b[0;36mMyCustomGrid._gen_grid\u001b[1;34m(self, width, height)\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplace_agent()\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Place a goal object\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplace_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGoal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\asus\\miniconda3\\envs\\accel\\Lib\\site-packages\\minigrid\\minigrid_env.py:353\u001b[0m, in \u001b[0;36mMiniGridEnv.place_obj\u001b[1;34m(self, obj, top, size, reject_fn, max_tries)\u001b[0m\n\u001b[0;32m    347\u001b[0m pos \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rand_int(top[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mmin\u001b[39m(top[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m size[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid\u001b[38;5;241m.\u001b[39mwidth)),\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rand_int(top[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mmin\u001b[39m(top[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m size[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid\u001b[38;5;241m.\u001b[39mheight)),\n\u001b[0;32m    350\u001b[0m )\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# Don't place the object on top of another object\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpos\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    354\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;66;03m# Don't place the object where the agent is\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\asus\\miniconda3\\envs\\accel\\Lib\\site-packages\\minigrid\\core\\grid.py:74\u001b[0m, in \u001b[0;36mGrid.get\u001b[1;34m(self, i, j)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m j \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheight\n\u001b[0;32m     71\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrow index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m outside of grid of height \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheight\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid[j \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;241m+\u001b[39m i] \u001b[38;5;241m=\u001b[39m v\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, i: \u001b[38;5;28mint\u001b[39m, j: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m WorldObj \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m j \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheight\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# 1. Custom MiniGrid Environment that returns only the image\n",
    "#    for SB3's PPO (which expects a Box space).\n",
    "# ====================================================\n",
    "class MyCustomGrid(MiniGridEnv):\n",
    "    \"\"\"\n",
    "    Simple MiniGrid environment that places random wall tiles\n",
    "    according to a config dict, returning only the 'image' observation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config=None, **kwargs):\n",
    "        if config is None:\n",
    "            config = {}\n",
    "        self.config = config\n",
    "\n",
    "        # Extract parameters from config\n",
    "        self.width = config.get(\"width\", 8)\n",
    "        self.height = config.get(\"height\", 8)\n",
    "        self.num_blocks = config.get(\"num_blocks\", 5)\n",
    "        self.custom_seed = config.get(\"seed_val\", None)\n",
    "        self.agent_start = config.get(\"agent_start\", None)\n",
    "\n",
    "        # For older MiniGrid, we pass 'grid_size' not 'width'/'height' to the parent.\n",
    "        # We'll pick one dimension to define 'grid_size'; but note we manually place walls to match 'width' and 'height' in _gen_grid.\n",
    "        # For simplicity, let's just do: grid_size = max(width, height)\n",
    "        grid_size = max(self.width, self.height)\n",
    "\n",
    "        mission_space = MissionSpace(mission_func=lambda: \"get to the green goal square\")\n",
    "\n",
    "        super().__init__(\n",
    "            grid_size=grid_size,\n",
    "            max_steps=self.width * self.height * 2,\n",
    "            see_through_walls=False,\n",
    "            agent_view_size=5,\n",
    "            mission_space=mission_space,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # Manually define our observation_space as a single Box (the image).\n",
    "        # By default, MiniGrid's image shape is (view_size, view_size, 3) if using partial obs,\n",
    "        # or (height, width, 3) if using full-grid observation. We'll do full-grid here:\n",
    "        # We'll define (self.height, self.width, 3) as the shape.\n",
    "        # In practice, \"image\" shape can vary if partial observations are used.\n",
    "        self.observation_space = Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=(self.height, self.width, 3),\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "\n",
    "    def _gen_grid(self, width, height):\n",
    "        \"\"\"\n",
    "        Generate the grid layout for a new episode.\n",
    "        We'll use self.width, self.height from config\n",
    "        but the underlying minigrid might store its own grid_size.\n",
    "        \"\"\"\n",
    "        # Create an empty grid of the \"true\" width x height from config\n",
    "        self.grid = Grid(self.width, self.height)\n",
    "        # Surround with walls\n",
    "        self.grid.wall_rect(0, 0, self.width, self.height)\n",
    "\n",
    "        # Place random walls inside\n",
    "        for _ in range(self.num_blocks):\n",
    "            r = self._rand_int(1, self.height - 1)\n",
    "            c = self._rand_int(1, self.width - 1)\n",
    "            self.put_obj(Wall(), c, r)\n",
    "\n",
    "        # Place the agent in a specific position or random\n",
    "        if self.agent_start is not None:\n",
    "            ax, ay = self.agent_start\n",
    "            self.place_agent(top=(ax, ay), size=(1, 1), rand_dir=False)\n",
    "        else:\n",
    "            self.place_agent()\n",
    "\n",
    "        # Place a goal object\n",
    "        self.place_obj(Goal())\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Override reset to ensure we only return the 'image' array\n",
    "        instead of a dict with 'image' and 'mission'.\n",
    "        \"\"\"\n",
    "        obs, info = super().reset(**kwargs)\n",
    "        obs = self._convert_obs(obs)\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Same for step: override to convert the dict observation into an image only.\n",
    "        \"\"\"\n",
    "        obs, reward, done, truncated, info = super().step(action)\n",
    "        obs = self._convert_obs(obs)\n",
    "        return obs, reward, done, truncated, info\n",
    "\n",
    "    def _convert_obs(self, original_obs):\n",
    "        \"\"\"\n",
    "        original_obs is typically {'image':..., 'mission':...}.\n",
    "        We'll just return original_obs['image'] to get a Box(low=0,high=255) shape.\n",
    "        \"\"\"\n",
    "        return original_obs[\"image\"]\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# 2. Simple “level buffer” \n",
    "# ====================================================\n",
    "# class to memorize generated levels and score\n",
    "class LevelBuffer: \n",
    "    def __init__(self, max_size=50):\n",
    "        self.max_size = max_size\n",
    "        self.data = []  # will store (config_dict, score)\n",
    "\n",
    "    def add(self, config, score):\n",
    "        self.data.append((config, score))\n",
    "        if len(self.data) > self.max_size:\n",
    "            self.data.sort(key=lambda x: x[1], reverse=True)\n",
    "            self.data = self.data[: self.max_size]\n",
    "            #it memorize only the highest score for each level\n",
    "\n",
    "    def sample_config(self): \n",
    "        # Samples a level from the buffer, weighting the probabilities \n",
    "        # based on the scores.\n",
    "        if len(self.data) == 0:\n",
    "            return None\n",
    "        scores = [item[1] for item in self.data]\n",
    "        total = sum(scores)\n",
    "        if total <= 1e-9:\n",
    "            # fallback to uniform\n",
    "            idx = np.random.randint(len(self.data))\n",
    "            return self.data[idx][0]\n",
    "        probs = [s / total for s in scores]\n",
    "        idx = np.random.choice(len(self.data), p=probs)\n",
    "        return self.data[idx][0]\n",
    "\n",
    "# ====================================================\n",
    "# 3. Utility Functions\n",
    "# ====================================================\n",
    "def random_config():\n",
    "    return {\n",
    "        \"width\": 5, # np.random.randint(5, 10)\n",
    "        \"height\": 5, # np.random.randint(5, 10)\n",
    "        \"num_blocks\": np.random.randint(0, 15),\n",
    "        \"seed_val\": np.random.randint(0, 999999),\n",
    "        # \"agent_start\": (x, y)\n",
    "    }\n",
    "# Modify an existing configuration, adding randomness.\n",
    "def edit_config(old_config):\n",
    "    new_config = dict(old_config)\n",
    "    new_config[\"num_blocks\"] = max(0, old_config[\"num_blocks\"] + np.random.choice([-2, -1, 1, 2]))\n",
    "    new_config[\"seed_val\"] = np.random.randint(0, 999999)\n",
    "    return new_config\n",
    "\n",
    "def calculate_regret(config, student_model,teacher_model, max_steps=50):\n",
    "    \"\"\"\n",
    "    Calculate regret as the difference between the teacher's performance\n",
    "    and the student's performance on the same level.\n",
    "    \"\"\"\n",
    "    env = MyCustomGrid(config)\n",
    "    #Teacher rollout \n",
    "    obs, _ = env.reset()\n",
    "    teacher_total_reward = 0\n",
    "    for _ in range(max_steps):\n",
    "        action, _ = teacher_model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, truncated, _ = env.step(action)\n",
    "        teacher_total_reward += reward\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "    #Student rollout\n",
    "    obs, _ = env.reset()\n",
    "    student_total_reward = 0\n",
    "    for _ in range(max_steps):\n",
    "        action, _ = student_model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, truncated, _ = env.step(action)\n",
    "        student_total_reward += reward\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "    return max(0, teacher_total_reward - student_total_reward)\n",
    "\n",
    "def initialize_ppo(env, learning_rate=1e-4):\n",
    "    return PPO(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        verbose=1,\n",
    "        n_steps=128,\n",
    "        batch_size=64,\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# 4. Main ACCEL Loop\n",
    "# ====================================================\n",
    "\n",
    "def main_accel_demo(total_iterations=30, replay_prob=0.7, train_steps=2000):\n",
    "    #Create a dummy environment to initialize the model\n",
    "    dummy_env = MyCustomGrid(config={\"width\": 5, \"height\": 5, \"num_blocks\": 1})\n",
    "    dummy_env.reset()\n",
    "    # Initialize teacher and student models with logging\n",
    "    teacher_model = initialize_ppo(dummy_env)\n",
    "    student_model = initialize_ppo(dummy_env)\n",
    "    # creates a layer buffer.\n",
    "    level_buffer = LevelBuffer(max_size=50)\n",
    "    iteration_regrets = []\n",
    "\n",
    "    # Pretrain teacher on a set of random levels\n",
    "    for _ in range(10):\n",
    "        cfg = random_config()\n",
    "        env = MyCustomGrid(cfg)\n",
    "        teacher_model.set_env(env)\n",
    "        teacher_model.learn(total_timesteps=train_steps)\n",
    "\n",
    "    # Populate buffer with initial levels\n",
    "    for _ in range(10):\n",
    "        cfg = random_config()\n",
    "        regret = calculate_regret(cfg, student_model, teacher_model)\n",
    "        level_buffer.add(cfg, regret)\n",
    "\n",
    "    for iteration in range(total_iterations):\n",
    "        print(f\"\\n=== ITERATION {iteration + 1}/{total_iterations} ===\")\n",
    "        use_replay = np.random.rand() < replay_prob\n",
    "        # Generates new random levels if you don't use replay\n",
    "        if not use_replay or len(level_buffer.data) == 0:\n",
    "            cfg = random_config()\n",
    "            regret = calculate_regret(cfg, student_model, teacher_model)\n",
    "            level_buffer.add(cfg, regret)\n",
    "            print(f\"  Sampled new config, regret={regret:.3f}\")\n",
    "        else:\n",
    "            # Replays an existing layer, edits it, and evaluates the new layer\n",
    "            old_cfg = level_buffer.sample_config()\n",
    "            env = MyCustomGrid(old_cfg)\n",
    "            student_model.set_env(env)\n",
    "            student_model.learn(total_timesteps=train_steps)\n",
    "\n",
    "            new_cfg = edit_config(old_cfg)\n",
    "            regret = calculate_regret(new_cfg, student_model, teacher_model)\n",
    "            level_buffer.add(new_cfg, regret)\n",
    "            print(f\"  Replayed + mutated config, regret={regret:.3f}\")\n",
    "        \n",
    "        iteration_regrets.append(regret)\n",
    "\n",
    "    # Visualize progress\n",
    "    plot_progress(total_iterations, iteration_regrets)\n",
    "    \n",
    "    print(\"\\nDone. Final buffer size:\", len(level_buffer.data))\n",
    "    print(\"Top-5 hardest levels (config, regret):\")\n",
    "    level_buffer.data.sort(key=lambda x: x[1], reverse=True)\n",
    "    for i, (cfg, sc) in enumerate(level_buffer.data[:5]):\n",
    "        print(f\"{i + 1}. regret={sc:.3f}, config={cfg}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_accel_demo()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "accel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
