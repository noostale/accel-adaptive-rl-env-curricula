{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACCEL IMPLEMENTATION TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "import gymnasium.spaces as spaces\n",
    "\n",
    "from minigrid.core.mission import MissionSpace\n",
    "from minigrid.core.world_object import Goal, Wall\n",
    "from minigrid.minigrid_env import MiniGridEnv, Grid\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'configure' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 253\u001b[0m\n\u001b[0;32m    250\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. regret=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, config=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 253\u001b[0m     \u001b[43mmain_accel_demo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 207\u001b[0m, in \u001b[0;36mmain_accel_demo\u001b[1;34m(total_iterations, replay_prob, train_steps, log_dir)\u001b[0m\n\u001b[0;32m    205\u001b[0m dummy_env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m    206\u001b[0m \u001b[38;5;66;03m# Initialize teacher and student models with logging\u001b[39;00m\n\u001b[1;32m--> 207\u001b[0m teacher_model \u001b[38;5;241m=\u001b[39m \u001b[43minitialize_ppo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdummy_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlog_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/teacher\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    208\u001b[0m student_model \u001b[38;5;241m=\u001b[39m initialize_ppo(dummy_env, log_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlog_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/student\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    209\u001b[0m \u001b[38;5;66;03m# creates a layer buffer.\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 185\u001b[0m, in \u001b[0;36minitialize_ppo\u001b[1;34m(env, log_dir, learning_rate)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03mInitialize a PPO model with optional logging to a directory.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m log_dir:\n\u001b[1;32m--> 185\u001b[0m     logger \u001b[38;5;241m=\u001b[39m \u001b[43mconfigure\u001b[49m(log_dir, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstdout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorboard\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    186\u001b[0m     PPO\u001b[38;5;241m.\u001b[39mlogger \u001b[38;5;241m=\u001b[39m logger\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m PPO(\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    190\u001b[0m     env,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    194\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate\n\u001b[0;32m    195\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'configure' is not defined"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# 1. Custom MiniGrid Environment that returns only the image\n",
    "#    for SB3's PPO (which expects a Box space).\n",
    "# ====================================================\n",
    "class MyCustomGrid(MiniGridEnv):\n",
    "    \"\"\"\n",
    "    Simple MiniGrid environment that places random wall tiles\n",
    "    according to a config dict, returning only the 'image' observation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config=None, **kwargs):\n",
    "        if config is None:\n",
    "            config = {}\n",
    "        self.config = config\n",
    "\n",
    "        # Extract parameters from config\n",
    "        self.width = config.get(\"width\", 8)\n",
    "        self.height = config.get(\"height\", 8)\n",
    "        self.num_blocks = config.get(\"num_blocks\", 5)\n",
    "        self.custom_seed = config.get(\"seed_val\", None)\n",
    "        self.agent_start = config.get(\"agent_start\", None)\n",
    "\n",
    "        # For older MiniGrid, we pass 'grid_size' not 'width'/'height' to the parent.\n",
    "        # We'll pick one dimension to define 'grid_size'; but note we manually place walls to match 'width' and 'height' in _gen_grid.\n",
    "        # For simplicity, let's just do: grid_size = max(width, height)\n",
    "        grid_size = max(self.width, self.height)\n",
    "\n",
    "        mission_space = MissionSpace(mission_func=lambda: \"get to the green goal square\")\n",
    "\n",
    "        super().__init__(\n",
    "            grid_size=grid_size,\n",
    "            max_steps=self.width * self.height * 2,\n",
    "            see_through_walls=False,\n",
    "            agent_view_size=5,\n",
    "            mission_space=mission_space,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # Manually define our observation_space as a single Box (the image).\n",
    "        # By default, MiniGrid's image shape is (view_size, view_size, 3) if using partial obs,\n",
    "        # or (height, width, 3) if using full-grid observation. We'll do full-grid here:\n",
    "        # We'll define (self.height, self.width, 3) as the shape.\n",
    "        # In practice, \"image\" shape can vary if partial observations are used.\n",
    "        self.observation_space = Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=(self.height, self.width, 3),\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "\n",
    "    def _gen_grid(self, width, height):\n",
    "        \"\"\"\n",
    "        Generate the grid layout for a new episode.\n",
    "        We'll use self.width, self.height from config\n",
    "        but the underlying minigrid might store its own grid_size.\n",
    "        \"\"\"\n",
    "        # Create an empty grid of the \"true\" width x height from config\n",
    "        self.grid = Grid(self.width, self.height)\n",
    "        # Surround with walls\n",
    "        self.grid.wall_rect(0, 0, self.width, self.height)\n",
    "\n",
    "        # Place random walls inside\n",
    "        for _ in range(self.num_blocks):\n",
    "            r = self._rand_int(1, self.height - 1)\n",
    "            c = self._rand_int(1, self.width - 1)\n",
    "            self.put_obj(Wall(), c, r)\n",
    "\n",
    "        # Place the agent in a specific position or random\n",
    "        if self.agent_start is not None:\n",
    "            ax, ay = self.agent_start\n",
    "            self.place_agent(top=(ax, ay), size=(1, 1), rand_dir=False)\n",
    "        else:\n",
    "            self.place_agent()\n",
    "\n",
    "        # Place a goal object\n",
    "        self.place_obj(Goal())\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Override reset to ensure we only return the 'image' array\n",
    "        instead of a dict with 'image' and 'mission'.\n",
    "        \"\"\"\n",
    "        obs, info = super().reset(**kwargs)\n",
    "        obs = self._convert_obs(obs)\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Same for step: override to convert the dict observation into an image only.\n",
    "        \"\"\"\n",
    "        obs, reward, done, truncated, info = super().step(action)\n",
    "        obs = self._convert_obs(obs)\n",
    "        return obs, reward, done, truncated, info\n",
    "\n",
    "    def _convert_obs(self, original_obs):\n",
    "        \"\"\"\n",
    "        original_obs is typically {'image':..., 'mission':...}.\n",
    "        We'll just return original_obs['image'] to get a Box(low=0,high=255) shape.\n",
    "        \"\"\"\n",
    "        return original_obs[\"image\"]\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# 2. Simple “level buffer” \n",
    "# ====================================================\n",
    "# class to memorize generated levels and score\n",
    "class LevelBuffer: \n",
    "    def __init__(self, max_size=50):\n",
    "        self.max_size = max_size\n",
    "        self.data = []  # will store (config_dict, score)\n",
    "\n",
    "    def add(self, config, score):\n",
    "        self.data.append((config, score))\n",
    "        if len(self.data) > self.max_size:\n",
    "            self.data.sort(key=lambda x: x[1], reverse=True)\n",
    "            self.data = self.data[: self.max_size]\n",
    "            #it memorize only the highest score for each level\n",
    "\n",
    "    def sample_config(self): \n",
    "        # Samples a level from the buffer, weighting the probabilities \n",
    "        # based on the scores.\n",
    "        if len(self.data) == 0:\n",
    "            return None\n",
    "        scores = [item[1] for item in self.data]\n",
    "        total = sum(scores)\n",
    "        if total <= 1e-9:\n",
    "            # fallback to uniform\n",
    "            idx = np.random.randint(len(self.data))\n",
    "            return self.data[idx][0]\n",
    "        probs = [s / total for s in scores]\n",
    "        idx = np.random.choice(len(self.data), p=probs)\n",
    "        return self.data[idx][0]\n",
    "\n",
    "# ====================================================\n",
    "# 3. Utility Functions\n",
    "# ====================================================\n",
    "def random_config():\n",
    "    return {\n",
    "        \"width\": 5, # np.random.randint(5, 10)\n",
    "        \"height\": 5, # np.random.randint(5, 10)\n",
    "        \"num_blocks\": np.random.randint(0, 15),\n",
    "        \"seed_val\": np.random.randint(0, 999999),\n",
    "        # \"agent_start\": (x, y)\n",
    "    }\n",
    "# Modify an existing configuration, adding randomness.\n",
    "def edit_config(old_config):\n",
    "    new_config = dict(old_config)\n",
    "    new_config[\"num_blocks\"] = max(0, old_config[\"num_blocks\"] + np.random.choice([-2, -1, 1, 2]))\n",
    "    new_config[\"seed_val\"] = np.random.randint(0, 999999)\n",
    "    return new_config\n",
    "\n",
    "def calculate_regret(config, student_model,teacher_model, max_steps=200):\n",
    "    \"\"\"\n",
    "    Calculate regret as the difference between the teacher's performance\n",
    "    and the student's performance on the same level.\n",
    "    \"\"\"\n",
    "    env = MyCustomGrid(config)\n",
    "    #Teacher rollout \n",
    "    obs, _ = env.reset()\n",
    "    teacher_total_reward = 0\n",
    "    for _ in range(max_steps):\n",
    "        action, _ = teacher_model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, truncated, _ = env.step(action)\n",
    "        teacher_total_reward += reward\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "    #Student rollout\n",
    "    obs, _ = env.reset()\n",
    "    student_total_reward = 0\n",
    "    for _ in range(max_steps):\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, truncated, _ = env.step(action)\n",
    "        student_total_reward += reward\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "    return max(0, teacher_total_reward - student_total_reward)\n",
    "\n",
    "def initialize_ppo(env, learning_rate=1e-4):\n",
    "    return PPO(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        verbose=1,\n",
    "        n_steps=128,\n",
    "        batch_size=64,\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# 4. Main ACCEL Loop\n",
    "# ====================================================\n",
    "\n",
    "def main_accel_demo(total_iterations=30, replay_prob=0.7, train_steps=2000):\n",
    "    #Create a dummy environment to initialize the model\n",
    "    dummy_env = MyCustomGrid(config={\"width\": 5, \"height\": 5, \"num_blocks\": 1})\n",
    "    dummy_env.reset()\n",
    "    # Initialize teacher and student models with logging\n",
    "    teacher_model = initialize_ppo(dummy_env)\n",
    "    student_model = initialize_ppo(dummy_env)\n",
    "    # creates a layer buffer.\n",
    "    level_buffer = LevelBuffer(max_size=50)\n",
    "    iteration_regrets = []\n",
    "\n",
    "    # Pretrain teacher on a set of random levels\n",
    "    for _ in range(10):\n",
    "        cfg = random_config()\n",
    "        env = MyCustomGrid(cfg)\n",
    "        teacher_model.set_env(env)\n",
    "        teacher_model.learn(total_timesteps=train_steps)\n",
    "\n",
    "    # Populate buffer with initial levels\n",
    "    for _ in range(10):\n",
    "        cfg = random_config()\n",
    "        regret = calculate_regret(cfg, student_model, teacher_model)\n",
    "        level_buffer.add(cfg, regret)\n",
    "\n",
    "    for iteration in range(total_iterations):\n",
    "        print(f\"\\n=== ITERATION {iteration + 1}/{total_iterations} ===\")\n",
    "        use_replay = np.random.rand() < replay_prob\n",
    "        # Generates new random levels if you don't use replay\n",
    "        if not use_replay or len(level_buffer.data) == 0:\n",
    "            cfg = random_config()\n",
    "            regret = calculate_regret(cfg, student_model, teacher_model)\n",
    "            level_buffer.add(cfg, regret)\n",
    "            print(f\"  Sampled new config, regret={regret:.3f}\")\n",
    "        else:\n",
    "            # Replays an existing layer, edits it, and evaluates the new layer\n",
    "            old_cfg = level_buffer.sample_config()\n",
    "            env = MyCustomGrid(old_cfg)\n",
    "            student_model.set_env(env)\n",
    "            student_model.learn(total_timesteps=train_steps)\n",
    "\n",
    "            new_cfg = edit_config(old_cfg)\n",
    "            regret = calculate_regret(new_cfg, student_model, teacher_model)\n",
    "            level_buffer.add(new_cfg, regret)\n",
    "            print(f\"  Replayed + mutated config, regret={regret:.3f}\")\n",
    "        \n",
    "        iteration_regrets.append(regret)\n",
    "\n",
    "    # Visualize progress\n",
    "    plot_progress(total_iterations, iteration_regrets)\n",
    "    \n",
    "    print(\"\\nDone. Final buffer size:\", len(level_buffer.data))\n",
    "    print(\"Top-5 hardest levels (config, regret):\")\n",
    "    level_buffer.data.sort(key=lambda x: x[1], reverse=True)\n",
    "    for i, (cfg, sc) in enumerate(level_buffer.data[:5]):\n",
    "        print(f\"{i + 1}. regret={sc:.3f}, config={cfg}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_accel_demo()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "accel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
