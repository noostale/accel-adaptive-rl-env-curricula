{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACCEL IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import wandb\n",
    "import imageio\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from dataclasses import dataclass, asdict\n",
    "import copy\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from collections import deque\n",
    "from gymnasium.spaces import Box\n",
    "\n",
    "from minigrid.core.mission import MissionSpace\n",
    "from minigrid.core.world_object import Goal, Wall\n",
    "from minigrid.minigrid_env import MiniGridEnv, Grid\n",
    "from minigrid.wrappers import ImgObsWrapper\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "\n",
    "device = 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ====================================================\n",
    "# Custom MiniGrid Environment \n",
    "# ====================================================\n",
    "class MyCustomGrid(MiniGridEnv):\n",
    "    \"\"\"\n",
    "    Simple MiniGrid environment that places random wall tiles\n",
    "    according to a config dict, returning only the 'image' observation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config=None, solvable_only=False, **kwargs):\n",
    "        if config is None:\n",
    "            config = {}\n",
    "        self.config = config\n",
    "        self.solvable_only = solvable_only\n",
    "\n",
    "        # Create a random number generator with the custom seed\n",
    "        self.rng = np.random.default_rng(seed=self.config.get(\"seed_val\"))\n",
    "\n",
    "        mission_space = MissionSpace(mission_func=lambda: \"get to the green goal square\")\n",
    "\n",
    "        super().__init__(\n",
    "            grid_size=self.config['width'],\n",
    "            max_steps=self.config['width'] * self.config['height'] * 2, # max_steps is typically 2x the grid size\n",
    "            see_through_walls=False,\n",
    "            agent_view_size=5,                      # Size of the agent's view square\n",
    "            mission_space=mission_space,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # Manually define our observation_space as a single Box (the image).\n",
    "        # By default, MiniGrid's image shape is (view_size, view_size, 3) if using partial obs,\n",
    "        # or (height, width, 3) if using full-grid observation. We'll do full-grid here:\n",
    "        # We'll define (self.height, self.width, 3) as the shape.\n",
    "        # In practice, \"image\" shape can vary if partial observations are used.\n",
    "        self.observation_space = Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=(self.agent_view_size, self.agent_view_size, 3),\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "\n",
    "            \n",
    "    def _gen_grid(self, width, height):\n",
    "        \"\"\"\n",
    "        Generate a new environment layout ensuring solvability if required.\n",
    "        \"\"\"\n",
    "        \n",
    "        check_stuck = 0\n",
    "        while True:  # Keep regenerating until a solvable layout is found\n",
    "            self.grid = Grid(width, height)\n",
    "            self.grid.wall_rect(0, 0, width, height)\n",
    "\n",
    "            # Place the goal\n",
    "            goal_pos = self.config.get(\"goal_pos\")\n",
    "            if goal_pos is None:\n",
    "                while True:\n",
    "                    goal_r = self.rng.integers(1, height - 1)\n",
    "                    goal_c = self.rng.integers(1, width - 1)\n",
    "                    if self.grid.get(goal_c, goal_r) is None:\n",
    "                        self.put_obj(Goal(), goal_c, goal_r)\n",
    "                        self.config[\"goal_pos\"] = (goal_c, goal_r)\n",
    "                        break\n",
    "            else:\n",
    "                self.put_obj(Goal(), goal_pos[0], goal_pos[1])\n",
    "\n",
    "            # Place the agent\n",
    "            start_pos = self.config.get(\"start_pos\")\n",
    "            if start_pos is None:\n",
    "                while True:\n",
    "                    start_r = self.rng.integers(1, height - 1)\n",
    "                    start_c = self.rng.integers(1, width - 1)\n",
    "                    if self.grid.get(start_c, start_r) is None and (start_c, start_r) != self.config[\"goal_pos\"]:\n",
    "                        self.agent_pos = (start_c, start_r)\n",
    "                        self.agent_dir = self.rng.integers(0, 4)\n",
    "                        self.config[\"start_pos\"] = (start_c, start_r)\n",
    "                        break\n",
    "            else:\n",
    "                self.agent_pos = start_pos\n",
    "                self.agent_dir = self.rng.integers(0, 4)\n",
    "                self.config[\"start_pos\"] = start_pos\n",
    "            \n",
    "            placed_blocks = 0\n",
    "            \n",
    "            # Maximum number of tries to place the blocks\n",
    "            max_num_tries = 300\n",
    "            \n",
    "            # Place random walls using config parameters\n",
    "            while placed_blocks < self.config[\"num_blocks\"]:\n",
    "                max_num_tries -= 1\n",
    "                r = self.rng.integers(1, height - 1)\n",
    "                c = self.rng.integers(1, width - 1)\n",
    "                if max_num_tries <= 0:\n",
    "                    print(\"Could not place all blocks in the grid.\")\n",
    "                    break\n",
    "                if self.grid.get(c, r) is None and (c, r) != self.config[\"start_pos\"] and (c, r) != self.config[\"goal_pos\"]:\n",
    "                    self.put_obj(Wall(), c, r)\n",
    "                    placed_blocks += 1\n",
    "\n",
    "            # Check solvability if required\n",
    "            if not self.solvable_only or self._is_solvable():\n",
    "                break\n",
    "            \n",
    "            check_stuck += 1\n",
    "            if check_stuck > 50:\n",
    "                #print(\"Re-randomizing start and goal positions...\")\n",
    "                self.config.pop(\"start_pos\", None)\n",
    "                self.config.pop(\"goal_pos\", None)\n",
    "                self.rng = np.random.default_rng(seed=self.config.get(\"seed_val\") + check_stuck)\n",
    "\n",
    "        \n",
    "    def _is_solvable(self):\n",
    "        \"\"\"\n",
    "        Uses Breadth-First Search (BFS) to check if there's a path \n",
    "        from the agent's start position to the goal.\n",
    "        \"\"\"\n",
    "        start_pos = self.config[\"start_pos\"]\n",
    "        goal_pos = self.config[\"goal_pos\"]\n",
    "        if not start_pos or not goal_pos:\n",
    "            return False\n",
    "\n",
    "        queue = deque([start_pos])\n",
    "        visited = set()\n",
    "        visited.add(start_pos)\n",
    "\n",
    "        while queue:\n",
    "            x, y = queue.popleft()\n",
    "            if (x, y) == goal_pos:\n",
    "                return True\n",
    "\n",
    "            # Possible moves: up, down, left, right\n",
    "            for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n",
    "                nx, ny = x + dx, y + dy\n",
    "                cell_obj = self.grid.get(nx, ny)\n",
    "                if (\n",
    "                    1 <= nx < self.width - 1 and  # Stay within grid bounds\n",
    "                    1 <= ny < self.height - 1 and\n",
    "                    (nx, ny) not in visited and\n",
    "                    self.grid.get(nx, ny) is None or isinstance(cell_obj, Goal)\n",
    "                ):\n",
    "                    queue.append((nx, ny))\n",
    "                    visited.add((nx, ny))\n",
    "        return False  # No path found\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Override reset to ensure we only return the 'image' array\n",
    "        instead of a dict with 'image' and 'mission'.\n",
    "        \"\"\"\n",
    "        obs, info = super().reset(**kwargs)\n",
    "        obs = self._convert_obs(obs)\n",
    "        \n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Same for step: override to convert the dict observation into an image only.\n",
    "        \"\"\"\n",
    "        obs, reward, done, truncated, info = super().step(action)\n",
    "        obs = self._convert_obs(obs)\n",
    "        return obs, reward, done, truncated, info\n",
    "\n",
    "    def _convert_obs(self, original_obs):\n",
    "        \"\"\"\n",
    "        original_obs is typically {'image':..., 'mission':...}.\n",
    "        We'll just return original_obs['image'] to get a Box(low=0,high=255) shape.\n",
    "        \"\"\"\n",
    "        return original_obs[\"image\"]\n",
    "        #return np.transpose(original_obs[\"image\"], (2, 0, 1))\n",
    "    \n",
    "    def update_config(self, new_config):\n",
    "        \"\"\"\n",
    "        Update the environment configuration with a new config dict.\n",
    "        \"\"\"\n",
    "        self.config = new_config\n",
    "        self.reset()\n",
    "\n",
    "def random_config(grid_size, num_blocks=None, seed=None):\n",
    "    \"\"\"\n",
    "    Utility function to generate a random configuration dict.\n",
    "    \"\"\"\n",
    "    \n",
    "    max_blocks = int(((grid_size - 1) * (grid_size - 1)) / 2)\n",
    "    \n",
    "    if num_blocks is None:\n",
    "        num_blocks = np.random.randint(1, max_blocks)\n",
    "    else:\n",
    "        num_blocks = min(num_blocks, max_blocks)\n",
    "        \n",
    "    config = {\n",
    "        \"width\": grid_size,\n",
    "        \"height\": grid_size,\n",
    "        \"num_blocks\": num_blocks,\n",
    "        \"start_pos\": None,\n",
    "        \"goal_pos\": None,\n",
    "        \"edited\": False,\n",
    "        \"seed_val\": seed if seed is not None else np.random.randint(0, 1000)\n",
    "    }\n",
    "    \n",
    "    # Set the start and goal positions\n",
    "    env = MyCustomGrid(config)\n",
    "    \n",
    "    # Reset the environment to get the start and goal positions\n",
    "    env.reset()\n",
    "    \n",
    "    # Get the new config from the environment\n",
    "    config = env.config\n",
    "        \n",
    "    return config\n",
    "\n",
    "def print_level_from_config(config, solvable_only=False):\n",
    "    \"\"\"\n",
    "    Function to display a level configuration as an image.\n",
    "    \"\"\"\n",
    "    \n",
    "    env = MyCustomGrid(config, render_mode='rgb_array', solvable_only=solvable_only)\n",
    "    env.reset()\n",
    "    full_level_image = env.render()  # This should return an RGB image of the full grid\n",
    "\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(full_level_image)\n",
    "    plt.title(\"Level Configuration: \" + str(config))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "def edit_config(old_config):\n",
    "    \"\"\"\n",
    "    Make a little edit to an existing configuration.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Modify an existing configuration, adding randomness.\n",
    "    max_blocks = int(((old_config[\"width\"] - 1) * (old_config[\"height\"] - 1)) / 2)\n",
    "    \n",
    "    new_config = dict(old_config)\n",
    "    \n",
    "    # Randomly change the number of blocks\n",
    "    new_number_blocks = old_config[\"num_blocks\"] + 1\n",
    "    \n",
    "    # Ensure the number of blocks is within bounds\n",
    "    new_config[\"num_blocks\"] = max(1, min(new_number_blocks, max_blocks))    \n",
    "    \n",
    "    # Mark the config as edited\n",
    "    new_config[\"edited\"] = True\n",
    "    \n",
    "    return new_config\n",
    "\n",
    "def create_vectorized_env(config, n_envs=4, solvable_only=False):\n",
    "    \"\"\"\n",
    "    Create a vectorized environment with n parallel environments.\n",
    "    \"\"\"\n",
    "    return make_vec_env(lambda: MyCustomGrid(config, solvable_only), n_envs=n_envs, vec_env_cls=SubprocVecEnv)\n",
    "\n",
    "# ====================================================\n",
    "# Level Buffer\n",
    "# ====================================================\n",
    "class LevelBuffer: \n",
    "    def __init__(self, max_size=50):\n",
    "        self.max_size = max_size\n",
    "        self.data = []  # will store (config_dict, score)\n",
    "\n",
    "    def add(self, config, score):\n",
    "        self.data.append((config, score))\n",
    "        if len(self.data) > self.max_size:\n",
    "            self.data.sort(key=lambda x: x[1], reverse=True)\n",
    "            self.data = self.data[: self.max_size]\n",
    "            #it memorize only the highest score for each level\n",
    "\n",
    "    def sample_config(self): \n",
    "        # Samples a level from the buffer, weighting the probabilities \n",
    "        # based on the scores.\n",
    "        if len(self.data) == 0:\n",
    "            return None\n",
    "        scores = [item[1] for item in self.data]\n",
    "        total = sum(scores)\n",
    "        if total <= 1e-9:\n",
    "            # fallback to uniform\n",
    "            idx = np.random.randint(len(self.data))\n",
    "            return self.data[idx][0]\n",
    "        probs = [s / total for s in scores]\n",
    "        idx = np.random.choice(len(self.data), p=probs)\n",
    "        return self.data[idx][0]\n",
    "\n",
    "# ====================================================\n",
    "# Test Functions\n",
    "# ====================================================\n",
    "def load_models(grid_size):\n",
    "    \"\"\"Load the RL models.\"\"\"\n",
    "    model_dr = PPO.load(f\"models/dr_model_{grid_size}x{grid_size}\")\n",
    "    model_plr = PPO.load(f\"models/plr_model_{grid_size}x{grid_size}\")\n",
    "    model_accel = PPO.load(f\"models/accel_model_{grid_size}x{grid_size}\")\n",
    "    model_accel_easy = PPO.load(f\"models/accel_model_easy_{grid_size}x{grid_size}\")\n",
    "    print(f\"Models trained on {grid_size}x{grid_size} grid loaded successfully.\")\n",
    "    \n",
    "    return model_dr, model_plr, model_accel, model_accel_easy\n",
    "\n",
    "def evalute_models(load_dim = -1, grid_size = 6, n_eval_episodes = 5, num_levels_per_difficulty = 10):\n",
    "    \n",
    "    if load_dim > 0:\n",
    "        model_dr, model_plr, model_accel, model_accel_easy = load_models(load_dim)\n",
    "        \n",
    "    # Inseert the models in a dictionary\n",
    "    models = {\"DR\": model_dr, 'PLR': model_plr, 'ACCEL': model_accel, 'ACCEL-EasyStart': model_accel_easy}\n",
    "\n",
    "    # Generate n levels difficulties with increasing complexity, for each level generate m configs\n",
    "    difficulties = 3\n",
    "    num_levels_per_difficulty = num_levels_per_difficulty\n",
    "\n",
    "    levels = []\n",
    "    for i in range(difficulties):\n",
    "        level = []\n",
    "        for _ in range(num_levels_per_difficulty):\n",
    "            cfg = random_config(grid_size, num_blocks=grid_size*(i+1))\n",
    "            #print_level_from_config(cfg, solvable_only=True)\n",
    "            level.append(cfg)\n",
    "        levels.append(level)\n",
    "        \n",
    "    \n",
    "    # Create a dummy config to initialize the vectorized environment\n",
    "    dummy_config = random_config(grid_size)\n",
    "    env = create_vectorized_env(dummy_config, n_envs=4, solvable_only=True)\n",
    "\n",
    "    # Evaluate the model on the generated levels\n",
    "    results = {}\n",
    "    for model_name, model in models.items():\n",
    "        results[model_name] = []\n",
    "        for i, level in enumerate(levels):\n",
    "            print(f\"Evaluating {num_levels_per_difficulty} levels of difficulty {i + 1} with {grid_size*(i+1)} blocks on a {grid_size}x{grid_size} grid for model {model_name}, ratio of blocks to grid size: {grid_size*(i+1) / (grid_size*grid_size):.2f}\")\n",
    "            r = []\n",
    "            for j, cfg in enumerate(level):\n",
    "                # Update the environment with the new config\n",
    "                env.env_method(\"update_config\", cfg)\n",
    "                mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=n_eval_episodes, deterministic=True)\n",
    "                r.append(mean_reward)\n",
    "            results[model_name].append(r)\n",
    "        print()\n",
    "        \n",
    "    # Print mean rewards for each level\n",
    "    for model_name in models.keys():\n",
    "        print(f\"Model: {model_name}\")\n",
    "        for i, level in enumerate(levels):\n",
    "            print(f\"Level {i + 1} - Complexity {grid_size*(i+1)}: {np.mean(results[model_name][i]):.2f}\")\n",
    "        print()\n",
    "    \n",
    "    #Comute the number of xticks based on the number of models\n",
    "    xticks = [i for i in range(1, len(models.keys()) + 1)]\n",
    "\n",
    "    # Boxplot of results, a plot for each level complexity comparing models\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for i, level in enumerate(levels):\n",
    "        plt.subplot(1, difficulties, i + 1)\n",
    "        plt.boxplot([results[model_name][i] for model_name in models.keys()])\n",
    "        plt.xticks([1,2,3,4], [model_name for model_name in models.keys()])\n",
    "        plt.title(f\"Level {i + 1} - Complexity {grid_size*(i+1)}\")\n",
    "        plt.ylabel(\"Mean Reward\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig(f\"plots/boxplot_{load_dim}x{load_dim}.png\")\n",
    "    \n",
    "def test_model(model, config, gif_path=\"level.gif\", max_steps=1000):\n",
    "    \"\"\"Evaluate a model on a given environment instance.\"\"\"\n",
    "    env = MyCustomGrid(config, render_mode='rgb_array', solvable_only=True)\n",
    "    obs, _ = env.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    total_reward = 0\n",
    "    i = 0\n",
    "\n",
    "    frames = []  # List to store frames\n",
    "\n",
    "    # Continue until either terminated or truncated is True\n",
    "    while not (terminated or truncated):\n",
    "        frame = env.render()  # Capture frame as an image\n",
    "        frames.append(Image.fromarray(frame))  # Convert to PIL image and store\n",
    "        \n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        i += 1\n",
    "        if i >= max_steps:\n",
    "            break\n",
    "    \n",
    "    # Add to the gif also the last frame\n",
    "    frame = env.render()\n",
    "    frames.append(Image.fromarray(frame))\n",
    "\n",
    "    # Save frames as a GIF\n",
    "    if frames:\n",
    "        frames[0].save(\n",
    "            gif_path, save_all=True, append_images=frames[1:], duration=500, loop=0\n",
    "        )\n",
    "    return total_reward  # Do not close env here, as we reuse it\n",
    "\n",
    "def evaluate_and_create_gifs(grid_size, num_levels=50, max_steps=1000, num_blocks=None):\n",
    "    # Load the models\n",
    "    model_dr, model_plr, model_accel, model_accel_easy = load_models(grid_size)\n",
    "\n",
    "    # Test the models on the same exact environment instance\n",
    "    rewards = {\"DR\": 0, \"PLR\": 0, \"ACCEL\": 0, \"ACCEL-EasyStart\": 0}\n",
    "    for i in range(num_levels):\n",
    "        if num_blocks is not None:\n",
    "            config = random_config(grid_size, num_blocks=num_blocks, seed=512 + i)\n",
    "        else:\n",
    "            config = random_config(grid_size, seed=512 + i)\n",
    "\n",
    "        reward = test_model(model_dr, config, gif_path=f\"gifs/{grid_size}x{grid_size}_level_{i}_dr.gif\", max_steps=max_steps)\n",
    "        rewards[\"DR\"] += reward\n",
    "\n",
    "        reward = test_model(model_plr, config, gif_path=f\"gifs/{grid_size}x{grid_size}_level_{i}_plr.gif\", max_steps=max_steps)\n",
    "        rewards[\"PLR\"] += reward\n",
    "\n",
    "        reward = test_model(model_accel, config, gif_path=f\"gifs/{grid_size}x{grid_size}_level_{i}_accel.gif\", max_steps=max_steps)\n",
    "        rewards[\"ACCEL\"] += reward\n",
    "\n",
    "        reward = test_model(model_accel_easy, config, gif_path=f\"gifs/{grid_size}x{grid_size}_level_{i}_accel_easy.gif\", max_steps=max_steps)\n",
    "        rewards[\"ACCEL-EasyStart\"] += reward\n",
    "\n",
    "    print(\"Total rewards:\")\n",
    "    for model_name, reward in rewards.items():\n",
    "        print(f\"{model_name}: {reward:.2f}\")\n",
    "\n",
    "# ====================================================\n",
    "# Regret Calculation with Parallel Environments\n",
    "# ====================================================\n",
    "\n",
    "def calculate_regret_gae_vectorized(vec_env, model, max_steps, gamma, lam, \n",
    "                                    specific_env_indices=None):\n",
    "    \"\"\"\n",
    "    Compute regrets (per the formula in the question) for each sub-environment in vec_env, in parallel.\n",
    "    \n",
    "    If specific_env_indices is not None, only store and compute regrets\n",
    "    for those sub-env indices. Otherwise, compute regrets for all sub-envs.\n",
    "\n",
    "    :param vec_env: A vectorized environment with n_envs sub-envs.\n",
    "    :param model: A (partially) trained stable-baselines PPO model.\n",
    "    :param max_steps: Maximum steps to roll out each sub-environment.\n",
    "    :param gamma: Discount factor for GAE (and for the regret formula).\n",
    "    :param lam: Lambda factor for GAE (and for the regret formula).\n",
    "    :param specific_env_indices: Optional. List of env indices to track. \n",
    "                                Default None => track all.\n",
    "    :return: If specific_env_indices is None:\n",
    "                regrets in a list of length n_envs\n",
    "             Else:\n",
    "                regrets in a list of length = len(specific_env_indices),\n",
    "                in the same order as specific_env_indices.\n",
    "    \"\"\"\n",
    "    n_envs = vec_env.num_envs\n",
    "\n",
    "    # Decide which indices to track\n",
    "    if specific_env_indices is None:\n",
    "        tracked_indices = list(range(n_envs))\n",
    "    else:\n",
    "        tracked_indices = list(specific_env_indices)\n",
    "\n",
    "    # We will store data only for \"tracked_indices\"\n",
    "    all_rewards = {i: [] for i in tracked_indices}\n",
    "    all_values = {i: [] for i in tracked_indices}\n",
    "    all_dones =  {i: [] for i in tracked_indices}\n",
    "\n",
    "    # Initialize the environment\n",
    "    obs_array = vec_env.reset()\n",
    "    obs_tensor = torch.as_tensor(obs_array, dtype=torch.float32, device=device)\n",
    "\n",
    "    # For stepping the entire vec_env in parallel, track if each env is done\n",
    "    done_flags = [False] * n_envs\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        # If all tracked envs are done, stop collecting\n",
    "        if all(done_flags[i] for i in tracked_indices):\n",
    "            break\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # The model predict handles all sub-envs\n",
    "            actions, _ = model.predict(obs_array, deterministic=True)\n",
    "            # We also get predicted values (shape: (n_envs,))\n",
    "            values_t = model.policy.predict_values(obs_tensor).squeeze(dim=-1)\n",
    "\n",
    "        # Step all n_envs in parallel\n",
    "        new_obs_array, rewards, dones, infos = vec_env.step(actions)\n",
    "\n",
    "        # Store reward/value/done for tracked envs that are still alive\n",
    "        for i in tracked_indices:\n",
    "            if not done_flags[i]:\n",
    "                all_rewards[i].append(rewards[i])\n",
    "                all_values[i].append(values_t[i].item())\n",
    "                all_dones[i].append(dones[i])\n",
    "\n",
    "        # Update the done flags for all envs\n",
    "        done_flags = [done_flags[i] or dones[i] for i in range(n_envs)]\n",
    "\n",
    "        # Update obs\n",
    "        obs_array = new_obs_array\n",
    "        obs_tensor = torch.as_tensor(obs_array, dtype=torch.float32, device=device)\n",
    "\n",
    "    # Compute terminal values for each env (if not done, we take V(s_final))\n",
    "    with torch.no_grad():\n",
    "        terminal_values = model.policy.predict_values(obs_tensor).cpu().numpy().flatten()\n",
    "\n",
    "    # Now compute the regret formula for each tracked env\n",
    "    regrets_dict = {}\n",
    "    for i in tracked_indices:\n",
    "        rewards_i = all_rewards[i]\n",
    "        values_i = all_values[i]\n",
    "        dones_i   = all_dones[i]\n",
    "\n",
    "        # If no trajectory was collected, regret = 0\n",
    "        if len(rewards_i) == 0:\n",
    "            regrets_dict[i] = 0.0\n",
    "            continue\n",
    "\n",
    "        # If the last step wasn't done, append the final predicted value\n",
    "        if not dones_i[-1]:\n",
    "            values_i.append(float(terminal_values[i]))\n",
    "        else:\n",
    "            values_i.append(0.0)\n",
    "\n",
    "        # Construct the delta array:\n",
    "        # delta_t = r_t + gamma * (1 - done_t) * V_{t+1} - V_t\n",
    "        deltas = []\n",
    "        for t in range(len(rewards_i)):\n",
    "            delta_t = (rewards_i[t]\n",
    "                       + gamma * values_i[t + 1] * (1 - dones_i[t])\n",
    "                       - values_i[t])\n",
    "            deltas.append(delta_t)\n",
    "\n",
    "        # We now implement exactly:\n",
    "        #   (1/T) * sum_{t=0..T-1} max( sum_{k=t..T-1} (gamma*lambda)^(k-t) * delta_k, 0 )\n",
    "        # Let N = len(deltas). If the trajectory ended early, T=N.\n",
    "\n",
    "        N = len(deltas)\n",
    "        partial_sums = [0.0]*N\n",
    "\n",
    "        # We can compute sum_{k=t}^{N-1} (gamma*lam)^(k-t) * delta_k\n",
    "        # efficiently with a backward recursion:\n",
    "        #     P_N = 0\n",
    "        #     P_t = delta_t + (gamma*lam)*P_{t+1}\n",
    "        # Then partial_sums[t] = P_t\n",
    "        # Then we take max(P_t, 0), sum up, then divide by N.\n",
    "\n",
    "        P_next = 0.0\n",
    "        for t in reversed(range(N)):\n",
    "            P_t = deltas[t] + (gamma * lam) * P_next\n",
    "            partial_sums[t] = max(P_t, 0.0)\n",
    "            P_next = P_t  # store for next iteration\n",
    "\n",
    "        env_regret = sum(partial_sums) / N\n",
    "        regrets_dict[i] = env_regret\n",
    "\n",
    "    return [regrets_dict[i] for i in tracked_indices]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main_accel_parallel(total_iterations, replay_prob, train_steps, level_buffer_size,\n",
    "               initial_fill_size, grid_size, n_envs, edit_levels, regret_threshold,\n",
    "               easy_start, domain_randomization, name, test_every_n_iterations, test_on_levels,\n",
    "               lr, batch_size):\n",
    "    \"\"\"\n",
    "    Main ACCEL function with parallel environments.\n",
    "    Re-ordered to match ACCEL pseudocode:\n",
    "      - d=0: sample new level, collect trajectory (stop-grad), compute regret, update buffer\n",
    "      - d=1: sample from buffer, collect trajectory & update policy, edit, collect trajectory (stop-grad),\n",
    "             compute regrets, update buffer\n",
    "    \"\"\"\n",
    "    # Initialize W&B if you like (commented out)\n",
    "    # wandb.init(project=\"accel\", config=config, mode=\"disabled\")\n",
    "    \n",
    "    def plot_display_avg_rew_list(avg_list):\n",
    "        plt.plot(avg_list)\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylabel(\"Average Reward\")\n",
    "        plt.title(\"Average Reward Progress during Training\")\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f\"plots/avg_reward_progress_{name}_{grid_size}x{grid_size}.png\")\n",
    "        plt.show()\n",
    "    \n",
    "    # Create a level buffer\n",
    "    level_buffer = LevelBuffer(max_size=level_buffer_size)\n",
    "    \n",
    "    # Generate a random configuration as a placeholder\n",
    "    dummy_config = random_config(grid_size)\n",
    "    \n",
    "    # Create a vectorized environment\n",
    "    vectorized_env = create_vectorized_env(dummy_config, n_envs=n_envs)\n",
    "\n",
    "    # Initialize PPO\n",
    "    print(\"Initializing student model PPO...\")\n",
    "    student_model = PPO(\n",
    "        \"MlpPolicy\",\n",
    "        vectorized_env,\n",
    "        verbose=0,\n",
    "        n_steps=512,\n",
    "        batch_size=batch_size,\n",
    "        learning_rate=lr,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    avg_rewards_list = []\n",
    "            \n",
    "    # ====================================================\n",
    "    # Initial buffer fill\n",
    "    # ====================================================\n",
    "    if not domain_randomization:\n",
    "        print(f\"Populating buffer with {initial_fill_size} initial levels (regret > {regret_threshold})...\")\n",
    "        \n",
    "        while len(level_buffer.data) < initial_fill_size:\n",
    "            # Generate n_envs configs\n",
    "            configs = []\n",
    "            for _ in range(n_envs):\n",
    "                if easy_start:\n",
    "                    cfg = random_config(grid_size, num_blocks=2, seed=42+len(level_buffer.data))\n",
    "                else:\n",
    "                    cfg = random_config(grid_size, seed=42+len(level_buffer.data))\n",
    "                configs.append(cfg)\n",
    "\n",
    "            # Set every environment with a different config\n",
    "            for i in range(n_envs):\n",
    "                vectorized_env.env_method(\"update_config\", configs[i], indices=i)\n",
    "                \n",
    "            # Train the model using the vectorized environment\n",
    "            student_model.learn(total_timesteps=train_steps)\n",
    "            \n",
    "            # Calculate regret for each environment\n",
    "            regrets = calculate_regret_gae_vectorized(\n",
    "                vectorized_env, student_model, max_steps=1000, gamma=0.99, lam=0.95\n",
    "            )\n",
    "            \n",
    "            # Add the configs with regret > threshold to the buffer\n",
    "            for i in range(n_envs):\n",
    "                cfg = configs[i]\n",
    "                regret = regrets[i]\n",
    "                if regret < regret_threshold:\n",
    "                    continue\n",
    "                level_buffer.add(cfg, regret)\n",
    "\n",
    "    # ====================================================\n",
    "    # Main ACCEL loop\n",
    "    # ====================================================\n",
    "    \n",
    "    iteration_regrets = []\n",
    "    iteration, skipped = 0, 0\n",
    "    \n",
    "    print(\"\\nMain training loop...\")\n",
    "\n",
    "    # If domain randomization is activated, we just do random levels.\n",
    "    if domain_randomization:\n",
    "        while iteration < total_iterations:\n",
    "            # Optional test code block:\n",
    "            \"\"\"\n",
    "            if iteration % test_every_n_iterations == 0:\n",
    "                rewards = 0\n",
    "                for i in range(test_on_levels):\n",
    "                    test_config = random_config(grid_size=grid_size, seed=12945 + i)\n",
    "                    reward = test_model(student_model, test_config)\n",
    "                    rewards += reward\n",
    "                print(f\"Reward over {test_on_levels} levels: {rewards}\")\n",
    "                avg_rewards_list.append(rewards)\n",
    "            \"\"\"\n",
    "\n",
    "            print(f\"\\n=== ITERATION {iteration + 1}/{total_iterations} (Domain Randomization) ===\")\n",
    "            \n",
    "            # Create n_envs random configs\n",
    "            configs = []\n",
    "            for _ in range(n_envs):\n",
    "                cfg = random_config(grid_size, seed=42+iteration)\n",
    "                configs.append(cfg)\n",
    "            \n",
    "            # Set every environment with a different config\n",
    "            for i in range(n_envs):\n",
    "                vectorized_env.env_method(\"update_config\", configs[i], indices=i)\n",
    "\n",
    "            # Train the model using the vectorized environment\n",
    "            student_model.learn(total_timesteps=train_steps)\n",
    "            iteration += n_envs  # counting it that way to keep your style\n",
    "\n",
    "        student_model.save(f\"models/{name}\")\n",
    "        print(f\"Model saved as models/{name}\")\n",
    "\n",
    "        print(\"The average rewards for each iteration are:\")\n",
    "        print(avg_rewards_list)\n",
    "        \n",
    "        #plot_display_avg_rew_list(avg_rewards_list)\n",
    "\n",
    "        # save to a file\n",
    "        with open(f\"rewards/{name}.txt\", \"w\") as f:\n",
    "            for item in avg_rewards_list:\n",
    "                f.write(\"%s\\n\" % item)\n",
    "\n",
    "        return student_model\n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # Otherwise: actual ACCEL logic (no domain randomization)\n",
    "    # ----------------------------------------------------\n",
    "    while iteration < total_iterations + skipped:\n",
    "        print(f\"\\n=== ITERATION {iteration + 1}/{total_iterations + skipped} SKIPPED: {skipped}, skip ratio: {skipped / (iteration + 1):.2f} ===\")\n",
    "\n",
    "        # Optional test code block:\n",
    "        \"\"\"\n",
    "        if iteration % test_every_n_iterations == 0:\n",
    "            rewards = 0\n",
    "            for i in range(test_on_levels):\n",
    "                test_config = random_config(grid_size=grid_size, seed=12945 + i)\n",
    "                reward = test_model(student_model, test_config)\n",
    "                rewards += reward\n",
    "            print(f\"Reward over {test_on_levels} levels: {rewards}\")\n",
    "            avg_rewards_list.append(rewards)\n",
    "        \"\"\"\n",
    "\n",
    "        # Decide for each environment: replay or new level\n",
    "        # d[i] = True means \"replay\" (d=1), else new level (d=0)\n",
    "        use_replay = [np.random.rand() < replay_prob for _ in range(n_envs)]\n",
    "        \n",
    "        # ----------------------------------------------------\n",
    "        # Step A: d=0. Sample new levels, collect trajectory (NO policy update), compute regrets, update buffer\n",
    "        # ----------------------------------------------------\n",
    "        new_indices = [i for i, rep in enumerate(use_replay) \n",
    "                       if (not rep) or (len(level_buffer.data) == 0)]\n",
    "        # We will gather these new-level environments, set them, \n",
    "        # collect regrets with no policy update, then update the buffer.\n",
    "        if len(new_indices) > 0:\n",
    "            new_configs = []\n",
    "            for i in new_indices:\n",
    "                # Sample new level from generator\n",
    "                cfg = random_config(grid_size, seed=42+iteration+i) \n",
    "                new_configs.append(cfg)\n",
    "\n",
    "            # Update only those sub-environments with the new configs\n",
    "            for idx, env_i in enumerate(new_indices):\n",
    "                vectorized_env.env_method(\"update_config\", new_configs[idx], indices=env_i)\n",
    "            \n",
    "            # --- Collect policy trajectory with stop-gradient ---\n",
    "            # We do NOT call student_model.learn(...) here,\n",
    "            # just compute regrets directly. \n",
    "            regrets_new = calculate_regret_gae_vectorized(\n",
    "                vectorized_env, student_model, max_steps=1024, gamma=0.99, lam=0.95, \n",
    "                specific_env_indices=new_indices\n",
    "            )\n",
    "\n",
    "            # Update buffer for those that pass threshold\n",
    "            for idx, env_i in enumerate(new_indices):\n",
    "                r = regrets_new[idx]\n",
    "                if r > regret_threshold:\n",
    "                    level_buffer.add(new_configs[idx], r)\n",
    "                    iteration_regrets.append(r)\n",
    "                else:\n",
    "                    skipped += 1\n",
    "\n",
    "            # Increase iteration count by however many new envs we processed\n",
    "            iteration += len(new_indices)\n",
    "        \n",
    "        # ----------------------------------------------------\n",
    "        # Step B: d=1. Replay levels from buffer, \n",
    "        #         1) collect trajectory & update policy\n",
    "        #         2) edit level\n",
    "        #         3) collect new trajectory (stop-grad)\n",
    "        #         4) compute regrets, update buffer\n",
    "        # ----------------------------------------------------\n",
    "        replay_indices = [i for i, rep in enumerate(use_replay) \n",
    "                          if rep and (len(level_buffer.data) > 0)]\n",
    "        if len(replay_indices) > 0:\n",
    "            # 1) Sample from buffer and set those envs\n",
    "            replay_original_cfgs = []\n",
    "            for i in replay_indices:\n",
    "                cfg = level_buffer.sample_config()\n",
    "                replay_original_cfgs.append(cfg)\n",
    "\n",
    "            for idx, env_i in enumerate(replay_indices):\n",
    "                vectorized_env.env_method(\"update_config\", replay_original_cfgs[idx], indices=env_i)\n",
    "            \n",
    "            # 2) Collect policy trajectory **AND update policy** here\n",
    "            student_model.learn(total_timesteps=train_steps)\n",
    "\n",
    "            # 3) Edit level\n",
    "            # If edit_levels is True, we edit the replayed level\n",
    "            replay_edited_cfgs = []\n",
    "            for idx, env_i in enumerate(replay_indices):\n",
    "                original = replay_original_cfgs[idx]\n",
    "                if edit_levels:\n",
    "                    edited = edit_config(original)\n",
    "                else:\n",
    "                    edited = original  # no edit\n",
    "                replay_edited_cfgs.append(edited)\n",
    "                # Update that sub-environment with the edited config\n",
    "                vectorized_env.env_method(\"update_config\", edited, indices=env_i)\n",
    "            \n",
    "            # 4) Collect new trajectory (stop-grad) on the edited level\n",
    "            regrets_replay_edited = calculate_regret_gae_vectorized(\n",
    "                vectorized_env, student_model, max_steps=1024, gamma=0.99, lam=0.95,\n",
    "                specific_env_indices=replay_indices\n",
    "            )\n",
    "\n",
    "            # Optionally, we also want the regret for the *original* level,\n",
    "            # but we only saved that before calling .learn(...). \n",
    "            # We can compute it again if needed. Typically, we'd do it right before \n",
    "            # the policy update. For simplicity, let's do it now:\n",
    "            # (You could store old rollouts, but let's just recalc for demonstration.)\n",
    "            for idx, env_i in enumerate(replay_indices):\n",
    "                # Re-load original config\n",
    "                vectorized_env.env_method(\"update_config\", replay_original_cfgs[idx], indices=env_i)\n",
    "            # Now compute regret with the *updated* policy (post-update).\n",
    "            regrets_replay_original_after_update = calculate_regret_gae_vectorized(\n",
    "                vectorized_env, student_model, max_steps=512, gamma=0.99, lam=0.95,\n",
    "                specific_env_indices=replay_indices\n",
    "            )\n",
    "            \n",
    "            # 5) Update buffer if regrets meet threshold\n",
    "            for idx, env_i in enumerate(replay_indices):\n",
    "                s_orig = regrets_replay_original_after_update[idx]\n",
    "                s_edit = regrets_replay_edited[idx]\n",
    "                \n",
    "                # Add original if passes threshold\n",
    "                if s_orig > regret_threshold:\n",
    "                    level_buffer.add(replay_original_cfgs[idx], s_orig)\n",
    "                    iteration_regrets.append(s_orig)\n",
    "                else:\n",
    "                    skipped += 1\n",
    "\n",
    "                # Add edited if passes threshold\n",
    "                if s_edit > regret_threshold:\n",
    "                    level_buffer.add(replay_edited_cfgs[idx], s_edit)\n",
    "                    iteration_regrets.append(s_edit)\n",
    "                else:\n",
    "                    skipped += 1\n",
    "\n",
    "            # Increase iteration count by however many replay envs we processed\n",
    "            iteration += len(replay_indices)\n",
    "        \n",
    "        # End of iteration loop\n",
    "\n",
    "    # close the environment\n",
    "    vectorized_env.close()\n",
    "    \n",
    "    # ====================================================\n",
    "    # Plotting and Saving\n",
    "    # ====================================================\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(iteration_regrets, marker='o')\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Regret\")\n",
    "    plt.title(\"Regret Progress during Training\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"plots/regret_progress_{name}_{grid_size}x{grid_size}.png\")\n",
    "    #plt.show()\n",
    "    \n",
    "    print(\"\\nDone. Final buffer size:\", len(level_buffer.data))\n",
    "    print(\"Top-5 hardest levels (config, regret):\")\n",
    "    level_buffer.data.sort(key=lambda x: x[1], reverse=True)\n",
    "    for i, (cfg, sc) in enumerate(level_buffer.data[:5]):\n",
    "        print(f\"{i + 1}. regret={sc:.5f}, config={cfg}\")\n",
    "        \n",
    "    print(\"Top-5 easiest levels (config, regret):\")\n",
    "    level_buffer.data.sort(key=lambda x: x[1])\n",
    "    for i, (cfg, sc) in enumerate(level_buffer.data[:5]):\n",
    "        print(f\"{i + 1}. regret={sc:.5f}, config={cfg}\")\n",
    "    \n",
    "    wandb.finish()\n",
    "    \n",
    "    # Save the model\n",
    "    student_model.save(f\"models/{name}\")\n",
    "    print(f\"Model saved as models/{name}\")\n",
    "\n",
    "    print(\"The average rewards for each iteration are:\")\n",
    "    print(avg_rewards_list)\n",
    "    \n",
    "    #plot_display_avg_rew_list(avg_rewards_list)\n",
    "\n",
    "    # save to a file\n",
    "    with open(f\"rewards/{name}.txt\", \"w\") as f:\n",
    "        for item in avg_rewards_list:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "\n",
    "    return student_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running accel_model_easy_12x12_rep_skipped with config: {'name': 'accel_model_easy_12x12_rep_skipped', 'grid_size': 12, 'train_steps': 2592, 'total_iterations': 1728, 'test_every_n_iterations': 36, 'replay_prob': 0.8, 'level_buffer_size': 256, 'initial_fill_size': 128, 'regret_threshold': 0.0, 'n_envs': 18, 'test_on_levels': 100, 'edit_levels': True, 'easy_start': True, 'domain_randomization': False, 'lr': 0.0001, 'batch_size': 128}\n",
      "Initializing student model PPO...\n",
      "Populating buffer with 128 initial levels (regret > 0.0)...\n",
      "\n",
      "Main training loop...\n",
      "\n",
      "=== ITERATION 1/1728 SKIPPED: 0, skip ratio: 0.00 ===\n",
      "\n",
      "=== ITERATION 19/1730 SKIPPED: 2, skip ratio: 0.11 ===\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 70\u001b[0m\n\u001b[0;32m     67\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(asdict(config), f)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with config: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00masdict(config)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)    \n\u001b[1;32m---> 70\u001b[0m \u001b[43mmain_accel_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43masdict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m============================================\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[13], line 806\u001b[0m, in \u001b[0;36mmain_accel_parallel\u001b[1;34m(total_iterations, replay_prob, train_steps, level_buffer_size, initial_fill_size, grid_size, n_envs, edit_levels, regret_threshold, easy_start, domain_randomization, name, test_every_n_iterations, test_on_levels, lr, batch_size)\u001b[0m\n\u001b[0;32m    803\u001b[0m     vectorized_env\u001b[38;5;241m.\u001b[39menv_method(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdate_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, replay_original_cfgs[idx], indices\u001b[38;5;241m=\u001b[39menv_i)\n\u001b[0;32m    805\u001b[0m \u001b[38;5;66;03m# 2) Collect policy trajectory **AND update policy** here\u001b[39;00m\n\u001b[1;32m--> 806\u001b[0m \u001b[43mstudent_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;66;03m# 3) Edit level\u001b[39;00m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;66;03m# If edit_levels is True, we edit the replayed level\u001b[39;00m\n\u001b[0;32m    810\u001b[0m replay_edited_cfgs \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\Utente\\miniconda3\\envs\\accel_env_cuda\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Utente\\miniconda3\\envs\\accel_env_cuda\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:323\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 323\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[0;32m    326\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Utente\\miniconda3\\envs\\accel_env_cuda\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:202\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;66;03m# Convert to pytorch tensor or to TensorDict\u001b[39;00m\n\u001b[0;32m    201\u001b[0m     obs_tensor \u001b[38;5;241m=\u001b[39m obs_as_tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 202\u001b[0m     actions, values, log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    203\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    205\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Utente\\miniconda3\\envs\\accel_env_cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Utente\\miniconda3\\envs\\accel_env_cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Utente\\miniconda3\\envs\\accel_env_cuda\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:645\u001b[0m, in \u001b[0;36mActorCriticPolicy.forward\u001b[1;34m(self, obs, deterministic)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;124;03mForward pass in all the networks (actor and critic)\u001b[39;00m\n\u001b[0;32m    639\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;124;03m:return: action, value and log probability of the action\u001b[39;00m\n\u001b[0;32m    643\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    644\u001b[0m \u001b[38;5;66;03m# Preprocess the observation if needed\u001b[39;00m\n\u001b[1;32m--> 645\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    646\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare_features_extractor:\n\u001b[0;32m    647\u001b[0m     latent_pi, latent_vf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_extractor(features)\n",
      "File \u001b[1;32mc:\\Users\\Utente\\miniconda3\\envs\\accel_env_cuda\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:672\u001b[0m, in \u001b[0;36mActorCriticPolicy.extract_features\u001b[1;34m(self, obs, features_extractor)\u001b[0m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;124;03mPreprocess the observation if needed and extract features.\u001b[39;00m\n\u001b[0;32m    665\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;124;03m    features for the actor and the features for the critic.\u001b[39;00m\n\u001b[0;32m    670\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare_features_extractor:\n\u001b[1;32m--> 672\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures_extractor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeatures_extractor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeatures_extractor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m features_extractor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Utente\\miniconda3\\envs\\accel_env_cuda\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:131\u001b[0m, in \u001b[0;36mBaseModel.extract_features\u001b[1;34m(self, obs, features_extractor)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03mPreprocess the observation if needed and extract features.\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m:return: The extracted features\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    130\u001b[0m preprocessed_obs \u001b[38;5;241m=\u001b[39m preprocess_obs(obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space, normalize_images\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize_images)\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfeatures_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocessed_obs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Utente\\miniconda3\\envs\\accel_env_cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1732\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1729\u001b[0m             tracing_state\u001b[38;5;241m.\u001b[39mpop_scope()\n\u001b[0;32m   1730\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m-> 1732\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapped_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1734\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    name: str = \"model\"\n",
    "    grid_size: int = 6  # Grid size\n",
    "    \n",
    "    # Dynamic parameters (set in __post_init__)\n",
    "    train_steps: int = None\n",
    "    total_iterations: int = None\n",
    "    test_every_n_iterations: int = None\n",
    "\n",
    "    replay_prob: float = 0.8\n",
    "    level_buffer_size: int = 256\n",
    "    initial_fill_size: int = 128\n",
    "    regret_threshold: float = 0.00\n",
    "    n_envs: int = 18\n",
    "    test_on_levels: int = 100\n",
    "    edit_levels: bool = True\n",
    "    easy_start: bool = True\n",
    "    domain_randomization: bool = False\n",
    "    lr: float = 1e-4\n",
    "    batch_size: int = 128\n",
    "\n",
    "    def update_dependent_params(self):\n",
    "        \"\"\"Recalculate dynamic parameters after updating grid_size.\"\"\"\n",
    "        self.train_steps = self.grid_size * self.grid_size * self.n_envs  # grid_size^2 * n_envs\n",
    "        self.test_every_n_iterations = self.n_envs * 2  # test every n_envs * 2 iterations\n",
    "        self.total_iterations = self.grid_size * self.grid_size * self.grid_size\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_config = Config()  # elements are set to default values and dynamically calculated\n",
    "\n",
    "    # Define different configuration\n",
    "    configs = [\n",
    "        #{\"name\": f\"dr_model_6x6\", \"domain_randomization\": True, \"edit_levels\": False, \"easy_start\": False, \"grid_size\": 6},\n",
    "        #{\"name\": f\"plr_model_6x6\", \"domain_randomization\": False, \"edit_levels\": False, \"easy_start\": False, \"grid_size\": 6},\n",
    "        #{\"name\": f\"accel_model_6x6\", \"domain_randomization\": False, \"edit_levels\": True, \"easy_start\": False, \"grid_size\": 6},\n",
    "        #{\"name\": f\"accel_model_easy_6x6\", \"domain_randomization\": False, \"edit_levels\": True, \"easy_start\": True, \"grid_size\": 6},\n",
    "        #{\"name\": f\"dr_model_8x8\", \"domain_randomization\": True, \"edit_levels\": False, \"easy_start\": False, \"grid_size\": 8},\n",
    "        #{\"name\": f\"plr_model_8x8\", \"domain_randomization\": False, \"edit_levels\": False, \"easy_start\": False, \"grid_size\": 8},\n",
    "        #{\"name\": f\"accel_model_8x8\", \"domain_randomization\": False, \"edit_levels\": True, \"easy_start\": False, \"grid_size\": 8},\n",
    "        #{\"name\": f\"accel_model_easy_8x8\", \"domain_randomization\": False, \"edit_levels\": True, \"easy_start\": True, \"grid_size\": 8},\n",
    "        #{\"name\": f\"dr_model_10x10\", \"domain_randomization\": True, \"edit_levels\": False, \"easy_start\": False, \"grid_size\": 10},\n",
    "        #{\"name\": f\"plr_model_10x10\", \"domain_randomization\": False, \"edit_levels\": False, \"easy_start\": False, \"grid_size\": 10},\n",
    "        #{\"name\": f\"accel_model_10x10\", \"domain_randomization\": False, \"edit_levels\": True, \"easy_start\": False, \"grid_size\": 10},\n",
    "        #{\"name\": f\"accel_model_easy_10x10\", \"domain_randomization\": False, \"edit_levels\": True, \"easy_start\": True, \"grid_size\": 10},\n",
    "        #{\"name\": f\"dr_model_12x12\", \"domain_randomization\": True, \"edit_levels\": False, \"easy_start\": False, \"grid_size\": 12},\n",
    "        #{\"name\": f\"plr_model_12x12\", \"domain_randomization\": False, \"edit_levels\": False, \"easy_start\": False, \"grid_size\": 12},\n",
    "        #{\"name\": f\"accel_model_12x12\", \"domain_randomization\": False, \"edit_levels\": True, \"easy_start\": False, \"grid_size\": 12},\n",
    "        #{\"name\": f\"accel_model_easy_12x12\", \"domain_randomization\": False, \"edit_levels\": True, \"easy_start\": True, \"grid_size\": 12},     \n",
    "        {\"name\": f\"accel_model_easy_12x12_rep_skipped\", \"domain_randomization\": False, \"edit_levels\": True, \"easy_start\": True, \"grid_size\": 12},     \n",
    "    ]\n",
    "\n",
    "    # Run models with different configurations\n",
    "    for config_updates in configs:\n",
    "        config = copy.deepcopy(base_config)  # Ensure each config is independent\n",
    "        \n",
    "        # Update attributes and recalculate dependent parameters\n",
    "        for key, value in config_updates.items():\n",
    "            setattr(config, key, value)\n",
    "        config.update_dependent_params()  # Ensure recalculated values reflect changes\n",
    "\n",
    "        # Save the dataclass as a dictionary\n",
    "        with open(f\"models/{config.name}.json\", \"w\") as f:\n",
    "            json.dump(asdict(config), f)\n",
    "        \n",
    "        print(f\"Running {config.name} with config: {asdict(config)}\")    \n",
    "        main_accel_parallel(**asdict(config))\n",
    "        print(\"\\n\\n============================================\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopped at accel 10x10 (not finished)\n",
    "evalute_models(load_dim = 10, grid_size = 10, n_eval_episodes = 5, num_levels_per_difficulty = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models trained on 10x10 grid loaded successfully.\n",
      "Total rewards:\n",
      "DR: 0.00\n",
      "PLR: 0.00\n",
      "ACCEL: 0.00\n",
      "ACCEL-EasyStart: 0.95\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create gif folder if it does not exist\n",
    "if not os.path.exists(\"gifs\"):\n",
    "    os.makedirs(\"gifs\")\n",
    "\n",
    "# Load the models and evaluate them on a set of levels\n",
    "evaluate_and_create_gifs(grid_size=10, num_levels=10, num_blocks=0.30*10*10)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENV TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random level and visualize it\n",
    "random_cnf = random_config(12, num_blocks=12)\n",
    "print(\"random_cnf:\", random_cnf)\n",
    "print_level_from_config(random_cnf)\n",
    "\n",
    "# Edit the level and visualize it\n",
    "edited_cnf = edit_config(random_cnf)\n",
    "print(\"edited_cnf:\", edited_cnf)\n",
    "print_level_from_config(edited_cnf)\n",
    "\n",
    "# Edit the level and visualize it\n",
    "edited_cnf = edit_config(edited_cnf)\n",
    "print(\"edited_cnf:\", edited_cnf)\n",
    "print_level_from_config(edited_cnf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate using the standard sb3 evaluate_policy function\n",
    "\n",
    "model_dr = PPO.load(\"models/dr_model_6x6\")\n",
    "model_plr = PPO.load(\"models/plr_model_6x6\")\n",
    "model_accel = PPO.load(\"models/accel_model_6x6\")\n",
    "model_accel_easy = PPO.load(\"models/accel_model_easy_6x6\")\n",
    "\n",
    "models = {\"DR\": model_dr, \"PLR\": model_plr, \"ACCEL\": model_accel, \"ACCEL-EasyStart\": model_accel_easy}\n",
    "\n",
    "\n",
    "num_levels = 50\n",
    "\n",
    "configs = [random_config(6, seed=i) for i in range(num_levels)]\n",
    "\n",
    "total_reward = 0\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    total_reward = 0\n",
    "    for config in configs:\n",
    "        env = MyCustomGrid(config, solvable_only=True)\n",
    "        mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=1)\n",
    "        total_reward += mean_reward\n",
    "        #print(f\"Model: {model_name}, Reward: {mean_reward}\")\n",
    "    print(f\"Total reward over {num_levels} levels for {model_name}: {total_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate using the standard sb3 evaluate_policy function\n",
    "\n",
    "model_dr = PPO.load(\"models/dr_model_8x8\")\n",
    "model_plr = PPO.load(\"models/plr_model_8x8\")\n",
    "model_accel = PPO.load(\"models/accel_model_8x8\")\n",
    "model_accel_easy = PPO.load(\"models/accel_model_easy_8x8\")\n",
    "\n",
    "models = {\"DR\": model_dr, \"PLR\": model_plr, \"ACCEL\": model_accel, \"ACCEL-EasyStart\": model_accel_easy}\n",
    "\n",
    "\n",
    "num_levels = 100\n",
    "\n",
    "configs = [random_config(8, seed=i) for i in range(num_levels)]\n",
    "\n",
    "total_reward = 0\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    total_reward = 0\n",
    "    for config in configs:\n",
    "        env = MyCustomGrid(config, solvable_only=True)\n",
    "        mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=1)\n",
    "        total_reward += mean_reward\n",
    "        #print(f\"Model: {model_name}, Reward: {mean_reward}\")\n",
    "    print(f\"Total reward over {num_levels} levels for {model_name}: {total_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate using the standard sb3 evaluate_policy function\n",
    "\n",
    "model_dr = PPO.load(\"models/dr_model_10x10\")\n",
    "model_plr = PPO.load(\"models/plr_model_10x10\")\n",
    "model_accel = PPO.load(\"models/accel_model_10x10\")\n",
    "model_accel_easy = PPO.load(\"models/accel_model_easy_10x10\")\n",
    "\n",
    "models = {\"DR\": model_dr, \"PLR\": model_plr, \"ACCEL\": model_accel, \"ACCEL-EasyStart\": model_accel_easy}\n",
    "\n",
    "\n",
    "num_levels = 100\n",
    "\n",
    "configs = [random_config(10, seed=i) for i in range(num_levels)]\n",
    "\n",
    "total_reward = 0\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    total_reward = 0\n",
    "    for config in configs:\n",
    "        env = MyCustomGrid(config, solvable_only=True)\n",
    "        mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=1)\n",
    "        total_reward += mean_reward\n",
    "        #print(f\"Model: {model_name}, Reward: {mean_reward}\")\n",
    "    print(f\"Total reward over {num_levels} levels for {model_name}: {total_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate using the standard sb3 evaluate_policy function\n",
    "\n",
    "model_dr = PPO.load(\"models/dr_model_12x12\")\n",
    "model_plr = PPO.load(\"models/plr_model_12x12\")\n",
    "model_accel = PPO.load(\"models/accel_model_12x12\")\n",
    "model_accel_easy = PPO.load(\"models/accel_model_easy_12x12\")\n",
    "\n",
    "models = {\"DR\": model_dr, \"PLR\": model_plr, \"ACCEL\": model_accel, \"ACCEL-EasyStart\": model_accel_easy}\n",
    "\n",
    "\n",
    "num_levels = 100\n",
    "\n",
    "configs = [random_config(12, seed=i) for i in range(num_levels)]\n",
    "\n",
    "total_reward = 0\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    total_reward = 0\n",
    "    for config in configs:\n",
    "        env = MyCustomGrid(config, solvable_only=True)\n",
    "        mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=1)\n",
    "        total_reward += mean_reward\n",
    "        #print(f\"Model: {model_name}, Reward: {mean_reward}\")\n",
    "    print(f\"Total reward over {num_levels} levels for {model_name}: {total_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6x6\n",
    "Total reward over 50 levels for DR: 0.99\n",
    "Total reward over 50 levels for PLR: 26.52\n",
    "Total reward over 50 levels for ACCEL: 35.43\n",
    "Total reward over 50 levels for ACCEL-EasyStart: 38.45\n",
    "\n",
    "8x8\n",
    "Total reward over 100 levels for DR: 5.92\n",
    "Total reward over 100 levels for PLR: 23.92\n",
    "Total reward over 100 levels for ACCEL: 36.76\n",
    "Total reward over 100 levels for ACCEL-EasyStart: 44.45\n",
    "\n",
    "10x10\n",
    "Total reward over 100 levels for DR: 2.97\n",
    "Total reward over 100 levels for PLR: 13.76\n",
    "Total reward over 100 levels for ACCEL: 12.79\n",
    "Total reward over 100 levels for ACCEL-EasyStart: 31.01\n",
    "\n",
    "12x12\n",
    "Total reward over 100 levels for DR: 27.15\n",
    "Total reward over 100 levels for PLR: 23.96\n",
    "Total reward over 100 levels for ACCEL: 12.80\n",
    "Total reward over 100 levels for ACCEL-EasyStart: 26.82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "accel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
