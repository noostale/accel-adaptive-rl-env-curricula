{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACCEL IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import wandb\n",
    "import imageio\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from dataclasses import dataclass, asdict\n",
    "import copy\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from collections import deque\n",
    "from gymnasium.spaces import Box\n",
    "\n",
    "from minigrid.core.mission import MissionSpace\n",
    "from minigrid.core.world_object import Goal, Wall\n",
    "from minigrid.minigrid_env import MiniGridEnv, Grid\n",
    "from minigrid.wrappers import ImgObsWrapper\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "\n",
    "device = 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ====================================================\n",
    "# Custom MiniGrid Environment \n",
    "# ====================================================\n",
    "class MyCustomGrid(MiniGridEnv):\n",
    "    \"\"\"\n",
    "    Simple MiniGrid environment that places random wall tiles\n",
    "    according to a config dict, returning only the 'image' observation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config=None, solvable_only=False, **kwargs):\n",
    "        if config is None:\n",
    "            config = {}\n",
    "        self.config = config\n",
    "        self.solvable_only = solvable_only\n",
    "\n",
    "        # Create a random number generator with the custom seed\n",
    "        self.rng = np.random.default_rng(seed=self.config.get(\"seed_val\"))\n",
    "\n",
    "        mission_space = MissionSpace(mission_func=lambda: \"get to the green goal square\")\n",
    "\n",
    "        super().__init__(\n",
    "            grid_size=self.config['width'],\n",
    "            max_steps=self.config['width'] * self.config['height'] * 2, # max_steps is typically 2x the grid size\n",
    "            see_through_walls=False,\n",
    "            agent_view_size=5,                      # Size of the agent's view square\n",
    "            mission_space=mission_space,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # Manually define our observation_space as a single Box (the image).\n",
    "        # By default, MiniGrid's image shape is (view_size, view_size, 3) if using partial obs,\n",
    "        # or (height, width, 3) if using full-grid observation. We'll do full-grid here:\n",
    "        # We'll define (self.height, self.width, 3) as the shape.\n",
    "        # In practice, \"image\" shape can vary if partial observations are used.\n",
    "        self.observation_space = Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=(self.agent_view_size, self.agent_view_size, 3),\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "\n",
    "            \n",
    "    def _gen_grid(self, width, height):\n",
    "        \"\"\"\n",
    "        Generate a new environment layout ensuring solvability if required.\n",
    "        \"\"\"\n",
    "        \n",
    "        check_stuck = 0\n",
    "        while True:  # Keep regenerating until a solvable layout is found\n",
    "            self.grid = Grid(width, height)\n",
    "            self.grid.wall_rect(0, 0, width, height)\n",
    "\n",
    "            # Place the goal\n",
    "            goal_pos = self.config.get(\"goal_pos\")\n",
    "            if goal_pos is None:\n",
    "                while True:\n",
    "                    goal_r = self.rng.integers(1, height - 1)\n",
    "                    goal_c = self.rng.integers(1, width - 1)\n",
    "                    if self.grid.get(goal_c, goal_r) is None:\n",
    "                        self.put_obj(Goal(), goal_c, goal_r)\n",
    "                        self.config[\"goal_pos\"] = (goal_c, goal_r)\n",
    "                        break\n",
    "            else:\n",
    "                self.put_obj(Goal(), goal_pos[0], goal_pos[1])\n",
    "\n",
    "            # Place the agent\n",
    "            start_pos = self.config.get(\"start_pos\")\n",
    "            if start_pos is None:\n",
    "                while True:\n",
    "                    start_r = self.rng.integers(1, height - 1)\n",
    "                    start_c = self.rng.integers(1, width - 1)\n",
    "                    if self.grid.get(start_c, start_r) is None and (start_c, start_r) != self.config[\"goal_pos\"]:\n",
    "                        self.agent_pos = (start_c, start_r)\n",
    "                        self.agent_dir = self.rng.integers(0, 4)\n",
    "                        self.config[\"start_pos\"] = (start_c, start_r)\n",
    "                        break\n",
    "            else:\n",
    "                self.agent_pos = start_pos\n",
    "                self.agent_dir = self.rng.integers(0, 4)\n",
    "                self.config[\"start_pos\"] = start_pos\n",
    "            \n",
    "            placed_blocks = 0\n",
    "            \n",
    "            # Maximum number of tries to place the blocks\n",
    "            max_num_tries = 100\n",
    "            \n",
    "            # Place random walls using config parameters\n",
    "            while placed_blocks < self.config[\"num_blocks\"]:\n",
    "                max_num_tries -= 1\n",
    "                r = self.rng.integers(1, height - 1)\n",
    "                c = self.rng.integers(1, width - 1)\n",
    "                if max_num_tries <= 0:\n",
    "                    print(\"Could not place all blocks in the grid.\")\n",
    "                    break\n",
    "                if self.grid.get(c, r) is None and (c, r) != self.config[\"start_pos\"] and (c, r) != self.config[\"goal_pos\"]:\n",
    "                    self.put_obj(Wall(), c, r)\n",
    "                    placed_blocks += 1\n",
    "\n",
    "            # Check solvability if required\n",
    "            if not self.solvable_only or self._is_solvable():\n",
    "                break\n",
    "            \n",
    "            check_stuck += 1\n",
    "            if check_stuck > 50:\n",
    "                #print(\"Re-randomizing start and goal positions...\")\n",
    "                self.config.pop(\"start_pos\", None)\n",
    "                self.config.pop(\"goal_pos\", None)\n",
    "                self.rng = np.random.default_rng(seed=self.config.get(\"seed_val\") + check_stuck)\n",
    "\n",
    "        \n",
    "    def _is_solvable(self):\n",
    "        \"\"\"\n",
    "        Uses Breadth-First Search (BFS) to check if there's a path \n",
    "        from the agent's start position to the goal.\n",
    "        \"\"\"\n",
    "        start_pos = self.config[\"start_pos\"]\n",
    "        goal_pos = self.config[\"goal_pos\"]\n",
    "        if not start_pos or not goal_pos:\n",
    "            return False\n",
    "\n",
    "        queue = deque([start_pos])\n",
    "        visited = set()\n",
    "        visited.add(start_pos)\n",
    "\n",
    "        while queue:\n",
    "            x, y = queue.popleft()\n",
    "            if (x, y) == goal_pos:\n",
    "                return True\n",
    "\n",
    "            # Possible moves: up, down, left, right\n",
    "            for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n",
    "                nx, ny = x + dx, y + dy\n",
    "                cell_obj = self.grid.get(nx, ny)\n",
    "                if (\n",
    "                    1 <= nx < self.width - 1 and  # Stay within grid bounds\n",
    "                    1 <= ny < self.height - 1 and\n",
    "                    (nx, ny) not in visited and\n",
    "                    self.grid.get(nx, ny) is None or isinstance(cell_obj, Goal)\n",
    "                ):\n",
    "                    queue.append((nx, ny))\n",
    "                    visited.add((nx, ny))\n",
    "        return False  # No path found\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Override reset to ensure we only return the 'image' array\n",
    "        instead of a dict with 'image' and 'mission'.\n",
    "        \"\"\"\n",
    "        obs, info = super().reset(**kwargs)\n",
    "        obs = self._convert_obs(obs)\n",
    "        \n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Same for step: override to convert the dict observation into an image only.\n",
    "        \"\"\"\n",
    "        obs, reward, done, truncated, info = super().step(action)\n",
    "        obs = self._convert_obs(obs)\n",
    "        return obs, reward, done, truncated, info\n",
    "\n",
    "    def _convert_obs(self, original_obs):\n",
    "        \"\"\"\n",
    "        original_obs is typically {'image':..., 'mission':...}.\n",
    "        We'll just return original_obs['image'] to get a Box(low=0,high=255) shape.\n",
    "        \"\"\"\n",
    "        return original_obs[\"image\"]\n",
    "        #return np.transpose(original_obs[\"image\"], (2, 0, 1))\n",
    "    \n",
    "    def update_config(self, new_config):\n",
    "        \"\"\"\n",
    "        Update the environment configuration with a new config dict.\n",
    "        \"\"\"\n",
    "        self.config = new_config\n",
    "        self.reset()\n",
    "\n",
    "def random_config(grid_size, num_blocks=None, seed=None):\n",
    "    \"\"\"\n",
    "    Utility function to generate a random configuration dict.\n",
    "    \"\"\"\n",
    "    \n",
    "    max_blocks = int(((grid_size - 1) * (grid_size - 1)) / 2)\n",
    "    \n",
    "    if num_blocks is None:\n",
    "        num_blocks = np.random.randint(1, max_blocks)\n",
    "    else:\n",
    "        num_blocks = min(num_blocks, max_blocks)\n",
    "        \n",
    "    config = {\n",
    "        \"width\": grid_size,\n",
    "        \"height\": grid_size,\n",
    "        \"num_blocks\": num_blocks,\n",
    "        \"start_pos\": None,\n",
    "        \"goal_pos\": None,\n",
    "        \"edited\": False,\n",
    "        \"seed_val\": seed if seed is not None else np.random.randint(0, 1000)\n",
    "    }\n",
    "    \n",
    "    # Set the start and goal positions\n",
    "    env = MyCustomGrid(config)\n",
    "    \n",
    "    # Reset the environment to get the start and goal positions\n",
    "    env.reset()\n",
    "    \n",
    "    # Get the new config from the environment\n",
    "    config = env.config\n",
    "        \n",
    "    return config\n",
    "\n",
    "def print_level_from_config(config, solvable_only=False):\n",
    "    \"\"\"\n",
    "    Function to display a level configuration as an image.\n",
    "    \"\"\"\n",
    "    \n",
    "    env = MyCustomGrid(config, render_mode='rgb_array', solvable_only=solvable_only)\n",
    "    env.reset()\n",
    "    full_level_image = env.render()  # This should return an RGB image of the full grid\n",
    "\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(full_level_image)\n",
    "    plt.title(\"Level Configuration: \" + str(config))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "def edit_config(old_config):\n",
    "    \"\"\"\n",
    "    Make a little edit to an existing configuration.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Modify an existing configuration, adding randomness.\n",
    "    max_blocks = int(((old_config[\"width\"] - 1) * (old_config[\"height\"] - 1)) / 2)\n",
    "    \n",
    "    new_config = dict(old_config)\n",
    "    \n",
    "    # Randomly change the number of blocks\n",
    "    new_number_blocks = old_config[\"num_blocks\"] + 1\n",
    "    \n",
    "    # Ensure the number of blocks is within bounds\n",
    "    new_config[\"num_blocks\"] = max(1, min(new_number_blocks, max_blocks))    \n",
    "    \n",
    "    # Mark the config as edited\n",
    "    new_config[\"edited\"] = True\n",
    "    \n",
    "    return new_config\n",
    "\n",
    "def create_vectorized_env(config, n_envs=4, solvable_only=False):\n",
    "    \"\"\"\n",
    "    Create a vectorized environment with n parallel environments.\n",
    "    \"\"\"\n",
    "    return make_vec_env(lambda: MyCustomGrid(config, solvable_only), n_envs=n_envs, vec_env_cls=SubprocVecEnv)\n",
    "\n",
    "# ====================================================\n",
    "# Level Buffer\n",
    "# ====================================================\n",
    "class LevelBuffer: \n",
    "    def __init__(self, max_size=50):\n",
    "        self.max_size = max_size\n",
    "        self.data = []  # will store (config_dict, score)\n",
    "\n",
    "    def add(self, config, score):\n",
    "        self.data.append((config, score))\n",
    "        if len(self.data) > self.max_size:\n",
    "            self.data.sort(key=lambda x: x[1], reverse=True)\n",
    "            self.data = self.data[: self.max_size]\n",
    "            #it memorize only the highest score for each level\n",
    "\n",
    "    def sample_config(self): \n",
    "        # Samples a level from the buffer, weighting the probabilities \n",
    "        # based on the scores.\n",
    "        if len(self.data) == 0:\n",
    "            return None\n",
    "        scores = [item[1] for item in self.data]\n",
    "        total = sum(scores)\n",
    "        if total <= 1e-9:\n",
    "            # fallback to uniform\n",
    "            idx = np.random.randint(len(self.data))\n",
    "            return self.data[idx][0]\n",
    "        probs = [s / total for s in scores]\n",
    "        idx = np.random.choice(len(self.data), p=probs)\n",
    "        return self.data[idx][0]\n",
    "\n",
    "# ====================================================\n",
    "# Test Functions\n",
    "# ====================================================\n",
    "def load_models(grid_size):\n",
    "    \"\"\"Load the RL models.\"\"\"\n",
    "    model_dr = PPO.load(f\"models/dr_model_{grid_size}x{grid_size}\")\n",
    "    model_plr = PPO.load(f\"models/plr_model_{grid_size}x{grid_size}\")\n",
    "    model_accel = PPO.load(f\"models/accel_model_{grid_size}x{grid_size}\")\n",
    "    model_accel_easy = PPO.load(f\"models/accel_model_easy_{grid_size}x{grid_size}\")\n",
    "    print(f\"Models trained on {grid_size}x{grid_size} grid loaded successfully.\")\n",
    "    \n",
    "    return model_dr, model_plr, model_accel, model_accel_easy\n",
    "\n",
    "def evalute_models(load_dim = -1, grid_size = 6, n_eval_episodes = 5, num_levels_per_difficulty = 10):\n",
    "    \n",
    "    if load_dim > 0:\n",
    "        model_dr, model_plr, model_accel, model_accel_easy = load_models(load_dim)\n",
    "        \n",
    "    # Inseert the models in a dictionary\n",
    "    models = {\"DR\": model_dr, 'PLR': model_plr, 'ACCEL': model_accel, 'ACCEL-EasyStart': model_accel_easy}\n",
    "\n",
    "    # Generate n levels difficulties with increasing complexity, for each level generate m configs\n",
    "    difficulties = 3\n",
    "    num_levels_per_difficulty = num_levels_per_difficulty\n",
    "\n",
    "    levels = []\n",
    "    for i in range(difficulties):\n",
    "        level = []\n",
    "        for _ in range(num_levels_per_difficulty):\n",
    "            cfg = random_config(grid_size, num_blocks=grid_size*(i+1))\n",
    "            #print_level_from_config(cfg, solvable_only=True)\n",
    "            level.append(cfg)\n",
    "        levels.append(level)\n",
    "        \n",
    "    \n",
    "    # Create a dummy config to initialize the vectorized environment\n",
    "    dummy_config = random_config(grid_size)\n",
    "    env = create_vectorized_env(dummy_config, n_envs=4, solvable_only=True)\n",
    "\n",
    "    # Evaluate the model on the generated levels\n",
    "    results = {}\n",
    "    for model_name, model in models.items():\n",
    "        results[model_name] = []\n",
    "        for i, level in enumerate(levels):\n",
    "            print(f\"Evaluating {num_levels_per_difficulty} levels of difficulty {i + 1} with {grid_size*(i+1)} blocks on a {grid_size}x{grid_size} grid for model {model_name}, ratio of blocks to grid size: {grid_size*(i+1) / (grid_size*grid_size):.2f}\")\n",
    "            r = []\n",
    "            for j, cfg in enumerate(level):\n",
    "                # Update the environment with the new config\n",
    "                env.env_method(\"update_config\", cfg)\n",
    "                mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=n_eval_episodes, deterministic=True)\n",
    "                r.append(mean_reward)\n",
    "            results[model_name].append(r)\n",
    "        print()\n",
    "        \n",
    "    # Print mean rewards for each level\n",
    "    for model_name in models.keys():\n",
    "        print(f\"Model: {model_name}\")\n",
    "        for i, level in enumerate(levels):\n",
    "            print(f\"Level {i + 1} - Complexity {grid_size*(i+1)}: {np.mean(results[model_name][i]):.2f}\")\n",
    "        print()\n",
    "    \n",
    "    #Comute the number of xticks based on the number of models\n",
    "    xticks = [i for i in range(1, len(models.keys()) + 1)]\n",
    "\n",
    "    # Boxplot of results, a plot for each level complexity comparing models\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for i, level in enumerate(levels):\n",
    "        plt.subplot(1, difficulties, i + 1)\n",
    "        plt.boxplot([results[model_name][i] for model_name in models.keys()])\n",
    "        plt.xticks([1,2,3,4], [model_name for model_name in models.keys()])\n",
    "        plt.title(f\"Level {i + 1} - Complexity {grid_size*(i+1)}\")\n",
    "        plt.ylabel(\"Mean Reward\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig(f\"plots/boxplot_{load_dim}x{load_dim}.png\")\n",
    "    \n",
    "def test_model(model, config, gif_path=\"level.gif\"):\n",
    "    \"\"\"Evaluate a model on a given environment instance.\"\"\"\n",
    "    env = MyCustomGrid(config, render_mode='rgb_array', solvable_only=True)\n",
    "    obs, _ = env.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    total_reward = 0\n",
    "    i = 0\n",
    "\n",
    "    frames = []  # List to store frames\n",
    "\n",
    "    # Continue until either terminated or truncated is True\n",
    "    while not (terminated or truncated):\n",
    "        frame = env.render()  # Capture frame as an image\n",
    "        frames.append(Image.fromarray(frame))  # Convert to PIL image and store\n",
    "        \n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        i += 1\n",
    "        if i > 20:\n",
    "            break\n",
    "    \n",
    "    # Add to the gif also the last frame\n",
    "    frame = env.render()\n",
    "    frames.append(Image.fromarray(frame))\n",
    "\n",
    "    # Save frames as a GIF\n",
    "    if frames:\n",
    "        frames[0].save(\n",
    "            gif_path, save_all=True, append_images=frames[1:], duration=500, loop=0\n",
    "        )\n",
    "    return total_reward  # Do not close env here, as we reuse it\n",
    "\n",
    "def evaluate_and_create_gifs(grid_size, num_levels=50):\n",
    "    # Load the models\n",
    "    model_dr, model_plr, model_accel, model_accel_easy = load_models(grid_size)\n",
    "\n",
    "    # Test the models on the same exact environment instance\n",
    "    rewards = {\"DR\": 0, \"PLR\": 0, \"ACCEL\": 0, \"ACCEL-EasyStart\": 0}\n",
    "    for i in range(num_levels):\n",
    "        config = random_config(grid_size, seed=512 + i)\n",
    "\n",
    "        reward = test_model(model_dr, config, gif_path=f\"gifs/{grid_size}x{grid_size}_level_{i}_dr.gif\")\n",
    "        rewards[\"DR\"] += reward\n",
    "\n",
    "        reward = test_model(model_plr, config, gif_path=f\"gifs/{grid_size}x{grid_size}_level_{i}_plr.gif\")\n",
    "        rewards[\"PLR\"] += reward\n",
    "\n",
    "        reward = test_model(model_accel, config, gif_path=f\"gifs/{grid_size}x{grid_size}_level_{i}_accel.gif\")\n",
    "        rewards[\"ACCEL\"] += reward\n",
    "\n",
    "        reward = test_model(model_accel_easy, config, gif_path=f\"gifs/{grid_size}x{grid_size}_level_{i}_accel_easy.gif\")\n",
    "        rewards[\"ACCEL-EasyStart\"] += reward\n",
    "\n",
    "    print(\"Total rewards:\")\n",
    "    for model_name, reward in rewards.items():\n",
    "        print(f\"{model_name}: {reward:.2f}\")\n",
    "\n",
    "# ====================================================\n",
    "# Regret Calculation with Parallel Environments\n",
    "# ====================================================\n",
    "def calculate_regret_gae_vectorized(vec_env, model, max_steps, gamma, lam):\n",
    "    \"\"\"\n",
    "    Compute GAE-based regrets for each sub-environment in vec_env, in parallel.\n",
    "\n",
    "    - vec_env: A vectorized environment with n_envs sub-envs, each configured differently.\n",
    "    - model: A trained (or partially trained) stable-baselines PPO model.\n",
    "    - max_steps: Maximum steps to roll out each sub-environment.\n",
    "    - gamma, lam: Discount and GAE lambda for advantage/td-error calculation.\n",
    "    \n",
    "    Returns: A list of regrets, one for each sub-environment.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_envs = vec_env.num_envs\n",
    "\n",
    "    # Lists to store per-environment roll data\n",
    "    all_rewards = [[] for _ in range(n_envs)]\n",
    "    all_values = [[] for _ in range(n_envs)]\n",
    "    all_dones = [[] for _ in range(n_envs)]\n",
    "\n",
    "    # Initialize observations\n",
    "    obs_array = vec_env.reset()\n",
    "    obs_tensor = torch.as_tensor(obs_array, dtype=torch.float32, device=device)\n",
    "\n",
    "    # Track \"still alive\" sub-envs\n",
    "    done_flags = [False] * n_envs\n",
    "\n",
    "    # Roll out the environments in parallel\n",
    "    for step in range(max_steps):\n",
    "        # Stop if all environments are finished\n",
    "        if all(done_flags):\n",
    "            break\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # model.predict() can handle vectorized obs of shape (n_envs, obs_dim,...)\n",
    "            actions, _ = model.predict(obs_array, deterministic=True)\n",
    "            # Predict values for GAE computation\n",
    "            values_t = model.policy.predict_values(obs_tensor).squeeze(dim=-1)  # shape: (n_envs,)\n",
    "\n",
    "        # Step all environments in parallel\n",
    "        new_obs_array, rewards, dones, infos = vec_env.step(actions)\n",
    "\n",
    "        # Store rewards, values, and done flags\n",
    "        for i in range(n_envs):\n",
    "            if not done_flags[i]: \n",
    "                all_rewards[i].append(rewards[i])\n",
    "                all_values[i].append(values_t[i].item())\n",
    "                all_dones[i].append(dones[i])\n",
    "\n",
    "        # Mark newly finished envs\n",
    "        done_flags = [done_flags[i] or dones[i] for i in range(n_envs)]\n",
    "\n",
    "        # Update observations\n",
    "        obs_array = new_obs_array\n",
    "        obs_tensor = torch.as_tensor(obs_array, dtype=torch.float32, device=device)\n",
    "\n",
    "    # Compute terminal values (if environment ended early, assume terminal_value = 0)\n",
    "    with torch.no_grad():\n",
    "        terminal_values = model.policy.predict_values(obs_tensor).cpu().numpy().flatten()  # shape: (n_envs,)\n",
    "\n",
    "    # GAE-based regret computation\n",
    "    regrets = []\n",
    "    for i in range(n_envs):\n",
    "        rewards_i = all_rewards[i]\n",
    "        values_i = all_values[i]\n",
    "        dones_i = all_dones[i]\n",
    "\n",
    "        if not rewards_i:\n",
    "            # No trajectory (e.g., env ended at t=0)\n",
    "            regrets.append(0.0)\n",
    "            continue\n",
    "\n",
    "        # Set terminal value\n",
    "        last_val = 0.0 if dones_i[-1] else float(terminal_values[i])\n",
    "        values_i.append(last_val)\n",
    "\n",
    "        # Compute GAE-based regret: sum of positive deltas\n",
    "        env_regrets = []\n",
    "        discount_factor = 1.0  # Tracks (gamma * lam)^t accumulation\n",
    "\n",
    "        for t in range(len(rewards_i)):\n",
    "            delta_t = rewards_i[t] + gamma * values_i[t + 1] * (1 - dones_i[t]) - values_i[t]\n",
    "            discounted_delta = discount_factor * delta_t\n",
    "            env_regrets.append(max(discounted_delta, 0.0))\n",
    "\n",
    "            # Update discount factor for next step\n",
    "            discount_factor *= gamma * lam\n",
    "\n",
    "        regrets.append(max(env_regrets) if env_regrets else 0.0)\n",
    "\n",
    "    return regrets\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# ACCEL Main Function with Parallel Environments\n",
    "# ====================================================\n",
    "def main_accel_parallel(total_iterations, replay_prob, train_steps, level_buffer_size,\n",
    "               initial_fill_size, grid_size, n_envs, edit_levels, regret_threshold,\n",
    "               easy_start, domain_randomization, name, test_every_n_iterations, test_on_levels,\n",
    "               lr, batch_size):\n",
    "    \n",
    "    # Initialize Weights and Biases\n",
    "    wandb.init(project=\"accel\", config=config, mode=\"disabled\")\n",
    "\n",
    "    \n",
    "    def plot_display_avg_rew_list(avg_list):\n",
    "        plt.plot(avg_list)\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylabel(\"Average Reward\")\n",
    "        plt.title(\"Average Reward Progress during Training\")\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f\"plots/avg_reward_progress_{name}_{grid_size}x{grid_size}.png\")\n",
    "        plt.show()\n",
    "    \n",
    "    # Create a level buffer, a personal class to store levels and scores\n",
    "    level_buffer = LevelBuffer(max_size=level_buffer_size)\n",
    "    \n",
    "    # Generate a random configuration {width, height, num_blocks, start_pos, goal_pos}\n",
    "    dummy_config = random_config(grid_size)\n",
    "    \n",
    "    # Create a vectorized environment, so a wrapper for MyCustomGrid that allows interconnection \n",
    "    # between gymnasium and stable-baselines3 to train the model in a vectorized way, since we\n",
    "    # are using DummyVecEnv, it is not true parallelism\n",
    "    vectorized_env = create_vectorized_env(dummy_config, n_envs=n_envs)\n",
    "\n",
    "    # Initialize PPO with vectorized environment\n",
    "    print(\"Initializing student model PPO...\")\n",
    "    student_model = PPO(\n",
    "                \"MlpPolicy\",                    # Multi-layer perceptron policy\n",
    "                vectorized_env,                 # The environment to train on\n",
    "                verbose=0,                      # Display training output\n",
    "                n_steps=512,                            # Number of steps to run for each environment per update\n",
    "                batch_size=batch_size,                 # Minibatch size for each gradient update\n",
    "                learning_rate=lr,             # Learning rate for optimizer\n",
    "                device=device                   # Use GPU if available\n",
    "            )\n",
    "\n",
    "    avg_rewards_list = []\n",
    "            \n",
    "    # ====================================================\n",
    "    # Initial buffer fill\n",
    "    # ====================================================\n",
    "    if not domain_randomization:\n",
    "        \n",
    "        print(f\"Populating buffer with {initial_fill_size} initial levels with regret > {regret_threshold}...\")\n",
    "        \n",
    "        while len(level_buffer.data) < initial_fill_size:\n",
    "            # Generate n_envs configs\n",
    "            configs = []\n",
    "            for _ in range(n_envs):\n",
    "                if easy_start:\n",
    "                    cfg = random_config(grid_size, num_blocks=2, seed=42+len(level_buffer.data))\n",
    "                else:\n",
    "                    cfg = random_config(grid_size, seed=42+len(level_buffer.data))\n",
    "                configs.append(cfg)\n",
    "\n",
    "            # Set every environment with a different config\n",
    "            for i in range(n_envs):\n",
    "                vectorized_env.env_method(\"update_config\", configs[i], indices=i)\n",
    "                \n",
    "            # Train the model using the vectorized environment            \n",
    "            student_model.learn(total_timesteps=train_steps)\n",
    "            \n",
    "            # Calculate regret for each environment\n",
    "            regrets = calculate_regret_gae_vectorized(vectorized_env, student_model, max_steps=1000, gamma=0.99, lam=0.95)\n",
    "            \n",
    "            # Add the configs with regret > threshold to the buffer\n",
    "            for i in range(n_envs):\n",
    "                cfg = configs[i]\n",
    "                regret = regrets[i]\n",
    "                if regret < regret_threshold: continue\n",
    "                level_buffer.add(cfg, regret)\n",
    "\n",
    "    # ====================================================\n",
    "    # Main ACCEL loop\n",
    "    # ====================================================\n",
    "    \n",
    "    iteration_regrets = []\n",
    "    iteration, skipped = 0, 0\n",
    "    \n",
    "    print(\"\\nMain training loop...\")\n",
    "    while iteration < total_iterations + skipped:\n",
    "    \n",
    "        # ====================================================\n",
    "        # Domain Randomization\n",
    "        # ====================================================\n",
    "        \n",
    "        if domain_randomization:\n",
    "            while iteration < total_iterations:\n",
    "\n",
    "                if iteration % test_every_n_iterations == 0:\n",
    "                    rewards = 0\n",
    "                    for i in range(test_on_levels):\n",
    "                        test_config = random_config(grid_size=grid_size, seed=12945 + i)\n",
    "                        reward = test_model(student_model, test_config)\n",
    "                        rewards += reward\n",
    "                    print(f\"Reward over {test_on_levels} levels: {rewards}\")\n",
    "                    avg_rewards_list.append(rewards)\n",
    "\n",
    "                print(f\"\\n=== ITERATION {iteration + 1}/{total_iterations} ===\")\n",
    "                \n",
    "                # Create n_envs random configs\n",
    "                configs = []\n",
    "                for _ in range(n_envs):\n",
    "                    cfg = random_config(grid_size, seed=42+iteration)\n",
    "                    configs.append(cfg)\n",
    "                \n",
    "                # Set every environment with a different config\n",
    "                for i in range(n_envs):\n",
    "                    vectorized_env.env_method(\"update_config\", configs[i], indices=i)\n",
    "\n",
    "                # Train the model using the vectorized environment\n",
    "                student_model.learn(total_timesteps=train_steps)\n",
    "                iteration += n_envs\n",
    "                \n",
    "            student_model.save(f\"models/{name}\")\n",
    "            print(f\"Model saved as models/{name}\")\n",
    "\n",
    "            print(\"The average rewards for each iteration are:\")\n",
    "            print(avg_rewards_list)\n",
    "            \n",
    "            plot_display_avg_rew_list(avg_rewards_list)\n",
    "\n",
    "            # save to a file\n",
    "            with open(f\"rewards/{name}.txt\", \"w\") as f:\n",
    "                for item in avg_rewards_list:\n",
    "                    f.write(\"%s\\n\" % item)\n",
    "\n",
    "            return student_model\n",
    "\n",
    "        \n",
    "        print(f\"\\n=== ITERATION {iteration + 1}/{total_iterations + skipped} SKIPPED: {skipped} ===\")\n",
    "\n",
    "\n",
    "        if iteration % test_every_n_iterations == 0:\n",
    "            rewards = 0\n",
    "            for i in range(test_on_levels):\n",
    "                test_config = random_config(grid_size=grid_size, seed=12945 + i)\n",
    "                reward = test_model(student_model, test_config)\n",
    "                rewards += reward\n",
    "            print(f\"Reward over {test_on_levels} levels: {rewards}\")\n",
    "            avg_rewards_list.append(rewards)\n",
    "        \n",
    "        \n",
    "        # Decide whether to replay or generate a new level for ench n_envs environments\n",
    "        use_replay = [np.random.rand() < replay_prob for _ in range(n_envs)]\n",
    "        configs = []\n",
    "        for replay_decision in use_replay:\n",
    "            if not replay_decision or len(level_buffer.data) == 0:\n",
    "                # Create a new random level\n",
    "                cfg = random_config(grid_size, seed=42+iteration)\n",
    "                #print(\"Generated new random level:\", cfg)\n",
    "            else:\n",
    "                # Sample a level from the buffer\n",
    "                cfg = level_buffer.sample_config()\n",
    "                #print(\"Sampled level from buffer:\", cfg)\n",
    "            configs.append(cfg)\n",
    "        \n",
    "        for i in range(n_envs):\n",
    "            # Update the vectorized environment with the selected config and train the model\n",
    "            vectorized_env.env_method(\"update_config\", configs[i], indices=i)\n",
    "        \n",
    "        \n",
    "        student_model.learn(total_timesteps=train_steps)\n",
    "        \n",
    "        \"\"\"wandb.log({\n",
    "            \"iteration\": iteration,\n",
    "            \"regret_score\": regret,\n",
    "            \"regret_threshold\": regret_threshold,\n",
    "            \"buffer_size\": len(level_buffer.data),\n",
    "            \"value_loss\": student_model.logger.name_to_value[\"train/value_loss\"],\n",
    "            \"entropy_loss\": student_model.logger.name_to_value[\"train/entropy_loss\"],\n",
    "            \"policy_loss\": student_model.logger.name_to_value[\"train/policy_loss\"],\n",
    "        })\"\"\"\n",
    "\n",
    "        for i in range(n_envs):\n",
    "            if use_replay[i] and edit_levels:\n",
    "                configs[i] = edit_config(configs[i])\n",
    "                #print(\"Edited level to:\", configs[i])\n",
    "\n",
    "\n",
    "        regrets = calculate_regret_gae_vectorized(vectorized_env, student_model, max_steps=2048, gamma=0.99, lam=0.95)\n",
    "        \n",
    "        # Pair each configuration with its computed regret\n",
    "        results = list(zip(configs, regrets))\n",
    "        for cfg_i, regret in results:\n",
    "            if regret <= regret_threshold:\n",
    "                #print(f\"Regret for current level is {regret:.5f} <= threshold {regret_threshold:.5f}. Skipping...\")\n",
    "                skipped += 1\n",
    "                #iteration_regrets.append(regret)\n",
    "                continue\n",
    "            else:\n",
    "                #print(f\"Regret for current level: {regret}, buffer size: {len(level_buffer.data)}\")\n",
    "                iteration_regrets.append(regret)\n",
    "                level_buffer.add(cfg_i, regret)\n",
    "        \n",
    "        iteration += n_envs\n",
    "        \n",
    "    # close the environment\n",
    "    vectorized_env.close()\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    # ====================================================\n",
    "    # Plotting and Saving\n",
    "    # ====================================================\n",
    "    \n",
    "    \n",
    "    # Plot and display regret progress\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(iteration_regrets, marker='o')\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Regret\")\n",
    "    plt.title(\"Regret Progress during Training\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"plots/regret_progress_{name}_{grid_size}x{grid_size}.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nDone. Final buffer size:\", len(level_buffer.data))\n",
    "    print(\"Top-5 hardest levels (config, regret):\")\n",
    "    level_buffer.data.sort(key=lambda x: x[1], reverse=True)\n",
    "    for i, (cfg, sc) in enumerate(level_buffer.data[:5]):\n",
    "        print(f\"{i + 1}. regret={sc:.5f}, config={cfg}\")\n",
    "        #print_level_from_config(cfg)\n",
    "        \n",
    "    print(\"Top-5 easiest levels (config, regret):\")\n",
    "    level_buffer.data.sort(key=lambda x: x[1])\n",
    "    for i, (cfg, sc) in enumerate(level_buffer.data[:5]):\n",
    "        print(f\"{i + 1}. regret={sc:.5f}, config={cfg}\")\n",
    "        #print_level_from_config(cfg)\n",
    "    \n",
    "    wandb.finish()\n",
    "    \n",
    "    # Save the model\n",
    "    student_model.save(f\"models/{name}\")\n",
    "    print(f\"Model saved as models/{name}\")\n",
    "\n",
    "    print(\"The average rewards for each iteration are:\")\n",
    "    print(avg_rewards_list)\n",
    "    \n",
    "    \n",
    "    plot_display_avg_rew_list(avg_rewards_list)\n",
    "\n",
    "    \n",
    "    # save to a file\n",
    "    with open(f\"rewards/{name}.txt\", \"w\") as f:\n",
    "        for item in avg_rewards_list:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "\n",
    "    return student_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running dr_model_6x6 with config: {'name': 'dr_model_6x6', 'grid_size': 6, 'train_steps': 216, 'total_iterations': 216, 'test_every_n_iterations': 12, 'replay_prob': 0.8, 'level_buffer_size': 256, 'initial_fill_size': 128, 'regret_threshold': 0.0, 'n_envs': 6, 'test_on_levels': 100, 'edit_levels': False, 'easy_start': False, 'domain_randomization': True, 'lr': 0.001, 'batch_size': 128}\n",
      "Initializing student model PPO...\n",
      "\n",
      "Main training loop...\n",
      "Reward over 100 levels: 0\n",
      "\n",
      "=== ITERATION 1/216 ===\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 62\u001b[0m\n\u001b[0;32m     59\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;28mstr\u001b[39m(config))\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with config: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00masdict(config)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)    \n\u001b[1;32m---> 62\u001b[0m \u001b[43mmain_accel_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43masdict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m============================================\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 670\u001b[0m, in \u001b[0;36mmain_accel_parallel\u001b[1;34m(total_iterations, replay_prob, train_steps, level_buffer_size, initial_fill_size, grid_size, n_envs, edit_levels, regret_threshold, easy_start, domain_randomization, name, test_every_n_iterations, test_on_levels, lr, batch_size)\u001b[0m\n\u001b[0;32m    667\u001b[0m         vectorized_env\u001b[38;5;241m.\u001b[39menv_method(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdate_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, configs[i], indices\u001b[38;5;241m=\u001b[39mi)\n\u001b[0;32m    669\u001b[0m     \u001b[38;5;66;03m# Train the model using the vectorized environment\u001b[39;00m\n\u001b[1;32m--> 670\u001b[0m     \u001b[43mstudent_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    671\u001b[0m     iteration \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m n_envs\n\u001b[0;32m    673\u001b[0m student_model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\noost\\anaconda3\\envs\\accel\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\noost\\anaconda3\\envs\\accel\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:323\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 323\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[0;32m    326\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\noost\\anaconda3\\envs\\accel\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:202\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;66;03m# Convert to pytorch tensor or to TensorDict\u001b[39;00m\n\u001b[0;32m    201\u001b[0m     obs_tensor \u001b[38;5;241m=\u001b[39m obs_as_tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 202\u001b[0m     actions, values, log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    203\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    205\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\noost\\anaconda3\\envs\\accel\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\noost\\anaconda3\\envs\\accel\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\noost\\anaconda3\\envs\\accel\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:645\u001b[0m, in \u001b[0;36mActorCriticPolicy.forward\u001b[1;34m(self, obs, deterministic)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;124;03mForward pass in all the networks (actor and critic)\u001b[39;00m\n\u001b[0;32m    639\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;124;03m:return: action, value and log probability of the action\u001b[39;00m\n\u001b[0;32m    643\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    644\u001b[0m \u001b[38;5;66;03m# Preprocess the observation if needed\u001b[39;00m\n\u001b[1;32m--> 645\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    646\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare_features_extractor:\n\u001b[0;32m    647\u001b[0m     latent_pi, latent_vf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_extractor(features)\n",
      "File \u001b[1;32mc:\\Users\\noost\\anaconda3\\envs\\accel\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:672\u001b[0m, in \u001b[0;36mActorCriticPolicy.extract_features\u001b[1;34m(self, obs, features_extractor)\u001b[0m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;124;03mPreprocess the observation if needed and extract features.\u001b[39;00m\n\u001b[0;32m    665\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;124;03m    features for the actor and the features for the critic.\u001b[39;00m\n\u001b[0;32m    670\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare_features_extractor:\n\u001b[1;32m--> 672\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures_extractor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeatures_extractor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeatures_extractor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m features_extractor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\noost\\anaconda3\\envs\\accel\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:130\u001b[0m, in \u001b[0;36mBaseModel.extract_features\u001b[1;34m(self, obs, features_extractor)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs: PyTorchObs, features_extractor: BaseFeaturesExtractor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    123\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03m    Preprocess the observation if needed and extract features.\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    :return: The extracted features\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m     preprocessed_obs \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_obs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize_images\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_images\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m features_extractor(preprocessed_obs)\n",
      "File \u001b[1;32mc:\\Users\\noost\\anaconda3\\envs\\accel\\Lib\\site-packages\\stable_baselines3\\common\\preprocessing.py:120\u001b[0m, in \u001b[0;36mpreprocess_obs\u001b[1;34m(obs, observation_space, normalize_images)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation_space, spaces\u001b[38;5;241m.\u001b[39mBox):\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m normalize_images \u001b[38;5;129;01mand\u001b[39;00m is_image_space(observation_space):\n\u001b[1;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obs\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation_space, spaces\u001b[38;5;241m.\u001b[39mDiscrete):\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;66;03m# One hot encoding and convert to float to avoid errors\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    name: str = \"model\"\n",
    "    grid_size: int = 6  # Grid size\n",
    "    \n",
    "    # Dynamic parameters\n",
    "    train_steps: int = None\n",
    "    total_iterations: int = None\n",
    "    test_every_n_iterations: int = None\n",
    "\n",
    "    replay_prob: float = 0.8\n",
    "    level_buffer_size: int = 256\n",
    "    initial_fill_size: int = 128\n",
    "    regret_threshold: float = 0.00\n",
    "    n_envs: int = 6\n",
    "    test_on_levels: int = 100\n",
    "    edit_levels: bool = True\n",
    "    easy_start: bool = True\n",
    "    domain_randomization: bool = False\n",
    "    lr: float = 1e-3\n",
    "    batch_size: int = 128\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"Dynamically calculate train_steps based on grid_size.\"\"\"\n",
    "        self.train_steps = self.grid_size * self.grid_size * self.n_envs                # grid_size^2 * n_envs\n",
    "        self.test_every_n_iterations = self.n_envs * 2                                  # test every n_envs * 2 iterations\n",
    "        self.total_iterations = self.grid_size * self.grid_size * self.grid_size        # grid_size^3\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_config = Config()  # elements are set to default values and dynamically calculated\n",
    "\n",
    "    # Define different configurations\n",
    "    configs = [\n",
    "        {\"name\": f\"dr_model_6x6\", \"domain_randomization\": True, \"edit_levels\": False, \"easy_start\": False, \"grid_size\": 6},\n",
    "        {\"name\": f\"plr_model_6x6\", \"domain_randomization\": False, \"edit_levels\": False, \"easy_start\": False, \"grid_size\": 6},\n",
    "        {\"name\": f\"accel_model_6x6\", \"domain_randomization\": False, \"edit_levels\": True, \"easy_start\": False, \"grid_size\": 6},\n",
    "        {\"name\": f\"accel_model_easy_6x6\", \"domain_randomization\": False, \"edit_levels\": True, \"easy_start\": True, \"grid_size\": 6},\n",
    "        {\"name\": f\"dr_model_8x8\", \"domain_randomization\": True, \"edit_levels\": False, \"easy_start\": False, \"grid_size\": 8},\n",
    "        {\"name\": f\"plr_model_8x8\", \"domain_randomization\": False, \"edit_levels\": False, \"easy_start\": False, \"grid_size\": 8},\n",
    "        {\"name\": f\"accel_model_8x8\", \"domain_randomization\": False, \"edit_levels\": True, \"easy_start\": False, \"grid_size\": 8},\n",
    "        {\"name\": f\"accel_model_easy_8x8\", \"domain_randomization\": False, \"edit_levels\": True, \"easy_start\": True, \"grid_size\": 8},\n",
    "        {\"name\": f\"dr_model_10x10\", \"domain_randomization\": True, \"edit_levels\": False, \"easy_start\": False, \"grid_size\": 10},\n",
    "        {\"name\": f\"plr_model_10x10\", \"domain_randomization\": False, \"edit_levels\": False, \"easy_start\": False, \"grid_size\": 10},\n",
    "        {\"name\": f\"accel_model_10x10\", \"domain_randomization\": False, \"edit_levels\": True, \"easy_start\": False, \"grid_size\": 10},\n",
    "        {\"name\": f\"accel_model_easy_10x10\", \"domain_randomization\": False, \"edit_levels\": True, \"easy_start\": True, \"grid_size\": 10},\n",
    "        {\"name\": f\"dr_model_12x12\", \"domain_randomization\": True, \"edit_levels\": False, \"easy_start\": False, \"grid_size\": 12},\n",
    "        {\"name\": f\"plr_model_12x12\", \"domain_randomization\": False, \"edit_levels\": False, \"easy_start\": False, \"grid_size\": 12},\n",
    "        {\"name\": f\"accel_model_12x12\", \"domain_randomization\": False, \"edit_levels\": True, \"easy_start\": False, \"grid_size\": 12},\n",
    "        {\"name\": f\"accel_model_easy_12x12\", \"domain_randomization\": False, \"edit_levels\": True, \"easy_start\": True, \"grid_size\": 12},     \n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "    # Run models with different configurations\n",
    "    for config_updates in configs:\n",
    "        config = copy.deepcopy(base_config)  # Ensure each config is independent\n",
    "        for key, value in config_updates.items():\n",
    "            setattr(config, key, value)\n",
    "        \n",
    "        # Save the dict as a txt file in models/name.txt\n",
    "        with open(f\"models/{config.name}.txt\", \"w\") as f:\n",
    "            f.write(str(config))\n",
    "        \n",
    "        print(f\"Running {config.name} with config: {asdict(config)}\")    \n",
    "        main_accel_parallel(**asdict(config))\n",
    "        print(\"\\n\\n============================================\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "evalute_models(load_dim = 8, grid_size = 8, n_eval_episodes = 5, num_levels_per_difficulty = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the models and evaluate them on a set of levels\n",
    "evaluate_and_create_gifs(grid_size=6, num_levels=50)\n",
    "print()\n",
    "evaluate_and_create_gifs(grid_size=8, num_levels=50)\n",
    "print()\n",
    "evaluate_and_create_gifs(grid_size=10, num_levels=50)\n",
    "print()\n",
    "evaluate_and_create_gifs(grid_size=12, num_levels=50)\n",
    "print()\n",
    "evaluate_and_create_gifs(grid_size=14, num_levels=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENV TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random level and visualize it\n",
    "random_cnf = random_config(8)\n",
    "print(\"random_cnf:\", random_cnf)\n",
    "\n",
    "print_level_from_config(random_cnf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "accel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
