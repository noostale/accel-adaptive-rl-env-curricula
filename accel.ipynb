{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACCEL - Adversarially Compounding Complexity by Editing Levels\n",
    "\n",
    "### 1.1 Unsupervised Environment Design (UED)\n",
    "UED addresses the question: **How can we generate a curriculum of environments (levels) that maximally challenge an agent so that it generalizes well?**\n",
    "\n",
    "- Levels are parameterized environments (e.g., different maze layouts, obstacle configurations, etc.).\n",
    "- A teacher (level-generator) creates or selects levels that push the agent (the student).\n",
    "- A regret-based objective is often used to identify the levels that the agent finds hardest.\n",
    "\n",
    "### 1.2 Regret & Minimax Regret\n",
    "- Regret is the difference between the return the optimal policy *could achieve* on a given level vs. what the current agent’s policy achieves.\n",
    "- Minimax Regret means the agent’s policy minimizes the worst-case regret across all levels. In practice, this encourages the agent to be robust to the most difficult levels.\n",
    "\n",
    "### 1.3 PLR (Prioritized Level Replay)\n",
    "PLR is a simpler form of UED that:\n",
    "\n",
    "- Randomly samples levels from a design space.\n",
    "- Scores each level by an approximate regret measure (e.g., large “TD-error” or “value loss” = “hard” level).\n",
    "- Curates a buffer of the highest-scoring (hardest) levels.\n",
    "- Trains the agent only (or primarily) on those curated levels.\n",
    "\n",
    "\n",
    "### 1.4 ACCEL (Adversarially Compounding Complexity by Editing Levels) \n",
    "ACCEL extends PLR by **evolving**  previously discovered levels rather than always sampling new random ones. Specifically: \n",
    "1. **Start**  with a generator that can randomly produce levels.\n",
    " \n",
    "2. **Keep**  a replay buffer $\\Lambda$ of highest-regret levels (like PLR).\n",
    " \n",
    "3. **Replay**  from $\\Lambda$ (train on those levels).\n",
    " \n",
    "4. **Edit (mutate)**  these replayed levels—small changes that typically make them *more* challenging.\n",
    " \n",
    "5. **Re-evaluate**  mutated levels for regret. If they’re also high-regret, add them to $\\Lambda$.\n",
    " \n",
    "6. **Iterate**  this process, thus “compounding complexity” because each level can build on previously discovered “frontier” levels.\n",
    "\n",
    "**Outcome** : Over time, the environment buffer $\\Lambda$ accumulates an increasingly difficult and diverse set of levels, pushing the agent’s capabilities further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example usage of gymnasium with minigrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Step through the environment with the chosen action\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Render the environment after each action\u001b[39;00m\n\u001b[0;32m     22\u001b[0m env\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[1;32mc:\\Users\\Emanuele\\miniconda3\\envs\\accel\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Emanuele\\miniconda3\\envs\\accel\\Lib\\site-packages\\gymnasium\\core.py:322\u001b[0m, in \u001b[0;36mWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[0;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    321\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Emanuele\\miniconda3\\envs\\accel\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Emanuele\\miniconda3\\envs\\accel\\Lib\\site-packages\\minigrid\\minigrid_env.py:591\u001b[0m, in \u001b[0;36mMiniGridEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    588\u001b[0m     truncated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    590\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 591\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    593\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen_obs()\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obs, reward, terminated, truncated, {}\n",
      "File \u001b[1;32mc:\\Users\\Emanuele\\miniconda3\\envs\\accel\\Lib\\site-packages\\minigrid\\minigrid_env.py:781\u001b[0m, in \u001b[0;36mMiniGridEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow\u001b[38;5;241m.\u001b[39mblit(bg, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m    780\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[1;32m--> 781\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    782\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[0;32m    784\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import minigrid\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"MiniGrid-Empty-5x5-v0\", render_mode=\"human\")\n",
    "\n",
    "# Reset the environment to start a new episode\n",
    "obs, info = env.reset()\n",
    "\n",
    "# Render the initial state of the environment\n",
    "env.render()\n",
    "\n",
    "# Take num_steps random actions in the environment\n",
    "for _ in range(50):\n",
    "    # Sample a random action\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # Step through the environment with the chosen action\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    # Render the environment after each action\n",
    "    env.render()\n",
    "    \n",
    "    # Check if the episode is done\n",
    "    if terminated or truncated:\n",
    "        print(\"Episode finished\")\n",
    "        break\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example usage of gymnasium with CartPole-v1 and PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to ./ppo_cartpole_tensorboard/PPO_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Emanuele\\miniconda3\\envs\\accel\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2000, episode_reward=112.60 +/- 13.18\n",
      "Episode length: 112.60 +/- 13.18\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 113      |\n",
      "|    mean_reward     | 113      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=126.20 +/- 37.93\n",
      "Episode length: 126.20 +/- 37.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 126      |\n",
      "|    mean_reward     | 126      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=141.40 +/- 35.57\n",
      "Episode length: 141.40 +/- 35.57\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 141      |\n",
      "|    mean_reward     | 141      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=95.80 +/- 9.15\n",
      "Episode length: 95.80 +/- 9.15\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 95.8     |\n",
      "|    mean_reward     | 95.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.2     |\n",
      "|    ep_rew_mean     | 22.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 2220     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# Create the Gymnasium environment\n",
    "env_id = \"CartPole-v1\"\n",
    "env = gym.make(env_id)\n",
    "\n",
    "# Optionally, you can create a vectorized environment for parallel training\n",
    "# This can speed up training by using multiple environments simultaneously\n",
    "# Here, we create 4 parallel environments\n",
    "vec_env = make_vec_env(env_id, n_envs=4)\n",
    "\n",
    "# Initialize the PPO agent\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",          # Multi-layer Perceptron policy\n",
    "    vec_env,              # Vectorized environment\n",
    "    verbose=1,            # Verbosity level (0: no output, 1: info)\n",
    "    tensorboard_log=\"./ppo_cartpole_tensorboard/\"  # Path for TensorBoard logs\n",
    ")\n",
    "\n",
    "# Set up an evaluation callback to monitor the agent's performance\n",
    "eval_env = gym.make(env_id)\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path='./logs/',\n",
    "    log_path='./logs/',\n",
    "    eval_freq=500,        # Evaluate the agent every 500 steps\n",
    "    n_eval_episodes=5,    # Number of episodes for evaluation\n",
    "    deterministic=True,\n",
    "    render=False\n",
    ")\n",
    "\n",
    "# Train the agent for a total of 100,000 steps, use the eval_callback every 500 steps\n",
    "model.learn(total_timesteps=100, callback=eval_callback)\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"ppo_cartpole\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = 283.0\n",
      "Episode 2: Total Reward = 995.0\n",
      "Episode 3: Total Reward = 397.0\n",
      "Episode 4: Total Reward = 423.0\n",
      "Episode 5: Total Reward = 601.0\n",
      "Episode 6: Total Reward = 124.0\n",
      "Episode 7: Total Reward = 278.0\n",
      "Episode 8: Total Reward = 1015.0\n",
      "Episode 9: Total Reward = 171.0\n",
      "Episode 10: Total Reward = 132.0\n"
     ]
    }
   ],
   "source": [
    "# Load the environment\n",
    "env_id = \"CartPole-v1\"\n",
    "env = gym.make(env_id, render_mode=\"human\")\n",
    "\n",
    "# To demonstrate loading the model, you can reload it as follows:\n",
    "model = PPO.load(\"ppo_cartpole\", env=vec_env)\n",
    "\n",
    "# Evaluate the trained agent\n",
    "episodes = 10\n",
    "for episode in range(1, episodes + 1):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        # Optionally, render the environment (requires a display)\n",
    "        # env.render()\n",
    "    print(f\"Episode {episode}: Total Reward = {total_reward}\")\n",
    "\n",
    "# Close the environments\n",
    "env.close()\n",
    "eval_env.close()\n",
    "vec_env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input**:  \n",
    "- Level buffer size $K$, initial fill ratio $\\rho$, level generator\n",
    "\n",
    "**Initialize**:  \n",
    "1. Initialize policy $\\pi(\\phi)$\n",
    "2. Initialize level buffer $\\Lambda$\n",
    "3. Sample $K \\cdot \\rho$ initial levels to populate $\\Lambda$\n",
    "\n",
    "**while** not converged **do**  \n",
    "   1. Sample replay decision $d \\sim P_D(d)$  \n",
    "   2. **if** $d = 0$ **then**  \n",
    "      - Sample level $\\theta$ from level generator  \n",
    "      - Collect $\\pi$'s trajectory $\\tau$ on $\\theta$, with stop-gradient $\\phi^\\perp$  \n",
    "      - Compute regret score $S$ for $\\theta$ (Equation 5)  \n",
    "      - Update $\\Lambda$ with $\\theta$ if score $S$ meets threshold  \n",
    "   3. **else**  \n",
    "      - Sample a replay level, $\\theta \\sim \\Lambda$  \n",
    "      - Collect policy trajectory $\\tau$ on $\\theta$  \n",
    "      - Update $\\pi$ with rewards $R(\\tau)$  \n",
    "      - Edit $\\theta$ to produce $\\theta'$  \n",
    "      - Collect $\\pi$'s trajectory $\\tau$ on $\\theta'$, with stop-gradient $\\phi^\\perp$  \n",
    "      - Compute regret score $S'$ for $\\theta'$  \n",
    "      - Update $\\Lambda$ with $\\theta$ or $\\theta'$ if score $S$ or $S'$ meets threshold  \n",
    "      - (Optionally) Update Editor using score $S$  \n",
    "   4. **end**  \n",
    "**end**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACCEL CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import minigrid\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "\n",
    "K   = 100   # Maximum number of levels to store in the replay buffer (levels to train on)\n",
    "pho = 0.5   # Initial fill rate of the replay buffer (initial ratio of random levels to store)\n",
    "p   = 0.5   # Probability of requesting a new level to be generated\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LevelGenerator:\n",
    "    def __init__(self, level_buffer):\n",
    "        # Fill the replay buffer with K levels generated with the initial fill rate\n",
    "        for _ in range(int(K * pho)):\n",
    "            self.generate(level_buffer)\n",
    "\n",
    "    def generate(self, level_buffer):\n",
    "        if len(level_buffer) < K:\n",
    "            level_buffer.append(gym.make(\"MiniGrid-LavaGapS5-v0\", render_mode=\"human\"))\n",
    "        else:\n",
    "            print(\"Replay buffer is full\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    levels = [] # Replay buffer to store the levels\n",
    "\n",
    "    # Initialize the level generator\n",
    "    level_generator = LevelGenerator(levels)\n",
    "\n",
    "    # Initialize the agent\n",
    "    model = PPO(\"MlpPolicy\", levels[0], verbose=1)\n",
    "\n",
    "    # Train the agent\n",
    "    for _ in range(1000):\n",
    "        # With probability p, generate a new level and add it to the replay buffer\n",
    "        if np.random.rand() < p:\n",
    "            level_generator.generate()\n",
    "        else:\n",
    "            # Sample a level from the replay buffer\n",
    "            level = np.random.choice(levels)\n",
    "            obs, reward, done, info = level.reset(), 0, False, {}\n",
    "            while not done:\n",
    "                action, _ = model.predict(obs, deterministic=True)\n",
    "                obs, reward, done, info = level.step(action)\n",
    "                level.render()\n",
    "            print(\"Episode finished\")\n",
    "\n",
    "\n",
    "    # Close all the environments in the replay buffer\n",
    "    for env in level_generator.levels:\n",
    "        env.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for ** or pow(): 'MissionSpace' and 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 255\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. score=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, config=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 255\u001b[0m     \u001b[43mmain_accel_demo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 206\u001b[0m, in \u001b[0;36mmain_accel_demo\u001b[1;34m(total_iterations, replay_prob, train_steps)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;124;03m- Create a dummy environment (for stable-baselines model init).\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;124;03m- Create a PPO model\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;124;03m   * with prob p: sample from buffer, train some steps, then mutate -> measure -> store\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;66;03m# 6.1. Create a dummy environment for model initialization\u001b[39;00m\n\u001b[1;32m--> 206\u001b[0m dummy_env \u001b[38;5;241m=\u001b[39m \u001b[43mMyCustomGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwidth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_blocks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    207\u001b[0m dummy_env\u001b[38;5;241m.\u001b[39mreset()     \n\u001b[0;32m    208\u001b[0m dummy_env\u001b[38;5;241m.\u001b[39mrender()\n",
      "Cell \u001b[1;32mIn[1], line 49\u001b[0m, in \u001b[0;36mMyCustomGrid.__init__\u001b[1;34m(self, config, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_start \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent_start\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Pass a minimal MissionSpace to the parent for initialization\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     44\u001b[0m     width\u001b[38;5;241m=\u001b[39mwidth,\n\u001b[0;32m     45\u001b[0m     height\u001b[38;5;241m=\u001b[39mheight,\n\u001b[0;32m     46\u001b[0m     max_steps\u001b[38;5;241m=\u001b[39mwidth \u001b[38;5;241m*\u001b[39m height \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     47\u001b[0m     see_through_walls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     48\u001b[0m     agent_view_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m---> 49\u001b[0m     mission_space \u001b[38;5;241m=\u001b[39m \u001b[43mMissionSpace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmission_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFind the goal in the grid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m     51\u001b[0m )\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# If you'd like to fix the seed for every reset call, you can do:\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# (Otherwise, minigrid will handle seeds via env.reset(seed=...))\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_seed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for ** or pow(): 'MissionSpace' and 'dict'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Discrete, Box, Text\n",
    "import gymnasium.spaces as spaces\n",
    "from minigrid.core.mission import MissionSpace\n",
    "from minigrid.core.world_object import Goal, Wall\n",
    "from minigrid.minigrid_env import MiniGridEnv, Grid\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# 1. Custom MiniGrid Environment that takes a config (https://minigrid.farama.org/content/create_env_tutorial/)\n",
    "# ====================================================\n",
    "class MyCustomGrid(MiniGridEnv):\n",
    "    \"\"\"\n",
    "    Simple MiniGrid environment that places random wall tiles\n",
    "    according to a config dict.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config=None, **kwargs):\n",
    "        \"\"\"\n",
    "        config: a dict with fields like:\n",
    "            {\n",
    "              \"width\": 8,\n",
    "              \"height\": 8,\n",
    "              \"num_blocks\": 5,\n",
    "              \"seed_val\": 42,\n",
    "              \"agent_start\": (1,1)\n",
    "            }\n",
    "        \"\"\"\n",
    "        if config is None:\n",
    "            config = {}\n",
    "        self.config = config\n",
    "\n",
    "        width = config.get(\"width\", 8)\n",
    "        height = config.get(\"height\", 8)\n",
    "        self.num_blocks = config.get(\"num_blocks\", 5)\n",
    "        self.custom_seed = config.get(\"seed_val\", None)\n",
    "        self.agent_start = config.get(\"agent_start\", None)\n",
    "\n",
    "        # Pass a minimal MissionSpace to the parent for initialization\n",
    "        super().__init__(\n",
    "            width=width,\n",
    "            height=height,\n",
    "            max_steps=width * height * 2,\n",
    "            see_through_walls = False,\n",
    "            agent_view_size = 5,\n",
    "            mission_space = MissionSpace(mission_func=lambda: \"Find the goal in the grid\")\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        # If you'd like to fix the seed for every reset call, you can do:\n",
    "        # (Otherwise, minigrid will handle seeds via env.reset(seed=...))\n",
    "        if self.custom_seed is not None:\n",
    "            self.seed(self.custom_seed)\n",
    "\n",
    "    def _gen_grid(self, width, height):\n",
    "        \"\"\"\n",
    "        Generate the grid layout for a new episode.\n",
    "        \"\"\"\n",
    "        # 1) Create an empty grid\n",
    "        self.grid = Grid(width, height)\n",
    "\n",
    "        # 2) Surround the grid with walls\n",
    "        #    If your version of Grid supports grid.wall_rect, you can call that directly.\n",
    "        #    Otherwise, place them manually:\n",
    "        for i in range(width):\n",
    "            self.put_obj(Wall(), i, 0)\n",
    "            self.put_obj(Wall(), i, height - 1)\n",
    "        for j in range(height):\n",
    "            self.put_obj(Wall(), 0, j)\n",
    "            self.put_obj(Wall(), width - 1, j)\n",
    "\n",
    "        # 3) Place random blocks (walls) inside\n",
    "        for _ in range(self.num_blocks):\n",
    "            r = self._rand_int(1, height - 1)\n",
    "            c = self._rand_int(1, width - 1)\n",
    "            # put_obj takes (object, i, j)\n",
    "            self.put_obj(Wall(), c, r)\n",
    "\n",
    "        # 4) Place the agent\n",
    "        if self.agent_start is not None:\n",
    "            ax, ay = self.agent_start\n",
    "            # place_agent(...) can put the agent exactly at (ax, ay)\n",
    "            # by specifying top=(ax, ay) and a size of (1,1)\n",
    "            self.place_agent(\n",
    "                top=(ax, ay),\n",
    "                size=(1, 1),\n",
    "                rand_dir=False  # Do not randomize direction\n",
    "            )\n",
    "        else:\n",
    "            # Let parent place agent randomly\n",
    "            self.place_agent()\n",
    "\n",
    "        # 5) Place a goal object somewhere randomly\n",
    "        self.place_obj(Goal())\n",
    "\n",
    "# ====================================================\n",
    "# 2. Simple “level buffer”\n",
    "# ====================================================\n",
    "class LevelBuffer:\n",
    "    def __init__(self, max_size=50):\n",
    "        self.max_size = max_size\n",
    "        # will store tuples of (config_dict, score)\n",
    "        self.data = []\n",
    "\n",
    "    def add(self, config, score):\n",
    "        # store\n",
    "        self.data.append((config, score))\n",
    "        # if over capacity, remove lowest-score\n",
    "        if len(self.data) > self.max_size:\n",
    "            self.data.sort(key=lambda x: x[1], reverse=True)\n",
    "            self.data = self.data[: self.max_size]\n",
    "\n",
    "    def sample_config(self):\n",
    "        # Weighted sampling by score\n",
    "        if len(self.data) == 0:\n",
    "            return None\n",
    "        scores = [item[1] for item in self.data]\n",
    "        total = sum(scores)\n",
    "        if total <= 1e-9:\n",
    "            # fallback to uniform\n",
    "            idx = np.random.randint(len(self.data))\n",
    "            return self.data[idx][0]\n",
    "        probs = [s / total for s in scores]\n",
    "        idx = np.random.choice(len(self.data), p=probs)\n",
    "        return self.data[idx][0]\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# 3. Utility: generate random config\n",
    "# ====================================================\n",
    "def random_config():\n",
    "    return {\n",
    "        \"width\": np.random.randint(5, 10),\n",
    "        \"height\": np.random.randint(5, 10),\n",
    "        \"num_blocks\": np.random.randint(0, 15),\n",
    "        \"seed_val\": np.random.randint(0, 999999),\n",
    "        # \"agent_start\": (1,1)  # optional\n",
    "    }\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# 4. Utility: mutate an existing config\n",
    "# ====================================================\n",
    "def edit_config(old_config):\n",
    "    new_config = dict(old_config)\n",
    "    # randomly tweak width/height a tiny bit\n",
    "    if np.random.rand() < 0.5:\n",
    "        new_config[\"width\"] = max(5, old_config[\"width\"] + np.random.choice([-1, 1]))\n",
    "    else:\n",
    "        new_config[\"height\"] = max(5, old_config[\"height\"] + np.random.choice([-1, 1]))\n",
    "\n",
    "    # tweak num_blocks\n",
    "    new_config[\"num_blocks\"] = max(0, old_config[\"num_blocks\"] + np.random.choice([-2, -1, 1, 2]))\n",
    "\n",
    "    # optionally change seed\n",
    "    new_config[\"seed_val\"] = np.random.randint(0, 999999)\n",
    "    return new_config\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# 5. Evaluate “difficulty” or “regret”\n",
    "#    (Simplified: we do a short rollout with\n",
    "#    the *current policy* and measure reward or steps)\n",
    "#    In a real ACCEL, you'd measure \"positive value loss.\"\n",
    "# ====================================================\n",
    "def measure_difficulty(config, model, max_steps=200):\n",
    "    \"\"\"\n",
    "    Run 1 episode with the current model. Return a 'difficulty' score.\n",
    "    Higher means the agent did poorly, so it's \"harder.\"\n",
    "    \"\"\"\n",
    "    env = MyCustomGrid(config)\n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    total_reward = 0\n",
    "    for _ in range(max_steps):\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "    # e.g. define difficulty as 1 - (normalized reward)\n",
    "    # or just do \"steps used,\" or negative of final reward, etc.\n",
    "    # We'll do a naive approach:\n",
    "    difficulty = max(0, (1.0 - (total_reward / 1.0)))\n",
    "    return difficulty\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# 6. Main demonstration\n",
    "# ====================================================\n",
    "def main_accel_demo(total_iterations=30, replay_prob=0.7, train_steps=2000):\n",
    "    \"\"\"\n",
    "    - Create a dummy environment (for stable-baselines model init).\n",
    "    - Create a PPO model\n",
    "    - Create a buffer\n",
    "    - Repeatedly do:\n",
    "       * with prob (1 - p): sample new config, measure difficulty -> store\n",
    "       * with prob p: sample from buffer, train some steps, then mutate -> measure -> store\n",
    "    \"\"\"\n",
    "\n",
    "    # 6.1. Create a dummy environment for model initialization\n",
    "    dummy_env = MyCustomGrid(config={\"width\": 5, \"height\": 5, \"num_blocks\": 1})\n",
    "    dummy_env.reset()     \n",
    "    dummy_env.render()\n",
    "\n",
    "    # Initialize PPO with MlpPolicy\n",
    "    model = PPO(\"MlpPolicy\", dummy_env, verbose=1, n_steps=128, batch_size=64)\n",
    "\n",
    "    # 6.2. Create the level buffer\n",
    "    level_buffer = LevelBuffer(max_size=50)\n",
    "\n",
    "    # Fill with some random configs initially\n",
    "    for _ in range(10):\n",
    "        cfg = random_config()\n",
    "        level_buffer.add(cfg, score=1.0)  # dummy score\n",
    "\n",
    "    # 6.3. Outer loop\n",
    "    for iteration in range(total_iterations):\n",
    "        print(f\"\\n=== ITERATION {iteration+1}/{total_iterations} ===\")\n",
    "        use_replay = np.random.rand() < replay_prob\n",
    "\n",
    "        # A) If we do NOT replay from buffer, we just sample a new config\n",
    "        if (not use_replay) or (len(level_buffer.data) == 0):\n",
    "            cfg = random_config()\n",
    "            # measure difficulty with current PPO\n",
    "            diff_score = measure_difficulty(cfg, model)\n",
    "            level_buffer.add(cfg, diff_score)\n",
    "            print(f\"  Sampled new config, difficulty={diff_score:.3f}\")\n",
    "        else:\n",
    "            # B) Replay from buffer: pick a config, do short train, mutate\n",
    "            old_cfg = level_buffer.sample_config()\n",
    "            # 1) Train the PPO model on old_cfg\n",
    "            env = MyCustomGrid(old_cfg)\n",
    "            model.set_env(env)\n",
    "            model.learn(total_timesteps=train_steps)  # short training\n",
    "\n",
    "            # 2) Now mutate config\n",
    "            new_cfg = edit_config(old_cfg)\n",
    "            diff_score = measure_difficulty(new_cfg, model)\n",
    "            level_buffer.add(new_cfg, diff_score)\n",
    "            print(f\"  Replayed + mutated. old_cfg -> new_cfg. difficulty={diff_score:.3f}\")\n",
    "\n",
    "    print(\"\\nDone. Final buffer size:\", len(level_buffer.data))\n",
    "    print(\"Buffer top-5 hardest levels (config, score):\")\n",
    "    level_buffer.data.sort(key=lambda x: x[1], reverse=True)\n",
    "    for i, (cfg, sc) in enumerate(level_buffer.data[:5]):\n",
    "        print(f\"{i+1}. score={sc:.3f}, config={cfg}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_accel_demo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "accel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
