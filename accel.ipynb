{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACCEL IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "from gymnasium.spaces import Box\n",
    "\n",
    "from minigrid.core.mission import MissionSpace\n",
    "from minigrid.core.world_object import Goal, Wall\n",
    "from minigrid.minigrid_env import MiniGridEnv, Grid\n",
    "\n",
    "from minigrid.wrappers import ImgObsWrapper\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import wandb\n",
    "\n",
    "device = 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ====================================================\n",
    "# 1. Custom MiniGrid Environment that returns only the image\n",
    "#    for SB3's PPO (which expects a Box space).\n",
    "# ====================================================\n",
    "class MyCustomGrid(MiniGridEnv):\n",
    "    \"\"\"\n",
    "    Simple MiniGrid environment that places random wall tiles\n",
    "    according to a config dict, returning only the 'image' observation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config=None, solvable_only=False, **kwargs):\n",
    "        if config is None:\n",
    "            config = {}\n",
    "        self.config = config\n",
    "        self.solvable_only = solvable_only\n",
    "\n",
    "        \n",
    "        # Create a random number generator with the custom seed\n",
    "        self.rng = np.random.default_rng(seed=self.config.get(\"seed_val\"))\n",
    "\n",
    "        mission_space = MissionSpace(mission_func=lambda: \"get to the green goal square\")\n",
    "\n",
    "        super().__init__(\n",
    "            grid_size=self.config['width'],\n",
    "            max_steps=self.config['width'] * self.config['height'] * 2, # max_steps is typically 2x the grid size\n",
    "            see_through_walls=False,\n",
    "            agent_view_size=5,                      # Size of the agent's view square\n",
    "            mission_space=mission_space,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # Manually define our observation_space as a single Box (the image).\n",
    "        # By default, MiniGrid's image shape is (view_size, view_size, 3) if using partial obs,\n",
    "        # or (height, width, 3) if using full-grid observation. We'll do full-grid here:\n",
    "        # We'll define (self.height, self.width, 3) as the shape.\n",
    "        # In practice, \"image\" shape can vary if partial observations are used.\n",
    "        self.observation_space = Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=(self.agent_view_size, self.agent_view_size, 3),\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "\n",
    "            \n",
    "    def _gen_grid(self, width, height):\n",
    "        \"\"\"\n",
    "        Generate a new environment layout ensuring solvability if required.\n",
    "        \"\"\"\n",
    "\n",
    "        while True:  # Keep regenerating until a solvable layout is found\n",
    "            self.grid = Grid(width, height)\n",
    "            self.grid.wall_rect(0, 0, width, height)\n",
    "\n",
    "            # Place the goal\n",
    "            goal_pos = self.config.get(\"goal_pos\")\n",
    "            if goal_pos is None:\n",
    "                while True:\n",
    "                    goal_r = self.rng.integers(1, height - 1)\n",
    "                    goal_c = self.rng.integers(1, width - 1)\n",
    "                    if self.grid.get(goal_c, goal_r) is None:\n",
    "                        self.put_obj(Goal(), goal_c, goal_r)\n",
    "                        self.config[\"goal_pos\"] = (goal_c, goal_r)\n",
    "                        break\n",
    "            else:\n",
    "                self.put_obj(Goal(), goal_pos[0], goal_pos[1])\n",
    "\n",
    "            # Place the agent\n",
    "            start_pos = self.config.get(\"start_pos\")\n",
    "            if start_pos is None:\n",
    "                while True:\n",
    "                    start_r = self.rng.integers(1, height - 1)\n",
    "                    start_c = self.rng.integers(1, width - 1)\n",
    "                    if self.grid.get(start_c, start_r) is None and (start_c, start_r) != self.config[\"goal_pos\"]:\n",
    "                        self.agent_pos = (start_c, start_r)\n",
    "                        self.agent_dir = self.rng.integers(0, 4)\n",
    "                        self.config[\"start_pos\"] = (start_c, start_r)\n",
    "                        break\n",
    "            else:\n",
    "                self.agent_pos = start_pos\n",
    "                self.agent_dir = self.rng.integers(0, 4)\n",
    "                self.config[\"start_pos\"] = start_pos\n",
    "            \n",
    "            placed_blocks = 0\n",
    "            \n",
    "            # Maximum number of tries to place the blocks\n",
    "            max_num_tries = 1000\n",
    "            \n",
    "            # Place random walls using config parameters\n",
    "            while placed_blocks < self.config[\"num_blocks\"]:\n",
    "                max_num_tries -= 1\n",
    "                r = self.rng.integers(1, height - 1)\n",
    "                c = self.rng.integers(1, width - 1)\n",
    "                if self.grid.get(c, r) is None and (c, r) != self.config[\"start_pos\"] and (c, r) != self.config[\"goal_pos\"]:\n",
    "                    self.put_obj(Wall(), c, r)\n",
    "                    placed_blocks += 1\n",
    "                if max_num_tries <= 0:\n",
    "                    print(\"Could not place all blocks in the grid.\")\n",
    "                    break\n",
    "                \n",
    "\n",
    "            # Check solvability if required\n",
    "            if not self.solvable_only or self._is_solvable():\n",
    "                break\n",
    "            \n",
    "            # If not solvable, change seed and try again\n",
    "            self.rng = np.random.default_rng(seed=self.rng.integers(0, 999999))\n",
    "        \n",
    "    def _is_solvable(self):\n",
    "        \"\"\"\n",
    "        Uses Breadth-First Search (BFS) to check if there's a path \n",
    "        from the agent's start position to the goal.\n",
    "        \"\"\"\n",
    "        start_pos = self.config[\"start_pos\"]\n",
    "        goal_pos = self.config[\"goal_pos\"]\n",
    "        if not start_pos or not goal_pos:\n",
    "            return False\n",
    "\n",
    "        queue = deque([start_pos])\n",
    "        visited = set()\n",
    "        visited.add(start_pos)\n",
    "\n",
    "        while queue:\n",
    "            x, y = queue.popleft()\n",
    "            if (x, y) == goal_pos:\n",
    "                return True\n",
    "\n",
    "            # Possible moves: up, down, left, right\n",
    "            for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n",
    "                nx, ny = x + dx, y + dy\n",
    "                cell_obj = self.grid.get(nx, ny)\n",
    "                if (\n",
    "                    1 <= nx < self.width - 1 and  # Stay within grid bounds\n",
    "                    1 <= ny < self.height - 1 and\n",
    "                    (nx, ny) not in visited and\n",
    "                    self.grid.get(nx, ny) is None or isinstance(cell_obj, Goal)\n",
    "                ):\n",
    "                    queue.append((nx, ny))\n",
    "                    visited.add((nx, ny))\n",
    "        return False  # No path found\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Override reset to ensure we only return the 'image' array\n",
    "        instead of a dict with 'image' and 'mission'.\n",
    "        \"\"\"\n",
    "        obs, info = super().reset(**kwargs)\n",
    "        obs = self._convert_obs(obs)\n",
    "        \n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Same for step: override to convert the dict observation into an image only.\n",
    "        \"\"\"\n",
    "        obs, reward, done, truncated, info = super().step(action)\n",
    "        obs = self._convert_obs(obs)\n",
    "        return obs, reward, done, truncated, info\n",
    "\n",
    "    def _convert_obs(self, original_obs):\n",
    "        \"\"\"\n",
    "        original_obs is typically {'image':..., 'mission':...}.\n",
    "        We'll just return original_obs['image'] to get a Box(low=0,high=255) shape.\n",
    "        \"\"\"\n",
    "        return original_obs[\"image\"]\n",
    "        #return np.transpose(original_obs[\"image\"], (2, 0, 1))\n",
    "    \n",
    "    def update_config(self, new_config):\n",
    "        self.config = new_config\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "\n",
    "def random_config(grid_size, num_blocks=None):\n",
    "    max_blocks = int(((grid_size - 1) * (grid_size - 1)) / 2)\n",
    "    \n",
    "    if num_blocks is None:\n",
    "        num_blocks = np.random.randint(1, max_blocks)\n",
    "    else:\n",
    "        num_blocks = min(num_blocks, max_blocks)\n",
    "        \n",
    "    config = {\n",
    "        \"width\": grid_size,\n",
    "        \"height\": grid_size,\n",
    "        \"num_blocks\": num_blocks,\n",
    "        \"start_pos\": None,\n",
    "        \"goal_pos\": None,\n",
    "        \"edited\": False,\n",
    "        \"seed_val\": np.random.randint(0, 999999),\n",
    "    }\n",
    "    \n",
    "    # Set the start and goal positions\n",
    "    env = MyCustomGrid(config)\n",
    "    \n",
    "    # Reset the environment to get the start and goal positions\n",
    "    env.reset()\n",
    "    \n",
    "    # Get the new config from the environment\n",
    "    config = env.config\n",
    "        \n",
    "    return config\n",
    "\n",
    "def print_level_from_config(config, solvable_only=False):\n",
    "    print(\"Putting up the level from config:\", config)\n",
    "    env = MyCustomGrid(config, render_mode='rgb_array', solvable_only=solvable_only)\n",
    "    env.reset()\n",
    "    full_level_image = env.render()  # This should return an RGB image of the full grid\n",
    "\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(full_level_image)\n",
    "    plt.title(\"Level Configuration: \" + str(config))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "\"\"\"# Modify an existing configuration, adding randomness.\n",
    "def edit_config(old_config):\n",
    "    max_blocks = int(((old_config[\"width\"] - 1) * (old_config[\"height\"] - 1)) / 2)\n",
    "    \n",
    "    new_config = dict(old_config)\n",
    "    \n",
    "    # Randomly change the number of blocks\n",
    "    new_number_blocks = old_config[\"num_blocks\"] + np.random.choice([1, 2, 3])\n",
    "    \n",
    "    # Ensure the number of blocks is within bounds\n",
    "    new_config[\"num_blocks\"] = max(1, min(new_number_blocks, max_blocks))    \n",
    "    \n",
    "    # Mark the config as edited\n",
    "    new_config[\"edited\"] = True\n",
    "    \n",
    "    return new_config\"\"\"\n",
    "    \n",
    "import numpy as np\n",
    "\n",
    "def edit_config(old_config, difficulty_level=1):\n",
    "\n",
    "    width, height = old_config[\"width\"], old_config[\"height\"]\n",
    "    total_cells = width * height\n",
    "\n",
    "    # Define a baseline max number of blocks\n",
    "    max_blocks = int(0.6 * total_cells)  # Ensure we don't overcrowd (max 60% coverage)\n",
    "    \n",
    "    # Calculate the new number of blocks using a logarithmic scale\n",
    "    base_growth = int(np.log2(total_cells) * difficulty_level)\n",
    "    \n",
    "    # Introduce some randomness while keeping it within a reasonable range\n",
    "    growth_factor = np.random.randint(base_growth // 2, base_growth + 1)\n",
    "    \n",
    "    # Compute the new block count\n",
    "    new_number_blocks = old_config[\"num_blocks\"] + growth_factor\n",
    "    \n",
    "    # Ensure it's within the allowed range\n",
    "    new_config = dict(old_config)\n",
    "    new_config[\"num_blocks\"] = max(1, min(new_number_blocks, max_blocks))  \n",
    "    \n",
    "    # Mark as edited\n",
    "    new_config[\"edited\"] = True\n",
    "\n",
    "    return new_config\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# 2. Simple “level buffer” \n",
    "# ====================================================\n",
    "# class to memorize generated levels and score\n",
    "class LevelBuffer: \n",
    "    def __init__(self, max_size=50):\n",
    "        self.max_size = max_size\n",
    "        self.data = []  # will store (config_dict, score)\n",
    "\n",
    "    def add(self, config, score):\n",
    "        self.data.append((config, score))\n",
    "        if len(self.data) > self.max_size:\n",
    "            self.data.sort(key=lambda x: x[1], reverse=True)\n",
    "            self.data = self.data[: self.max_size]\n",
    "            #it memorize only the highest score for each level\n",
    "\n",
    "    def sample_config(self): \n",
    "        # Samples a level from the buffer, weighting the probabilities \n",
    "        # based on the scores.\n",
    "        if len(self.data) == 0:\n",
    "            return None\n",
    "        scores = [item[1] for item in self.data]\n",
    "        total = sum(scores)\n",
    "        if total <= 1e-9:\n",
    "            # fallback to uniform\n",
    "            idx = np.random.randint(len(self.data))\n",
    "            return self.data[idx][0]\n",
    "        probs = [s / total for s in scores]\n",
    "        idx = np.random.choice(len(self.data), p=probs)\n",
    "        return self.data[idx][0]\n",
    "\n",
    "# ====================================================\n",
    "# 3. Utility Functions\n",
    "# ====================================================\n",
    "\n",
    "# Calculate regret using Generalized Advantage Estimation (GAE) with Stable-Baselines3's PPO model.\n",
    "# PLR approximates regret using a score function such as the positive value loss.\n",
    "def calculate_regret_gae(env, model, max_steps, gamma, lam):\n",
    "    \"\"\"\n",
    "    Calculate regret using Generalized Advantage Estimation (GAE)\n",
    "    with Stable-Baselines3's PPO model.\n",
    "    \"\"\"\n",
    "    obs, _ = env.reset()\n",
    "    regrets = []\n",
    "    rewards = []\n",
    "    dones = []\n",
    "    values = []\n",
    "\n",
    "    for t in range(max_steps):\n",
    "        # Add batch dimension to the observation tensor\n",
    "        obs_tensor = torch.as_tensor(obs).float().unsqueeze(0).to(device)\n",
    "        \n",
    "        # Use the model's policy to get the value and action.\n",
    "        # For actions, model.predict handles single observations well.\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        \n",
    "        # Compute the value from the policy.\n",
    "        value_t = model.policy.predict_values(obs_tensor).item()\n",
    "        values.append(value_t)\n",
    "        \n",
    "        # Perform the step in the environment\n",
    "        obs, reward, done, truncated, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "    # Add value of the terminal state (0 if done/truncated)\n",
    "    if done or truncated:\n",
    "        terminal_value = 0.0\n",
    "    else:\n",
    "        terminal_obs_tensor = torch.as_tensor(obs).float().unsqueeze(0).to(device)\n",
    "        terminal_value = model.policy.predict_values(terminal_obs_tensor).item()\n",
    "    values.append(terminal_value)\n",
    "\n",
    "    # Compute TD-errors and GAE-like regret score\n",
    "    for t in range(len(rewards)):\n",
    "        delta_t = rewards[t] + gamma * values[t + 1] * (1 - dones[t]) - values[t]\n",
    "        discounted_error = (gamma * lam) ** t * delta_t\n",
    "        regrets.append(max(0, discounted_error))\n",
    "\n",
    "    # Return the maximum positive regret score (or 0 if empty)\n",
    "    return max(regrets) if regrets else 0.0\n",
    "\n",
    "\n",
    "def initialize_ppo(env, learning_rate=1e-4):\n",
    "    return PPO(\n",
    "        \"MlpPolicy\",                    # Multi-layer perceptron policy\n",
    "        env,                            # environment to learn from\n",
    "        verbose=0,                      # Display training output\n",
    "        n_steps=256,                    # Number of steps to run for each environment per update\n",
    "        batch_size=64,                  # Minibatch size for each gradient update\n",
    "        learning_rate=learning_rate,    # Learning rate for optimizer\n",
    "        device=device                   # Use GPU if available\n",
    "    )\n",
    "    \n",
    "# Use vectorized environment\n",
    "def create_vectorized_env(config, n_envs=4, solvable_only=False):\n",
    "    \"\"\"\n",
    "    Create a vectorized environment with n parallel environments.\n",
    "    \"\"\"\n",
    "    return make_vec_env(lambda: MyCustomGrid(config, solvable_only), n_envs=n_envs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalute_models(load=False):\n",
    "    \n",
    "    if load:\n",
    "        # Load the models\n",
    "        model_plr = PPO.load(\"models/plr_model\")\n",
    "        model_accel = PPO.load(\"models/accel_model\")\n",
    "        model_accel_easy = PPO.load(\"models/accel_model_easy\")\n",
    "\n",
    "    # Inseert the models in a dictionary\n",
    "    models = {'PLR': model_plr, 'ACCEL': model_accel, 'ACCEL-EasyStart': model_accel_easy}\n",
    "\n",
    "    # Generate n levels difficulties with increasing complexity, for each level generate m configs\n",
    "    difficulties = 3\n",
    "    config[\"grid_size\"] = 11\n",
    "\n",
    "    levels = []\n",
    "    for i in range(difficulties):\n",
    "        level = []\n",
    "        for _ in range(10):\n",
    "            cfg = random_config(config[\"grid_size\"], num_blocks=config[\"grid_size\"]*(i+1))\n",
    "            #print_level_from_config(cfg)\n",
    "            level.append(cfg)\n",
    "        levels.append(level)\n",
    "\n",
    "    # Evaluate the model on the generated levels\n",
    "    results = {}\n",
    "    for model_name, model in models.items():\n",
    "        results[model_name] = []\n",
    "        for i, level in enumerate(levels):\n",
    "            print(f\"Evaluating level {i + 1} with {config['grid_size']*(i+1)} blocks for model {model_name}...\")\n",
    "            r = []\n",
    "            for j, cfg in enumerate(level):\n",
    "                # Create vectorized environment\n",
    "                env = create_vectorized_env(cfg, n_envs=4, solvable_only=True)\n",
    "                mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=5, deterministic=True)\n",
    "                r.append(mean_reward)\n",
    "            results[model_name].append(r)\n",
    "        print()\n",
    "        \n",
    "    # Print mean rewards for each level\n",
    "    for model_name in models.keys():\n",
    "        print(f\"Model: {model_name}\")\n",
    "        for i, level in enumerate(levels):\n",
    "            print(f\"Level {i + 1} - Complexity {config['grid_size']*(i+1)}: {np.mean(results[model_name][i]):.2f}\")\n",
    "        print()\n",
    "\n",
    "    # Boxplot of results, a plot for each level complexity comparing models\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for i, level in enumerate(levels):\n",
    "        plt.subplot(1, difficulties, i + 1)\n",
    "        plt.boxplot([results[model_name][i] for model_name in models.keys()])\n",
    "        plt.xticks([1,2,3], [model_name for model_name in models.keys()])\n",
    "        plt.title(f\"Level {i + 1} - Complexity {config['grid_size']*(i+1)}\")\n",
    "        plt.ylabel(\"Mean Reward\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_accel(total_iterations, replay_prob, train_steps, level_buffer_size,\n",
    "               initial_fill_size, grid_size, n_envs, edit_levels, regret_threshold,\n",
    "               easy_start):\n",
    "    \n",
    "    # Initialize Weights and Biases\n",
    "    wandb.init(project=\"accel\", config=config)\n",
    "    \n",
    "    # Create a level buffer, a personal class to store levels and scores\n",
    "    level_buffer = LevelBuffer(max_size=level_buffer_size)\n",
    "    \n",
    "    # Generate a random configuration {width, height, num_blocks, start_pos, goal_pos}\n",
    "    dummy_config = random_config(grid_size)\n",
    "    \n",
    "    # Create a vectorized environment, so a wrapper for MyCustomGrid that allows interconnection \n",
    "    # between gymnasium and stable-baselines3 to train the model in a vectorized way, since we\n",
    "    # are using DummyVecEnv, it is not true parallelism\n",
    "    vectorized_env = create_vectorized_env(dummy_config, n_envs=n_envs)\n",
    "\n",
    "    # Initialize PPO with vectorized environment\n",
    "    print(\"Initializing student model PPO...\")\n",
    "    student_model = initialize_ppo(vectorized_env)\n",
    "\n",
    "    # ====================================================\n",
    "    # Initial buffer fill\n",
    "    # ====================================================\n",
    "    \n",
    "    print(f\"Populating buffer with {initial_fill_size} initial levels with regret > {regret_threshold}...\")\n",
    "    while len(level_buffer.data) < initial_fill_size:\n",
    "        \n",
    "        if easy_start:\n",
    "            cfg = random_config(grid_size, num_blocks=2)\n",
    "        else:\n",
    "            cfg = random_config(grid_size)\n",
    "        \n",
    "        for monitor in vectorized_env.envs:\n",
    "            monitor.env.update_config(cfg)\n",
    "        \n",
    "        student_model.learn(total_timesteps=100)\n",
    "        \n",
    "        regret = calculate_regret_gae(MyCustomGrid(cfg), student_model, max_steps=1000, gamma=0.99, lam=0.95)\n",
    "\n",
    "        # Skip levels with low regret\n",
    "        if regret < regret_threshold: continue\n",
    "\n",
    "        level_buffer.add(cfg, regret)\n",
    "\n",
    "    # ====================================================\n",
    "    # Main ACCEL loop\n",
    "    # ====================================================\n",
    "    \n",
    "    iteration_regrets = []\n",
    "    iteration, skipped = 0, 0\n",
    "    \n",
    "    print(\"\\nMain training loop...\")\n",
    "    while iteration < total_iterations + skipped:\n",
    "        print(f\"\\n=== ITERATION {iteration + 1}/{total_iterations + skipped} SKIPPED: {skipped} ===\")\n",
    "        \n",
    "        iteration += 1\n",
    "        \n",
    "        # Decide whether to replay or generate a new level\n",
    "        use_replay = np.random.rand() < replay_prob\n",
    "\n",
    "        if not use_replay or len(level_buffer.data) == 0:\n",
    "            # Create a new random level\n",
    "            cfg = random_config(grid_size)\n",
    "            print(\"Generated new random level:\", cfg)\n",
    "        else:\n",
    "            # Sample a level from the buffer\n",
    "            cfg = level_buffer.sample_config()\n",
    "            print(\"Sampled level from buffer:\", cfg)\n",
    "            \n",
    "        # Update the vectorized environment with the selected config and train the model\n",
    "        for monitor in vectorized_env.envs:\n",
    "            monitor.env.update_config(cfg)\n",
    "        \n",
    "        student_model.learn(total_timesteps=train_steps)\n",
    "        \n",
    "        wandb.log({\n",
    "            \"iteration\": iteration,\n",
    "            \"regret_score\": regret,\n",
    "            \"regret_threshold\": regret_threshold,\n",
    "            \"buffer_size\": len(level_buffer.data),\n",
    "            \"value_loss\": student_model.logger.name_to_value[\"train/value_loss\"],\n",
    "            \"entropy_loss\": student_model.logger.name_to_value[\"train/entropy_loss\"],\n",
    "        })\n",
    "\n",
    "        if use_replay and edit_levels:\n",
    "            # Edit the level and calculate regret\n",
    "            cfg = edit_config(cfg)\n",
    "            print(\"Edited level to:\", cfg)\n",
    "\n",
    "        regret = calculate_regret_gae(MyCustomGrid(cfg), student_model, max_steps=1000, gamma=0.99, lam=0.95)\n",
    "        \n",
    "        if regret <= regret_threshold:\n",
    "            print(f\"Regret for current level is {regret:.5f} <= threshold {regret_threshold:.5f}. Skipping...\")\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "\n",
    "        print(f\"Regret for current level: {regret}, buffer size: {len(level_buffer.data)}\")\n",
    "        level_buffer.add(cfg, regret)\n",
    "        iteration_regrets.append(regret)\n",
    "        \n",
    "        # Increase the regret threshold slightly\n",
    "        regret_threshold += 0.0001\n",
    "        \n",
    "    \n",
    "    # Plot and display the progress\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(iteration_regrets, marker='o')\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Regret\")\n",
    "    plt.title(\"Regret Progress during Training\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nDone. Final buffer size:\", len(level_buffer.data))\n",
    "    print(\"Top-5 hardest levels (config, regret):\")\n",
    "    level_buffer.data.sort(key=lambda x: x[1], reverse=True)\n",
    "    for i, (cfg, sc) in enumerate(level_buffer.data[:5]):\n",
    "        print(f\"{i + 1}. regret={sc:.5f}, config={cfg}\")\n",
    "        #print_level_from_config(cfg)\n",
    "        \n",
    "    print(\"Top-5 easiest levels (config, regret):\")\n",
    "    level_buffer.data.sort(key=lambda x: x[1])\n",
    "    for i, (cfg, sc) in enumerate(level_buffer.data[:5]):\n",
    "        print(f\"{i + 1}. regret={sc:.5f}, config={cfg}\")\n",
    "        #print_level_from_config(cfg)\n",
    "    \n",
    "    wandb.finish()\n",
    "    \n",
    "    return student_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>buffer_size</td><td>▁▂▃▅▆▇██████████████████████████████████</td></tr><tr><td>entropy_loss</td><td>▁▃▃▄▃▃▃▄▅▄▅▅▇▇▆▆▆██▇▇▇███▇▇█▇█▇████████▇</td></tr><tr><td>iteration</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>regret_score</td><td>▁█▁▁█▁▆▂▅▄▃▂▅▃▂▇▃▃▂▃▄▆▂▂▃▁▂▂▂▂▂▁▁▂▂▂▃▂▂▁</td></tr><tr><td>regret_threshold</td><td>▁▁▂▂▃▄▄▄▅▅▅▆▇▇▇▇▇▇▇▇████████████████████</td></tr><tr><td>value_loss</td><td>▃▁▃▃▃▁█▂▄▂▁▄▁▂▂▃▁▃▂▂▁▁▁▁▁▁▃▁▃▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>buffer_size</td><td>64</td></tr><tr><td>entropy_loss</td><td>-0.05633</td></tr><tr><td>iteration</td><td>301</td></tr><tr><td>regret_score</td><td>0.07296</td></tr><tr><td>regret_threshold</td><td>0.48</td></tr><tr><td>value_loss</td><td>0.00086</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vivid-plasma-24</strong> at: <a href='https://wandb.ai/noostale-organization/accel/runs/51p35iwu' target=\"_blank\">https://wandb.ai/noostale-organization/accel/runs/51p35iwu</a><br> View project at: <a href='https://wandb.ai/noostale-organization/accel' target=\"_blank\">https://wandb.ai/noostale-organization/accel</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250117_015915-51p35iwu\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running PLR with config: {'grid_size': 8, 'total_iterations': 500, 'train_steps': 1000, 'replay_prob': 0.5, 'level_buffer_size': 64, 'initial_fill_size': 32, 'regret_threshold': 0.0, 'n_envs': 3, 'edit_levels': False, 'easy_start': False}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Emanuele\\Documents\\GitHub\\adaptive-rl-env-curricula\\wandb\\run-20250117_020822-8nrp0rlj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/noostale-organization/accel/runs/8nrp0rlj' target=\"_blank\">earnest-morning-25</a></strong> to <a href='https://wandb.ai/noostale-organization/accel' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/noostale-organization/accel' target=\"_blank\">https://wandb.ai/noostale-organization/accel</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/noostale-organization/accel/runs/8nrp0rlj' target=\"_blank\">https://wandb.ai/noostale-organization/accel/runs/8nrp0rlj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing student model PPO...\n",
      "Populating buffer with 32 initial levels with regret > 0.0...\n",
      "\n",
      "Main training loop...\n",
      "\n",
      "=== ITERATION 1/500 SKIPPED: 0 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 3, 'start_pos': (3, 2), 'goal_pos': (2, 3), 'edited': False, 'seed_val': 574048}\n",
      "Regret for current level: 0.03939574703574181, buffer size: 32\n",
      "\n",
      "=== ITERATION 2/500 SKIPPED: 0 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 3, 'start_pos': (3, 2), 'goal_pos': (2, 3), 'edited': False, 'seed_val': 574048}\n",
      "Regret for current level: 0.046449637711048125, buffer size: 33\n",
      "\n",
      "=== ITERATION 3/500 SKIPPED: 0 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 16, 'start_pos': (6, 5), 'goal_pos': (2, 5), 'edited': False, 'seed_val': 836153}\n",
      "Regret for current level: 0.06878476411104201, buffer size: 34\n",
      "\n",
      "=== ITERATION 4/500 SKIPPED: 0 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 16, 'start_pos': (5, 4), 'goal_pos': (4, 1), 'edited': False, 'seed_val': 120064}\n",
      "Regret for current level: 0.001872336789965623, buffer size: 35\n",
      "\n",
      "=== ITERATION 5/500 SKIPPED: 0 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 13, 'start_pos': (5, 2), 'goal_pos': (6, 4), 'edited': False, 'seed_val': 857444}\n",
      "Regret for current level: 0.1024972255527973, buffer size: 36\n",
      "\n",
      "=== ITERATION 6/500 SKIPPED: 0 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 12, 'start_pos': (1, 3), 'goal_pos': (3, 6), 'edited': False, 'seed_val': 276562}\n",
      "Regret for current level: 0.0036467968761920953, buffer size: 37\n",
      "\n",
      "=== ITERATION 7/500 SKIPPED: 0 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 17, 'start_pos': (5, 5), 'goal_pos': (2, 6), 'edited': False, 'seed_val': 70008}\n",
      "Regret for current level is 0.00000 <= threshold 0.00060. Skipping...\n",
      "\n",
      "=== ITERATION 8/501 SKIPPED: 1 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 8, 'start_pos': (4, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 775458}\n",
      "Regret for current level: 0.061378397792577755, buffer size: 38\n",
      "\n",
      "=== ITERATION 9/501 SKIPPED: 1 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 15, 'start_pos': (4, 1), 'goal_pos': (1, 1), 'edited': False, 'seed_val': 234574}\n",
      "Regret for current level is 0.00000 <= threshold 0.00070. Skipping...\n",
      "\n",
      "=== ITERATION 10/502 SKIPPED: 2 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 8, 'start_pos': (4, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 775458}\n",
      "Regret for current level: 0.07515109926462171, buffer size: 39\n",
      "\n",
      "=== ITERATION 11/502 SKIPPED: 2 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 20, 'start_pos': (3, 1), 'goal_pos': (5, 1), 'edited': False, 'seed_val': 804510}\n",
      "Regret for current level is 0.00000 <= threshold 0.00080. Skipping...\n",
      "\n",
      "=== ITERATION 12/503 SKIPPED: 3 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.7210813879966735, buffer size: 40\n",
      "\n",
      "=== ITERATION 13/503 SKIPPED: 3 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 13, 'start_pos': (1, 1), 'goal_pos': (4, 1), 'edited': False, 'seed_val': 559342}\n",
      "Regret for current level is 0.00000 <= threshold 0.00090. Skipping...\n",
      "\n",
      "=== ITERATION 14/504 SKIPPED: 4 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 3, 'start_pos': (4, 6), 'goal_pos': (4, 1), 'edited': False, 'seed_val': 464171}\n",
      "Regret for current level: 0.29710089934810996, buffer size: 41\n",
      "\n",
      "=== ITERATION 15/504 SKIPPED: 4 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 16, 'start_pos': (2, 5), 'goal_pos': (4, 1), 'edited': False, 'seed_val': 404388}\n",
      "Regret for current level is 0.00000 <= threshold 0.00100. Skipping...\n",
      "\n",
      "=== ITERATION 16/505 SKIPPED: 5 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 3, 'start_pos': (4, 6), 'goal_pos': (4, 1), 'edited': False, 'seed_val': 464171}\n",
      "Regret for current level: 0.29174381007760763, buffer size: 42\n",
      "\n",
      "=== ITERATION 17/505 SKIPPED: 5 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 8, 'start_pos': (4, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 775458}\n",
      "Regret for current level: 0.14716366827487948, buffer size: 43\n",
      "\n",
      "=== ITERATION 18/505 SKIPPED: 5 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 8, 'start_pos': (4, 3), 'goal_pos': (2, 5), 'edited': False, 'seed_val': 519997}\n",
      "Regret for current level: 0.4059567841887474, buffer size: 44\n",
      "\n",
      "=== ITERATION 19/505 SKIPPED: 5 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.7005951523780822, buffer size: 45\n",
      "\n",
      "=== ITERATION 20/505 SKIPPED: 5 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.565509033203125, buffer size: 46\n",
      "\n",
      "=== ITERATION 21/505 SKIPPED: 5 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 14, 'start_pos': (2, 6), 'goal_pos': (1, 4), 'edited': False, 'seed_val': 593498}\n",
      "Regret for current level: 0.0330710405111313, buffer size: 47\n",
      "\n",
      "=== ITERATION 22/505 SKIPPED: 5 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 17, 'start_pos': (4, 4), 'goal_pos': (6, 3), 'edited': False, 'seed_val': 809963}\n",
      "Regret for current level is 0.00000 <= threshold 0.00160. Skipping...\n",
      "\n",
      "=== ITERATION 23/506 SKIPPED: 6 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 3, 'start_pos': (4, 6), 'goal_pos': (4, 1), 'edited': False, 'seed_val': 464171}\n",
      "Regret for current level: 0.2674853143197298, buffer size: 48\n",
      "\n",
      "=== ITERATION 24/506 SKIPPED: 6 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.6760439693927764, buffer size: 49\n",
      "\n",
      "=== ITERATION 25/506 SKIPPED: 6 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 15, 'start_pos': (6, 5), 'goal_pos': (3, 6), 'edited': False, 'seed_val': 109028}\n",
      "Regret for current level is 0.00000 <= threshold 0.00180. Skipping...\n",
      "\n",
      "=== ITERATION 26/507 SKIPPED: 7 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 13, 'start_pos': (2, 6), 'goal_pos': (1, 4), 'edited': False, 'seed_val': 942949}\n",
      "Regret for current level: 0.023870278298854825, buffer size: 50\n",
      "\n",
      "=== ITERATION 27/507 SKIPPED: 7 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 2, 'start_pos': (4, 4), 'goal_pos': (5, 6), 'edited': False, 'seed_val': 760853}\n",
      "Regret for current level: 0.10338861733675003, buffer size: 51\n",
      "\n",
      "=== ITERATION 28/507 SKIPPED: 7 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 2, 'start_pos': (3, 2), 'goal_pos': (2, 6), 'edited': False, 'seed_val': 488162}\n",
      "Regret for current level: 0.07720179468393323, buffer size: 52\n",
      "\n",
      "=== ITERATION 29/507 SKIPPED: 7 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 13, 'start_pos': (3, 4), 'goal_pos': (3, 5), 'edited': False, 'seed_val': 156782}\n",
      "Regret for current level is 0.00000 <= threshold 0.00210. Skipping...\n",
      "\n",
      "=== ITERATION 30/508 SKIPPED: 8 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 16, 'start_pos': (5, 3), 'goal_pos': (2, 5), 'edited': False, 'seed_val': 492117}\n",
      "Regret for current level is 0.00000 <= threshold 0.00210. Skipping...\n",
      "\n",
      "=== ITERATION 31/509 SKIPPED: 9 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.6545353055000305, buffer size: 53\n",
      "\n",
      "=== ITERATION 32/509 SKIPPED: 9 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.5936176896095275, buffer size: 54\n",
      "\n",
      "=== ITERATION 33/509 SKIPPED: 9 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 8, 'start_pos': (4, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 775458}\n",
      "Regret for current level: 0.15037001192569732, buffer size: 55\n",
      "\n",
      "=== ITERATION 34/509 SKIPPED: 9 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 3, 'start_pos': (4, 6), 'goal_pos': (4, 1), 'edited': False, 'seed_val': 464171}\n",
      "Regret for current level: 0.2584763720065355, buffer size: 56\n",
      "\n",
      "=== ITERATION 35/509 SKIPPED: 9 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 9, 'start_pos': (4, 6), 'goal_pos': (2, 1), 'edited': False, 'seed_val': 26318}\n",
      "Regret for current level: 0.09684044688940047, buffer size: 57\n",
      "\n",
      "=== ITERATION 36/509 SKIPPED: 9 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 18, 'start_pos': (1, 2), 'goal_pos': (6, 6), 'edited': False, 'seed_val': 706839}\n",
      "Regret for current level is 0.00000 <= threshold 0.00260. Skipping...\n",
      "\n",
      "=== ITERATION 37/510 SKIPPED: 10 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.829689872264862, buffer size: 58\n",
      "\n",
      "=== ITERATION 38/510 SKIPPED: 10 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 4, 'start_pos': (3, 6), 'goal_pos': (5, 5), 'edited': False, 'seed_val': 325453}\n",
      "Regret for current level: 0.03176110841743649, buffer size: 59\n",
      "\n",
      "=== ITERATION 39/510 SKIPPED: 10 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.6680152833461761, buffer size: 60\n",
      "\n",
      "=== ITERATION 40/510 SKIPPED: 10 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 20, 'start_pos': (6, 2), 'goal_pos': (5, 1), 'edited': False, 'seed_val': 205285}\n",
      "Regret for current level is 0.00000 <= threshold 0.00290. Skipping...\n",
      "\n",
      "=== ITERATION 41/511 SKIPPED: 11 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.6214318096637725, buffer size: 61\n",
      "\n",
      "=== ITERATION 42/511 SKIPPED: 11 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 8, 'start_pos': (4, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 775458}\n",
      "Regret for current level: 0.15378792643547057, buffer size: 62\n",
      "\n",
      "=== ITERATION 43/511 SKIPPED: 11 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.5605418801307678, buffer size: 63\n",
      "\n",
      "=== ITERATION 44/511 SKIPPED: 11 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 19, 'start_pos': (3, 2), 'goal_pos': (4, 1), 'edited': False, 'seed_val': 874987}\n",
      "Regret for current level is 0.00000 <= threshold 0.00320. Skipping...\n",
      "\n",
      "=== ITERATION 45/512 SKIPPED: 12 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.6192052781581878, buffer size: 64\n",
      "\n",
      "=== ITERATION 46/512 SKIPPED: 12 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 13, 'start_pos': (1, 6), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 588047}\n",
      "Regret for current level is 0.00000 <= threshold 0.00330. Skipping...\n",
      "\n",
      "=== ITERATION 47/513 SKIPPED: 13 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (2, 2), 'goal_pos': (6, 1), 'edited': False, 'seed_val': 929292}\n",
      "Regret for current level is 0.00000 <= threshold 0.00330. Skipping...\n",
      "\n",
      "=== ITERATION 48/514 SKIPPED: 14 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.6389871060848236, buffer size: 64\n",
      "\n",
      "=== ITERATION 49/514 SKIPPED: 14 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 2, 'start_pos': (4, 3), 'goal_pos': (1, 3), 'edited': False, 'seed_val': 544505}\n",
      "Regret for current level: 0.10079659253358841, buffer size: 64\n",
      "\n",
      "=== ITERATION 50/514 SKIPPED: 14 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 2, 'start_pos': (1, 4), 'goal_pos': (1, 1), 'edited': False, 'seed_val': 504030}\n",
      "Regret for current level: 0.11808756871530042, buffer size: 64\n",
      "\n",
      "=== ITERATION 51/514 SKIPPED: 14 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 3, 'start_pos': (4, 6), 'goal_pos': (4, 1), 'edited': False, 'seed_val': 464171}\n",
      "Regret for current level: 0.24955283038884402, buffer size: 64\n",
      "\n",
      "=== ITERATION 52/514 SKIPPED: 14 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.6610951542854309, buffer size: 64\n",
      "\n",
      "=== ITERATION 53/514 SKIPPED: 14 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 8, 'start_pos': (4, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 775458}\n",
      "Regret for current level: 0.15521268129348753, buffer size: 64\n",
      "\n",
      "=== ITERATION 54/514 SKIPPED: 14 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 23, 'start_pos': (4, 4), 'goal_pos': (3, 2), 'edited': False, 'seed_val': 195825}\n",
      "Regret for current level is 0.00000 <= threshold 0.00390. Skipping...\n",
      "\n",
      "=== ITERATION 55/515 SKIPPED: 15 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 2, 'start_pos': (1, 4), 'goal_pos': (1, 1), 'edited': False, 'seed_val': 504030}\n",
      "Regret for current level: 0.09229539566099482, buffer size: 64\n",
      "\n",
      "=== ITERATION 56/515 SKIPPED: 15 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 8, 'start_pos': (2, 4), 'goal_pos': (6, 3), 'edited': False, 'seed_val': 515233}\n",
      "Regret for current level is 0.00185 <= threshold 0.00400. Skipping...\n",
      "\n",
      "=== ITERATION 57/516 SKIPPED: 16 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 4, 'start_pos': (4, 1), 'goal_pos': (1, 1), 'edited': False, 'seed_val': 229217}\n",
      "Regret for current level is 0.00000 <= threshold 0.00400. Skipping...\n",
      "\n",
      "=== ITERATION 58/517 SKIPPED: 17 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 11, 'start_pos': (1, 6), 'goal_pos': (5, 6), 'edited': False, 'seed_val': 743185}\n",
      "Regret for current level: 0.05275363484030142, buffer size: 64\n",
      "\n",
      "=== ITERATION 59/517 SKIPPED: 17 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.8271778643131256, buffer size: 64\n",
      "\n",
      "=== ITERATION 60/517 SKIPPED: 17 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 8, 'start_pos': (4, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 775458}\n",
      "Regret for current level: 0.15251222729682923, buffer size: 64\n",
      "\n",
      "=== ITERATION 61/517 SKIPPED: 17 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 14, 'start_pos': (4, 1), 'goal_pos': (6, 4), 'edited': False, 'seed_val': 358950}\n",
      "Regret for current level is 0.00000 <= threshold 0.00430. Skipping...\n",
      "\n",
      "=== ITERATION 62/518 SKIPPED: 18 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 8, 'start_pos': (4, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 775458}\n",
      "Regret for current level: 0.14810543835163115, buffer size: 64\n",
      "\n",
      "=== ITERATION 63/518 SKIPPED: 18 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.7774329304695129, buffer size: 64\n",
      "\n",
      "=== ITERATION 64/518 SKIPPED: 18 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 16, 'start_pos': (4, 2), 'goal_pos': (3, 6), 'edited': False, 'seed_val': 576067}\n",
      "Regret for current level is 0.00000 <= threshold 0.00450. Skipping...\n",
      "\n",
      "=== ITERATION 65/519 SKIPPED: 19 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.7320251584053039, buffer size: 64\n",
      "\n",
      "=== ITERATION 66/519 SKIPPED: 19 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.6885473251342773, buffer size: 64\n",
      "\n",
      "=== ITERATION 67/519 SKIPPED: 19 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.5872453272342681, buffer size: 64\n",
      "\n",
      "=== ITERATION 68/519 SKIPPED: 19 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 6, 'start_pos': (5, 2), 'goal_pos': (4, 1), 'edited': False, 'seed_val': 510981}\n",
      "Regret for current level: 0.02171582341194156, buffer size: 64\n",
      "\n",
      "=== ITERATION 69/519 SKIPPED: 19 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.6569754004478454, buffer size: 64\n",
      "\n",
      "=== ITERATION 70/519 SKIPPED: 19 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.5551236391067504, buffer size: 64\n",
      "\n",
      "=== ITERATION 71/519 SKIPPED: 19 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 20, 'start_pos': (6, 2), 'goal_pos': (2, 5), 'edited': False, 'seed_val': 289914}\n",
      "Regret for current level: 0.06252772361040115, buffer size: 64\n",
      "\n",
      "=== ITERATION 72/519 SKIPPED: 19 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 8, 'start_pos': (2, 6), 'goal_pos': (1, 4), 'edited': False, 'seed_val': 958014}\n",
      "Regret for current level: 0.07551373004913331, buffer size: 64\n",
      "\n",
      "=== ITERATION 73/519 SKIPPED: 19 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 12, 'start_pos': (2, 1), 'goal_pos': (6, 5), 'edited': False, 'seed_val': 795401}\n",
      "Regret for current level is 0.00000 <= threshold 0.00530. Skipping...\n",
      "\n",
      "=== ITERATION 74/520 SKIPPED: 20 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 12, 'start_pos': (5, 5), 'goal_pos': (5, 6), 'edited': False, 'seed_val': 207656}\n",
      "Regret for current level is 0.00000 <= threshold 0.00530. Skipping...\n",
      "\n",
      "=== ITERATION 75/521 SKIPPED: 21 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 9, 'start_pos': (2, 4), 'goal_pos': (2, 6), 'edited': False, 'seed_val': 664834}\n",
      "Regret for current level is 0.00000 <= threshold 0.00530. Skipping...\n",
      "\n",
      "=== ITERATION 76/522 SKIPPED: 22 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 10, 'start_pos': (1, 3), 'goal_pos': (6, 3), 'edited': False, 'seed_val': 942911}\n",
      "Regret for current level: 0.29564758191539703, buffer size: 64\n",
      "\n",
      "=== ITERATION 77/522 SKIPPED: 22 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 11, 'start_pos': (2, 2), 'goal_pos': (2, 3), 'edited': False, 'seed_val': 411352}\n",
      "Regret for current level: 0.7242274701595306, buffer size: 64\n",
      "\n",
      "=== ITERATION 78/522 SKIPPED: 22 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 18, 'start_pos': (4, 3), 'goal_pos': (6, 6), 'edited': False, 'seed_val': 940138}\n",
      "Regret for current level is 0.00000 <= threshold 0.00550. Skipping...\n",
      "\n",
      "=== ITERATION 79/523 SKIPPED: 23 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.6449468851089477, buffer size: 64\n",
      "\n",
      "=== ITERATION 80/523 SKIPPED: 23 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 2, 'start_pos': (3, 1), 'goal_pos': (2, 6), 'edited': False, 'seed_val': 85100}\n",
      "Regret for current level: 0.3503779762983322, buffer size: 64\n",
      "\n",
      "=== ITERATION 81/523 SKIPPED: 23 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.5030056655406951, buffer size: 64\n",
      "\n",
      "=== ITERATION 82/523 SKIPPED: 23 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 17, 'start_pos': (1, 1), 'goal_pos': (6, 5), 'edited': False, 'seed_val': 626444}\n",
      "Regret for current level is 0.00000 <= threshold 0.00580. Skipping...\n",
      "\n",
      "=== ITERATION 83/524 SKIPPED: 24 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 14, 'start_pos': (2, 3), 'goal_pos': (4, 2), 'edited': False, 'seed_val': 133942}\n",
      "Regret for current level: 0.36133776533424855, buffer size: 64\n",
      "\n",
      "=== ITERATION 84/524 SKIPPED: 24 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 1, 'start_pos': (3, 5), 'goal_pos': (4, 2), 'edited': False, 'seed_val': 354949}\n",
      "Regret for current level: 0.283496643478896, buffer size: 64\n",
      "\n",
      "=== ITERATION 85/524 SKIPPED: 24 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.5493816256523132, buffer size: 64\n",
      "\n",
      "=== ITERATION 86/524 SKIPPED: 24 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.4680674791336059, buffer size: 64\n",
      "\n",
      "=== ITERATION 87/524 SKIPPED: 24 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 23, 'start_pos': (4, 4), 'goal_pos': (3, 3), 'edited': False, 'seed_val': 171067}\n",
      "Regret for current level: 0.2700437328869104, buffer size: 64\n",
      "\n",
      "=== ITERATION 88/524 SKIPPED: 24 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 2, 'start_pos': (3, 1), 'goal_pos': (2, 6), 'edited': False, 'seed_val': 85100}\n",
      "Regret for current level: 0.294885892868042, buffer size: 64\n",
      "\n",
      "=== ITERATION 89/524 SKIPPED: 24 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.5291613757610321, buffer size: 64\n",
      "\n",
      "=== ITERATION 90/524 SKIPPED: 24 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 3), 'goal_pos': (3, 1), 'edited': False, 'seed_val': 916702}\n",
      "Regret for current level is 0.00000 <= threshold 0.00650. Skipping...\n",
      "\n",
      "=== ITERATION 91/525 SKIPPED: 25 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 11, 'start_pos': (2, 2), 'goal_pos': (2, 3), 'edited': False, 'seed_val': 411352}\n",
      "Regret for current level: 0.3798198223114013, buffer size: 64\n",
      "\n",
      "=== ITERATION 92/525 SKIPPED: 25 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 2, 'start_pos': (1, 4), 'goal_pos': (1, 1), 'edited': False, 'seed_val': 504030}\n",
      "Regret for current level: 0.1732050157158246, buffer size: 64\n",
      "\n",
      "=== ITERATION 93/525 SKIPPED: 25 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 9, 'start_pos': (6, 4), 'goal_pos': (5, 4), 'edited': False, 'seed_val': 530097}\n",
      "Regret for current level: 0.451800000667572, buffer size: 64\n",
      "\n",
      "=== ITERATION 94/525 SKIPPED: 25 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 8, 'start_pos': (4, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 775458}\n",
      "Regret for current level: 0.14821664217428449, buffer size: 64\n",
      "\n",
      "=== ITERATION 95/525 SKIPPED: 25 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 4, 'start_pos': (6, 5), 'goal_pos': (5, 2), 'edited': False, 'seed_val': 205922}\n",
      "Regret for current level: 0.3327109139869679, buffer size: 64\n",
      "\n",
      "=== ITERATION 96/525 SKIPPED: 25 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.33346713781356807, buffer size: 64\n",
      "\n",
      "=== ITERATION 97/525 SKIPPED: 25 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.3150639653205871, buffer size: 64\n",
      "\n",
      "=== ITERATION 98/525 SKIPPED: 25 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 9, 'start_pos': (2, 4), 'goal_pos': (4, 4), 'edited': False, 'seed_val': 736918}\n",
      "Regret for current level: 0.09087981475867392, buffer size: 64\n",
      "\n",
      "=== ITERATION 99/525 SKIPPED: 25 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 7, 'start_pos': (3, 4), 'goal_pos': (1, 6), 'edited': False, 'seed_val': 574415}\n",
      "Regret for current level: 0.07362550151448592, buffer size: 64\n",
      "\n",
      "=== ITERATION 100/525 SKIPPED: 25 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 21, 'start_pos': (3, 5), 'goal_pos': (5, 3), 'edited': False, 'seed_val': 683716}\n",
      "Regret for current level: 0.2305370828962326, buffer size: 64\n",
      "\n",
      "=== ITERATION 101/525 SKIPPED: 25 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.2859667062759399, buffer size: 64\n",
      "\n",
      "=== ITERATION 102/525 SKIPPED: 25 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 12, 'start_pos': (1, 6), 'goal_pos': (4, 2), 'edited': False, 'seed_val': 31669}\n",
      "Regret for current level: 0.12930039283395264, buffer size: 64\n",
      "\n",
      "=== ITERATION 103/525 SKIPPED: 25 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 11, 'start_pos': (5, 4), 'goal_pos': (6, 6), 'edited': False, 'seed_val': 536402}\n",
      "Regret for current level: 0.1880594783869395, buffer size: 64\n",
      "\n",
      "=== ITERATION 104/525 SKIPPED: 25 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 19, 'start_pos': (5, 4), 'goal_pos': (4, 5), 'edited': False, 'seed_val': 753696}\n",
      "Regret for current level is 0.00000 <= threshold 0.00780. Skipping...\n",
      "\n",
      "=== ITERATION 105/526 SKIPPED: 26 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 14, 'start_pos': (5, 3), 'goal_pos': (5, 6), 'edited': False, 'seed_val': 676685}\n",
      "Regret for current level: 0.14813513319420712, buffer size: 64\n",
      "\n",
      "=== ITERATION 106/526 SKIPPED: 26 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 4, 'start_pos': (6, 2), 'goal_pos': (1, 3), 'edited': False, 'seed_val': 938431}\n",
      "Regret for current level: 0.08631100466658768, buffer size: 64\n",
      "\n",
      "=== ITERATION 107/526 SKIPPED: 26 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 2, 'start_pos': (1, 4), 'goal_pos': (1, 1), 'edited': False, 'seed_val': 504030}\n",
      "Regret for current level: 0.45716931811585454, buffer size: 64\n",
      "\n",
      "=== ITERATION 108/526 SKIPPED: 26 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 10, 'start_pos': (1, 3), 'goal_pos': (5, 6), 'edited': False, 'seed_val': 898701}\n",
      "Regret for current level is 0.00000 <= threshold 0.00810. Skipping...\n",
      "\n",
      "=== ITERATION 109/527 SKIPPED: 27 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 2, 'start_pos': (1, 4), 'goal_pos': (1, 1), 'edited': False, 'seed_val': 504030}\n",
      "Regret for current level: 0.5107426821247639, buffer size: 64\n",
      "\n",
      "=== ITERATION 110/527 SKIPPED: 27 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 2, 'start_pos': (1, 4), 'goal_pos': (1, 1), 'edited': False, 'seed_val': 504030}\n",
      "Regret for current level: 0.4669652471170083, buffer size: 64\n",
      "\n",
      "=== ITERATION 111/527 SKIPPED: 27 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.30481721162796016, buffer size: 64\n",
      "\n",
      "=== ITERATION 112/527 SKIPPED: 27 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 10, 'start_pos': (6, 6), 'goal_pos': (2, 5), 'edited': False, 'seed_val': 349597}\n",
      "Regret for current level: 0.09450404763221742, buffer size: 64\n",
      "\n",
      "=== ITERATION 113/527 SKIPPED: 27 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.2872874855995178, buffer size: 64\n",
      "\n",
      "=== ITERATION 114/527 SKIPPED: 27 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 17, 'start_pos': (3, 1), 'goal_pos': (1, 1), 'edited': False, 'seed_val': 36931}\n",
      "Regret for current level is 0.00000 <= threshold 0.00860. Skipping...\n",
      "\n",
      "=== ITERATION 115/528 SKIPPED: 28 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 13, 'start_pos': (3, 5), 'goal_pos': (5, 5), 'edited': False, 'seed_val': 733272}\n",
      "Regret for current level: 0.3117594162991901, buffer size: 64\n",
      "\n",
      "=== ITERATION 116/528 SKIPPED: 28 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.23143113851547237, buffer size: 64\n",
      "\n",
      "=== ITERATION 117/528 SKIPPED: 28 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 2, 'start_pos': (5, 1), 'goal_pos': (4, 3), 'edited': False, 'seed_val': 421243}\n",
      "Regret for current level: 0.11193243744045497, buffer size: 64\n",
      "\n",
      "=== ITERATION 118/528 SKIPPED: 28 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 5, 'start_pos': (5, 1), 'goal_pos': (2, 1), 'edited': False, 'seed_val': 56608}\n",
      "Regret for current level: 0.28075226281398186, buffer size: 64\n",
      "\n",
      "=== ITERATION 119/528 SKIPPED: 28 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (2, 5), 'goal_pos': (3, 6), 'edited': False, 'seed_val': 450907}\n",
      "Regret for current level: 0.33164524112233673, buffer size: 64\n",
      "\n",
      "=== ITERATION 120/528 SKIPPED: 28 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.2496912598609924, buffer size: 64\n",
      "\n",
      "=== ITERATION 121/528 SKIPPED: 28 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.20932800769805904, buffer size: 64\n",
      "\n",
      "=== ITERATION 122/528 SKIPPED: 28 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 17, 'start_pos': (4, 1), 'goal_pos': (3, 3), 'edited': False, 'seed_val': 958220}\n",
      "Regret for current level is 0.00000 <= threshold 0.00930. Skipping...\n",
      "\n",
      "=== ITERATION 123/529 SKIPPED: 29 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.18831033706665035, buffer size: 64\n",
      "\n",
      "=== ITERATION 124/529 SKIPPED: 29 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (4, 1), 'goal_pos': (5, 6), 'edited': False, 'seed_val': 287439}\n",
      "Regret for current level: 0.0196318833822012, buffer size: 64\n",
      "\n",
      "=== ITERATION 125/529 SKIPPED: 29 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (2, 5), 'goal_pos': (3, 6), 'edited': False, 'seed_val': 450907}\n",
      "Regret for current level: 0.26070367630057023, buffer size: 64\n",
      "\n",
      "=== ITERATION 126/529 SKIPPED: 29 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 14, 'start_pos': (5, 3), 'goal_pos': (4, 2), 'edited': False, 'seed_val': 41498}\n",
      "Regret for current level is 0.00000 <= threshold 0.00960. Skipping...\n",
      "\n",
      "=== ITERATION 127/530 SKIPPED: 30 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.17212398052215572, buffer size: 64\n",
      "\n",
      "=== ITERATION 128/530 SKIPPED: 30 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.16287000179290767, buffer size: 64\n",
      "\n",
      "=== ITERATION 129/530 SKIPPED: 30 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 8, 'start_pos': (4, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 775458}\n",
      "Regret for current level: 0.13385009185902536, buffer size: 64\n",
      "\n",
      "=== ITERATION 130/530 SKIPPED: 30 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 2, 'start_pos': (5, 2), 'goal_pos': (1, 2), 'edited': False, 'seed_val': 201310}\n",
      "Regret for current level: 0.22755195738384343, buffer size: 64\n",
      "\n",
      "=== ITERATION 131/530 SKIPPED: 30 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.1691640734672546, buffer size: 64\n",
      "\n",
      "=== ITERATION 132/530 SKIPPED: 30 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.1446310997009277, buffer size: 64\n",
      "\n",
      "=== ITERATION 133/530 SKIPPED: 30 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 23, 'start_pos': (4, 2), 'goal_pos': (3, 4), 'edited': False, 'seed_val': 915812}\n",
      "Regret for current level: 0.09560345277503654, buffer size: 64\n",
      "\n",
      "=== ITERATION 134/530 SKIPPED: 30 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 2, 'start_pos': (1, 4), 'goal_pos': (1, 1), 'edited': False, 'seed_val': 504030}\n",
      "Regret for current level: 0.3723855205041234, buffer size: 64\n",
      "\n",
      "=== ITERATION 135/530 SKIPPED: 30 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.15578300952911373, buffer size: 64\n",
      "\n",
      "=== ITERATION 136/530 SKIPPED: 30 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.14893974065780635, buffer size: 64\n",
      "\n",
      "=== ITERATION 137/530 SKIPPED: 30 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.13549555540084834, buffer size: 64\n",
      "\n",
      "=== ITERATION 138/530 SKIPPED: 30 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 11, 'start_pos': (4, 3), 'goal_pos': (3, 5), 'edited': False, 'seed_val': 826911}\n",
      "Regret for current level: 0.23325510187178847, buffer size: 64\n",
      "\n",
      "=== ITERATION 139/530 SKIPPED: 30 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 8, 'start_pos': (2, 4), 'goal_pos': (3, 1), 'edited': False, 'seed_val': 310962}\n",
      "Regret for current level: 0.08609474746179645, buffer size: 64\n",
      "\n",
      "=== ITERATION 140/530 SKIPPED: 30 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 1, 'start_pos': (3, 5), 'goal_pos': (4, 2), 'edited': False, 'seed_val': 354949}\n",
      "Regret for current level: 0.1330484354496002, buffer size: 64\n",
      "\n",
      "=== ITERATION 141/530 SKIPPED: 30 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 16, 'start_pos': (6, 6), 'goal_pos': (3, 4), 'edited': False, 'seed_val': 839044}\n",
      "Regret for current level: 0.11901107334315772, buffer size: 64\n",
      "\n",
      "=== ITERATION 142/530 SKIPPED: 30 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 6, 'start_pos': (5, 6), 'goal_pos': (1, 5), 'edited': False, 'seed_val': 665721}\n",
      "Regret for current level is 0.00000 <= threshold 0.01110. Skipping...\n",
      "\n",
      "=== ITERATION 143/531 SKIPPED: 31 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 13, 'start_pos': (3, 5), 'goal_pos': (5, 5), 'edited': False, 'seed_val': 733272}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "        \n",
    "    wandb.finish()\n",
    "        \n",
    "    config = {\n",
    "            \"grid_size\": 8,\n",
    "            \n",
    "            \"total_iterations\": 500,\n",
    "            \"train_steps\": 1000,\n",
    "\n",
    "            \"replay_prob\": 0.5,            # Probability of replaying a level and editing it vs. generating a new one\n",
    "            \"level_buffer_size\": 128,       # Maximum number of levels to store in the buffer\n",
    "            \"initial_fill_size\": 64,       # Number of levels to pre-fill the buffer with\n",
    "            \"regret_threshold\": 0.00,      # Minimum regret threshold to consider a level for the buffer\n",
    "            \n",
    "            \"n_envs\": 3,                   # Number of parallel environments to use for training\n",
    "            \n",
    "            \"edit_levels\": True,           # Whether to edit levels during training i.e. ACCEL or PLR\n",
    "            \"easy_start\": True,            # Whether to fill the buffer with easy levels first i.e. minimum number of blocks\n",
    "    \n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "    config[\"edit_levels\"] = False\n",
    "    config[\"easy_start\"] = False\n",
    "    print(f\"Running PLR with config: {config}\")\n",
    "    model_plr = main_accel(**config)\n",
    "\n",
    "    # Save the model\n",
    "    model_plr.save(f\"models/plr_model_{config['grid_size']}x{config['grid_size']}\")\n",
    "\n",
    "    print(\"\\n\\n============================================\\n\\n\")\n",
    "\n",
    "    config[\"edit_levels\"] = True\n",
    "    config[\"easy_start\"] = False\n",
    "    print(f\"Running ACCEL with config: {config}\")\n",
    "    model_accel = main_accel(**config)\n",
    "\n",
    "    # Save the model\n",
    "    model_accel.save(f\"models/accel_model_{config['grid_size']}x{config['grid_size']}\")\n",
    "\n",
    "    print(\"\\n\\n============================================\\n\\n\")\n",
    "    \n",
    "\n",
    "    config[\"edit_levels\"] = True\n",
    "    config[\"easy_start\"] = True\n",
    "    print(f\"Running ACCEL with easy start with config: {config}\")\n",
    "    model_accel_easy = main_accel(**config)\n",
    "\n",
    "    # Save the model\n",
    "    model_accel_easy.save(f\"models/accel_model_easy_{config['grid_size']}x{config['grid_size']}\")\n",
    "    \n",
    "    # Evaluate the models\n",
    "    evalute_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, config):\n",
    "    env = MyCustomGrid(config, render_mode='human', solvable_only=True)\n",
    "    obs, _ = env.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    total_reward = 0\n",
    "    i = 0\n",
    "\n",
    "    # Continue until either terminated or truncated is True\n",
    "    while not (terminated or truncated):\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        i += 1\n",
    "        if i > 20:\n",
    "            break\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "# Test the models on a few ranom levels\n",
    "for i in range(30):\n",
    "    #test_model(model_plr, random_config(6))\n",
    "    #test_model(model_accel, random_config(6))\n",
    "    test_model(model_accel_easy, random_config(6))\n",
    "\"\"\"\n",
    "models = {\n",
    "    \"PLR\": model_plr,\n",
    "    \"ACCEL\": model_accel,\n",
    "    \"ACCEL-EasyStart\": model_accel_easy\n",
    "}\n",
    "    \n",
    "# Test the models on previously evaluated levels\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Testing model {model_name} on previously evaluated levels...\")\n",
    "    for i, level in enumerate(levels):\n",
    "        print(f\"Level {i + 1} - Complexity {(i+2)**2}:\")\n",
    "        for j, cfg in enumerate(level):\n",
    "            mean_reward = test_model(model, cfg)\n",
    "            print(f\"  Config {j + 1}: {mean_reward:.2f}\")\n",
    "        print()\n",
    "    print()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level Editor Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random level and visualize it\n",
    "random_cnf = random_config(8)\n",
    "print(\"random_cnf:\", random_cnf)\n",
    "\n",
    "print_level_from_config(random_cnf, solvable_only=True)\n",
    "'''\n",
    "# Edit the random level and visualize it\n",
    "edited_config = edit_config(random_cnf)\n",
    "print_level_from_config(edited_config)\n",
    "print(\"edited_config:\", edited_config)\n",
    "\n",
    "# Edit the random level and visualize it\n",
    "edited_config = edit_config(random_cnf)\n",
    "print_level_from_config(edited_config)\n",
    "print(\"edited_config:\", edited_config)\n",
    "\n",
    "# Edit the random level and visualize it\n",
    "edited_config = edit_config(random_cnf)\n",
    "print_level_from_config(edited_config)\n",
    "print(\"edited_config:\", edited_config)\n",
    "'''\n",
    "\n",
    "config = random_config(6)\n",
    "\n",
    "print_level_from_config(config, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bipedal Walker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://gymnasium.farama.org/environments/box2d/bipedal_walker/\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.utils import obs_as_tensor\n",
    "import random\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "class BipedalWalkerParamWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    A wrapper around BipedalWalker (or BipedalWalkerHardcore) \n",
    "    that sets custom parameters for terrain generation at reset time.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env_id=\"BipedalWalker-v3\", hardcore=False):\n",
    "        super().__init__(gym.make(env_id))\n",
    "        self.hardcore = hardcore\n",
    "\n",
    "        # The environment's internal parameters. \n",
    "        # We will override these at reset to control terrain generation.\n",
    "        self.config = {\n",
    "            \"seed_val\": None,\n",
    "            \"stump_height\": 0.0,\n",
    "            \"stair_height\": 0.0,\n",
    "            \"stair_steps\": 1,\n",
    "            \"roughness\": 0.0,\n",
    "            \"pit_gap\": 0.0,\n",
    "        }\n",
    "\n",
    "        # If we want hardcore version, you can do so with:\n",
    "        if self.hardcore:\n",
    "            self.env = gym.make(\"BipedalWalkerHardcore-v3\")\n",
    "\n",
    "    def set_config(self, config_dict):\n",
    "        \"\"\"Update environment parameters.\"\"\"\n",
    "        for k, v in config_dict.items():\n",
    "            self.config[k] = v\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Modify the environment's terrain parameters right before reset.\n",
    "        This monkey-patches internal Box2D variables, if needed,\n",
    "        or sets seeds to randomize terrain accordingly.\n",
    "        \"\"\"\n",
    "        # 1) Seed:\n",
    "        #if self.config[\"seed_val\"] is not None:\n",
    "        #    self.env.seed(self.config[\"seed_val\"])\n",
    "\n",
    "        # 2) Override terrain parameters in the underlying env\n",
    "        #    (We rely on the environment reading these at reset or having\n",
    "        #     references in the terrain generation code. Depending on \n",
    "        #     your exact BipedalWalker implementation, you might need \n",
    "        #     to modify the source or do partial overrides.)\n",
    "        self.env.unwrapped.stump_height = self.config[\"stump_height\"]\n",
    "        self.env.unwrapped.stair_height = self.config[\"stair_height\"]\n",
    "        self.env.unwrapped.stair_steps = int(self.config[\"stair_steps\"])\n",
    "        self.env.unwrapped.roughness = self.config[\"roughness\"]\n",
    "        self.env.unwrapped.pit_gap = self.config[\"pit_gap\"]\n",
    "\n",
    "        obs = self.env.reset(**kwargs)\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "    \n",
    "\n",
    "\n",
    "# The min and max values for each terrain parameter.\n",
    "# For simplicity, define them here, but you can store these in a table/dict.\n",
    "PARAM_BOUNDS = {\n",
    "    \"stump_height\":  (0.0, 5.0),\n",
    "    \"stair_height\":  (0.0, 5.0),\n",
    "    \"stair_steps\":   (1,   9),\n",
    "    \"roughness\":     (0.0, 10.0),\n",
    "    \"pit_gap\":       (0.0, 10.0)\n",
    "}\n",
    "\n",
    "def random_config(easy_init=False):\n",
    "    \"\"\"\n",
    "    Sample a random environment config. If easy_init=True,\n",
    "    you might restrict sampling to small values, so the agent\n",
    "    starts with simpler terrain.\n",
    "    \"\"\"\n",
    "    cfg = {}\n",
    "    \n",
    "    # Optionally set a random seed \n",
    "    cfg[\"seed_val\"] = random.randint(0, 1_000_000)\n",
    "\n",
    "    def sample_uniform(low, high):\n",
    "        return random.uniform(low, high)\n",
    "\n",
    "    if easy_init:\n",
    "        # Possibly narrower range for simpler (initial) levels\n",
    "        cfg[\"stump_height\"] = sample_uniform(0.0, 0.4)\n",
    "        cfg[\"stair_height\"] = sample_uniform(0.0, 0.4)\n",
    "        cfg[\"stair_steps\"]  = random.randint(1, 2)\n",
    "        cfg[\"roughness\"]    = sample_uniform(0.0, 0.6)\n",
    "        cfg[\"pit_gap\"]      = sample_uniform(0.0, 0.8)\n",
    "    else:\n",
    "        cfg[\"stump_height\"] = sample_uniform(*PARAM_BOUNDS[\"stump_height\"])\n",
    "        cfg[\"stair_height\"] = sample_uniform(*PARAM_BOUNDS[\"stair_height\"])\n",
    "        cfg[\"stair_steps\"]  = random.randint(*PARAM_BOUNDS[\"stair_steps\"])\n",
    "        cfg[\"roughness\"]    = sample_uniform(*PARAM_BOUNDS[\"roughness\"])\n",
    "        cfg[\"pit_gap\"]      = sample_uniform(*PARAM_BOUNDS[\"pit_gap\"])\n",
    "\n",
    "    return cfg\n",
    "\n",
    "def edit_config(old_cfg):\n",
    "    \"\"\"\n",
    "    Make a small 'mutation' to the old_cfg.\n",
    "    This can be random increments/decrements to each parameter, \n",
    "    or randomly choose one parameter to mutate.\n",
    "    \"\"\"\n",
    "    new_cfg = dict(old_cfg)\n",
    "    param_to_edit = random.choice([\"stump_height\", \"stair_height\", \"stair_steps\", \"roughness\", \"pit_gap\"])\n",
    "    # pick small delta \n",
    "    delta = random.uniform(0.1, 1.0)\n",
    "\n",
    "    # Add or subtract\n",
    "    sign = random.choice([-1, 1])\n",
    "    new_val = new_cfg[param_to_edit] + sign * delta\n",
    "    \n",
    "    # Clip to valid range\n",
    "    low, high = PARAM_BOUNDS[param_to_edit]\n",
    "    new_cfg[param_to_edit] = np.clip(new_val, low, high)\n",
    "\n",
    "    # Possibly update the seed too \n",
    "    new_cfg[\"seed_val\"] = random.randint(0, 1_000_000)\n",
    "    return new_cfg\n",
    "\n",
    "\n",
    "\n",
    "class LevelReplayBuffer:\n",
    "    \"\"\"\n",
    "    Stores (config, score). Score is e.g. a 'regret' approximation.\n",
    "    We keep the highest-score configs up to max_size.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_size=100):\n",
    "        self.max_size = max_size\n",
    "        self.data = []\n",
    "\n",
    "    def add(self, cfg, score):\n",
    "        self.data.append((cfg, score))\n",
    "        # keep only top K by score\n",
    "        self.data.sort(key=lambda x: x[1], reverse=True)\n",
    "        self.data = self.data[:self.max_size]\n",
    "\n",
    "    def sample(self):\n",
    "        if len(self.data) == 0:\n",
    "            return None\n",
    "        # Weighted sample by score or just pick the top \n",
    "        # For simplicity, pick randomly from top half:\n",
    "        half = len(self.data) // 2\n",
    "        idx = np.random.randint(0, max(1, half))\n",
    "        return self.data[idx][0]\n",
    "\n",
    "def estimate_regret(env, model, max_steps=1000, gamma=0.99, lam=0.95):\n",
    "    \"\"\"\n",
    "    Calculate regret using Generalized Advantage Estimation (GAE)\n",
    "    with Stable-Baselines3's PPO model.\n",
    "    \"\"\"\n",
    "    obs = env.reset()[0]\n",
    "    regrets = []\n",
    "    rewards = []\n",
    "    dones = []\n",
    "    values = []\n",
    "    \n",
    "\n",
    "    for t in range(max_steps):\n",
    "        # Add batch dimension to the observation tensor\n",
    "        obs_tensor = torch.as_tensor(obs).float().unsqueeze(0).to(device)\n",
    "        \n",
    "        # Use the model's policy to get the value and action.\n",
    "        # For actions, model.predict handles single observations well.\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        \n",
    "        # Compute the value from the policy.\n",
    "        value_t = model.policy.predict_values(obs_tensor).item()\n",
    "        values.append(value_t)\n",
    "        \n",
    "        # Perform the step in the environment\n",
    "        obs, reward, done, truncated, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "    # Add value of the terminal state (0 if done/truncated)\n",
    "    if done or truncated:\n",
    "        terminal_value = 0.0\n",
    "    else:\n",
    "        terminal_obs_tensor = torch.as_tensor(obs).float().unsqueeze(0).to(device)\n",
    "        terminal_value = model.policy.predict_values(terminal_obs_tensor).item()\n",
    "    values.append(terminal_value)\n",
    "\n",
    "    # Compute TD-errors and GAE-like regret score\n",
    "    for t in range(len(rewards)):\n",
    "        delta_t = rewards[t] + gamma * values[t + 1] * (1 - dones[t]) - values[t]\n",
    "        discounted_error = (gamma * lam) ** t * delta_t\n",
    "        regrets.append(max(0, discounted_error))\n",
    "\n",
    "    # Return the maximum positive regret score (or 0 if empty)\n",
    "    return max(regrets) if regrets else 0.0\n",
    "\n",
    "def run_accel_bipedal(\n",
    "    total_iterations=50,\n",
    "    steps_per_iteration=10000,\n",
    "    replay_prob=0.8,\n",
    "    regret_threshold=1.0,\n",
    "    max_buf_size=100,\n",
    "    easy_start=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Minimal ACCEL training loop on BipedalWalker.\n",
    "    \"\"\"\n",
    "    # 1) Create the environment wrapper + vectorize\n",
    "    def make_env():\n",
    "        env = BipedalWalkerParamWrapper(env_id=\"BipedalWalkerHardcore-v3\", hardcore=True)\n",
    "        return env\n",
    "    \n",
    "    # (Alternatively, do SubprocVecEnv if you want parallel CPU rollouts.)\n",
    "    vec_env = DummyVecEnv([make_env])\n",
    "\n",
    "    # 2) Initialize PPO\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\", \n",
    "        vec_env, \n",
    "        n_steps=2048,        # must be multiple of vec_env.num_envs\n",
    "        batch_size=64,\n",
    "        learning_rate=3e-4,\n",
    "        gamma=0.99,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    # 3) Create LevelReplayBuffer\n",
    "    level_buffer = LevelReplayBuffer(max_size=max_buf_size)\n",
    "\n",
    "    # 4) [Optional] Pre-fill buffer with some easy or random levels\n",
    "    for _ in range(10):  # pre-fill 10 levels\n",
    "        print(\"Pre-filling buffer...\")\n",
    "        cfg = random_config(easy_init=easy_start)\n",
    "        env = make_env()  # fresh environment\n",
    "        env.set_config(cfg)\n",
    "        # Train on it briefly to get a partial updated policy \n",
    "        model.set_env(DummyVecEnv([lambda: env]))\n",
    "        model.learn(total_timesteps=2000)\n",
    "\n",
    "        # Evaluate regret\n",
    "        rew_env = make_env()\n",
    "        rew_env.set_config(cfg)\n",
    "        regret = estimate_regret(rew_env, model)\n",
    "\n",
    "        if regret >= regret_threshold:\n",
    "            level_buffer.add(cfg, regret)\n",
    "        \n",
    "    \n",
    "    print(\"Buffer pre-filled. Starting main loop...\")\n",
    "\n",
    "    # 5) ACCEL main loop\n",
    "    for it in range(total_iterations):\n",
    "        # Decide: replay from buffer or create new\n",
    "        use_replay = (np.random.rand() < replay_prob) and (len(level_buffer.data) > 0)\n",
    "        if use_replay:\n",
    "            cfg = level_buffer.sample()\n",
    "            # Optionally edit the config to keep pushing frontier\n",
    "            cfg = edit_config(cfg)\n",
    "        else:\n",
    "            cfg = random_config(easy_init=False)\n",
    "\n",
    "        # Train on that config\n",
    "        env = make_env()\n",
    "        env.set_config(cfg)\n",
    "        model.set_env(DummyVecEnv([lambda: env]))\n",
    "        model.learn(total_timesteps=steps_per_iteration)\n",
    "\n",
    "        # Now measure regret\n",
    "        rew_env = make_env()\n",
    "        rew_env.set_config(cfg)\n",
    "        regret = estimate_regret(rew_env, model)\n",
    "\n",
    "        if regret >= regret_threshold:\n",
    "            level_buffer.add(cfg, regret)\n",
    "\n",
    "        print(f\"[Iteration {it+1}/{total_iterations}] => Config = {cfg}, Regret = {regret:.3f}, \"\n",
    "              f\"Buffer size = {len(level_buffer.data)}\")\n",
    "\n",
    "    return model, level_buffer\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trained_model, buffer_data = run_accel_bipedal(\n",
    "        total_iterations=250,\n",
    "        steps_per_iteration=1000,\n",
    "        replay_prob=0.8,\n",
    "        regret_threshold=1.0,\n",
    "        max_buf_size=100,\n",
    "        easy_start=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Evaluate final performance on some reference environment\n",
    "    test_env = gym.make(\"BipedalWalkerHardcore-v3\", render_mode='human')\n",
    "    mean_return = 0.0\n",
    "    N = 10\n",
    "    for _ in range(N):\n",
    "        obs = test_env.reset()[0]\n",
    "        done = False\n",
    "        episode_reward = 0.0\n",
    "        while not done:\n",
    "            action, _ = trained_model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, truncated, info = test_env.step(action)\n",
    "            episode_reward += reward\n",
    "            if truncated:\n",
    "                done = True\n",
    "        mean_return += episode_reward\n",
    "    mean_return /= N\n",
    "    print(f\"Avg Return on BipedalWalkerHardcore-v3 over {N} trials: {mean_return}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maze generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_maze(width, height):\n",
    "    maze = [[1 for _ in range(width)] for _ in range(height)]  # 1 for walls\n",
    "    stack = []\n",
    "    directions = [(0, 2), (0, -2), (2, 0), (-2, 0)]\n",
    "\n",
    "    def is_valid(x, y):\n",
    "        return 0 < x < height - 1 and 0 < y < width - 1 and maze[x][y] == 1\n",
    "\n",
    "    def carve(x, y):\n",
    "        maze[x][y] = 0\n",
    "        random.shuffle(directions)\n",
    "        for dx, dy in directions:\n",
    "            nx, ny = x + dx, y + dy\n",
    "            if is_valid(nx, ny):\n",
    "                maze[x + dx // 2][y + dy // 2] = 0\n",
    "                carve(nx, ny)\n",
    "\n",
    "    carve(1, 1)  # Start at (1, 1)\n",
    "\n",
    "    return maze\n",
    "\n",
    "# Display the maze\n",
    "def print_maze(maze):\n",
    "    for row in maze:\n",
    "        print(\"\".join(\"█\" if cell == 1 else \" \" for cell in row))\n",
    "\n",
    "maze = generate_maze(21, 21)\n",
    "print_maze(maze)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomMaze(MiniGridEnv):\n",
    "    \"\"\"\n",
    "    Simple MiniGrid environment that places random wall tiles\n",
    "    according to a config dict, returning only the 'image' observation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config=None, **kwargs):\n",
    "        if config is None:\n",
    "            config = {}\n",
    "        self.config = config\n",
    "\n",
    "        # Extract parameters from config\n",
    "        self.width = config.get(\"width\")\n",
    "        self.height = config.get(\"height\")\n",
    "        self.num_blocks = config.get(\"num_blocks\")\n",
    "        self.custom_seed = config.get(\"seed_val\")\n",
    "        \n",
    "        \n",
    "        # Create a random number generator with the custom seed\n",
    "        self.rng = np.random.default_rng(seed=self.custom_seed)\n",
    "\n",
    "        grid_size = max(self.width, self.height)\n",
    "\n",
    "        mission_space = MissionSpace(mission_func=lambda: \"get to the green goal square\")\n",
    "\n",
    "        super().__init__(\n",
    "            grid_size=grid_size,\n",
    "            max_steps=self.width * self.height * 2, # max_steps is typically 2x the grid size\n",
    "            see_through_walls=False,\n",
    "            agent_view_size=5,                      # Size of the agent's view square\n",
    "            mission_space=mission_space,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # Manually define our observation_space as a single Box (the image).\n",
    "        # By default, MiniGrid's image shape is (view_size, view_size, 3) if using partial obs,\n",
    "        # or (height, width, 3) if using full-grid observation. We'll do full-grid here:\n",
    "        # We'll define (self.height, self.width, 3) as the shape.\n",
    "        # In practice, \"image\" shape can vary if partial observations are used.\n",
    "        self.observation_space = Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=(self.agent_view_size, self.agent_view_size, 3),\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "\n",
    "    \n",
    "    def _gen_grid(self, width, height):\n",
    "        \"\"\"\n",
    "        Generate the grid layout for a new episode using the DFS Maze Generation Algorithm.\n",
    "        \"\"\"\n",
    "        # Create an empty grid of the \"true\" width x height from config\n",
    "        self.grid = Grid(self.width, self.height)\n",
    "        # Surround the grid with walls\n",
    "        self.grid.wall_rect(0, 0, self.width, self.height)\n",
    "\n",
    "        # Initialize the maze as walls\n",
    "        maze = [[1 for _ in range(self.width)] for _ in range(self.height)]\n",
    "\n",
    "        # Define directions for DFS\n",
    "        directions = [(0, 2), (0, -2), (2, 0), (-2, 0)]\n",
    "\n",
    "        def is_valid(x, y):\n",
    "            \"\"\"Check if a cell is valid for carving.\"\"\"\n",
    "            return 0 < x < self.height - 1 and 0 < y < self.width - 1 and maze[x][y] == 1\n",
    "\n",
    "        def carve(x, y):\n",
    "            \"\"\"Carve passages in the maze using DFS.\"\"\"\n",
    "            maze[x][y] = 0  # Mark the cell as part of the maze\n",
    "            self.grid.set(x, y, None)  # Clear the wall in the grid\n",
    "            self.rng.shuffle(directions)\n",
    "            for dx, dy in directions:\n",
    "                nx, ny = x + dx, y + dy\n",
    "                if is_valid(nx, ny):\n",
    "                    # Remove the wall between cells\n",
    "                    maze[x + dx // 2][y + dy // 2] = 0\n",
    "                    self.grid.set(x + dx // 2, y + dy // 2, None)\n",
    "                    carve(nx, ny)\n",
    "\n",
    "        # Start carving from the top-left corner\n",
    "        carve(1, 1)\n",
    "\n",
    "        # Place the goal object in a random position not occupied by a wall\n",
    "        while True:\n",
    "            r = self.rng.integers(1, self.height - 1)\n",
    "            c = self.rng.integers(1, self.width - 1)\n",
    "            if self.grid.get(c, r) is None:\n",
    "                self.put_obj(Goal(), c, r)\n",
    "                break\n",
    "\n",
    "        # Place the agent in a random position not occupied by a wall and not on the goal\n",
    "        while True:\n",
    "            r = self.rng.integers(1, self.height - 1)\n",
    "            c = self.rng.integers(1, self.width - 1)\n",
    "            if self.grid.get(c, r) is None:\n",
    "                self.place_agent(top=(c, r), rand_dir=True)\n",
    "                break\n",
    "    \n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Override reset to ensure we only return the 'image' array\n",
    "        instead of a dict with 'image' and 'mission'.\n",
    "        \"\"\"\n",
    "        obs, info = super().reset(**kwargs)\n",
    "        obs = self._convert_obs(obs)\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Same for step: override to convert the dict observation into an image only.\n",
    "        \"\"\"\n",
    "        obs, reward, done, truncated, info = super().step(action)\n",
    "        obs = self._convert_obs(obs)\n",
    "        return obs, reward, done, truncated, info\n",
    "\n",
    "    def _convert_obs(self, original_obs):\n",
    "        \"\"\"\n",
    "        original_obs is typically {'image':..., 'mission':...}.\n",
    "        We'll just return original_obs['image'] to get a Box(low=0,high=255) shape.\n",
    "        \"\"\"\n",
    "        return original_obs[\"image\"]\n",
    "        #return np.transpose(original_obs[\"image\"], (2, 0, 1))\n",
    "\n",
    "\n",
    "\n",
    "def print_maze_from_config(config):\n",
    "    env = MyCustomMaze(config, render_mode='rgb_array')\n",
    "    env.reset()\n",
    "    full_level_image = env.render()  # This should return an RGB image of the full grid\n",
    "\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(full_level_image)\n",
    "    plt.title(\"Maze Configuration: \" + str(config))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# Generate a random maze and visualize it\n",
    "random_maze = random_config(6)\n",
    "print_maze_from_config(random_maze)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD CODE, NOT VECORIZED, REDUNDANT CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "# ====================================================\n",
    "# 4. Main ACCEL Loop\n",
    "# ====================================================\n",
    "\n",
    "def main_accel_demo(total_iterations, replay_prob, train_steps, level_buffer_size,\n",
    "                    initial_fill_size, grid_size):\n",
    "    \n",
    "    \n",
    "    # Create a level buffer to store generated levels and their scores\n",
    "    level_buffer = LevelBuffer(max_size=level_buffer_size)\n",
    "    iteration_regrets = []\n",
    "        \n",
    "    #Create a dummy environment to initialize the model\n",
    "    dummy_env = MyCustomGrid(random_config(grid_size))\n",
    "    vectorized_env = create_vectorized_env(dummy_env, n_envs=4)\n",
    "\n",
    "    # Initialize student model with PPO\n",
    "    print(\"Initializing student model PPO...\")\n",
    "    student_model = initialize_ppo(dummy_env)\n",
    "\n",
    "    skipped = 0\n",
    "\n",
    "    # Populate buffer with initial levels\n",
    "    print(f\"Populating buffer with {initial_fill_size} initial levels with regret != 0...\")\n",
    "    for _ in range(initial_fill_size + skipped):\n",
    "        cfg = random_config(grid_size)\n",
    "        regret = calculate_regret_gae(MyCustomGrid(cfg), student_model, max_steps=1000, gamma=0.99, lam=0.95)\n",
    "        \n",
    "        # Skip levels with 0 regret\n",
    "        if regret == 0:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        level_buffer.add(cfg, regret)\n",
    "        \n",
    "    \n",
    "    print(\"\\nDone. Number of skipped levels with zero regret:\", skipped)\n",
    "    \n",
    "    skipped = 0\n",
    "    iteration = 0\n",
    "    # Main ACCEL loop\n",
    "    print(\"\\nMain ACCEL loop...\")\n",
    "    while iteration < total_iterations + skipped:\n",
    "        print(f\"\\n=== ITERATION {iteration + 1}/{total_iterations} SKIPPED {skipped} ===\")\n",
    "        \n",
    "        iteration += 1\n",
    "        \n",
    "        # Decide whether to use replay or generate a new level\n",
    "        use_replay = np.random.rand() < replay_prob\n",
    "        \n",
    "        # Generates new random levels if you don't use replay\n",
    "        if not use_replay or len(level_buffer.data) == 0:\n",
    "            cfg = random_config(grid_size)\n",
    "            regret = calculate_regret_gae(MyCustomGrid(cfg), student_model, max_steps=100, gamma=0.99, lam=0.95)\n",
    "            \n",
    "            if regret == 0:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            level_buffer.add(cfg, regret)\n",
    "            print(f\"  Sampled new config, regret={regret:.3f}\")\n",
    "        else:\n",
    "            # Replays an existing layer, edits it, and evaluates the new layer\n",
    "            old_cfg = level_buffer.sample_config()\n",
    "            env = MyCustomGrid(old_cfg)\n",
    "            \n",
    "            student_model.set_env(env)\n",
    "            student_model.learn(total_timesteps=train_steps)\n",
    "\n",
    "            new_cfg = edit_config(old_cfg)\n",
    "            regret = calculate_regret_gae(MyCustomGrid(new_cfg), student_model, max_steps=100, gamma=0.99, lam=0.95)\n",
    "            \n",
    "            if regret == 0:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            level_buffer.add(new_cfg, regret)\n",
    "            print(f\"  Replayed + mutated config, regret={regret:.3f}\")\n",
    "        \n",
    "        iteration_regrets.append(regret)\n",
    "    \n",
    "    print(\"\\nDone. Number of skipped levels with zero regret:\", skipped)\n",
    "\n",
    "    # Visualize progress of the regret over iterations.\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(iteration_regrets, marker='o')\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Regret\")\n",
    "    plt.title(\"Regret Progress during ACCEL Training\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    \n",
    "    config = {\n",
    "        \"grid_size\": 8,\n",
    "        \n",
    "        \"total_iterations\": 64,\n",
    "        \"train_steps\": 1024,\n",
    "\n",
    "        \"replay_prob\": 0.7,           # Probability of replaying a level and editing it vs. generating a new one\n",
    "        \"level_buffer_size\": 128,     # Maximum number of levels to store in the buffer\n",
    "        \"initial_fill_size\": 64,      # Number of levels to pre-fill the buffer with\n",
    "        \"regret_threshold\": 0.0,      # Minimum regret threshold to consider a level for the buffer\n",
    "        \n",
    "        \"n_envs\": 8,                  # Number of parallel environments to use for training\n",
    "        \n",
    "        \"edit_levels\": True,          # Whether to edit levels during training i.e. ACCEL or PLR\n",
    "        \"easy_start\": True            # Whether to fill the buffer with easy levels first i.e. minimum number of blocks\n",
    "    }\n",
    "    \n",
    "    print(\"Running ACCEL with config:\")\n",
    "    print(config, \"\\n\")\n",
    "    \n",
    "    main_accel(**config)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_regret_gae_parallel(env_config, model, max_steps=1000, gamma=0.99, lam=0.95, n_envs=4):\n",
    "    '''\n",
    "    Roll out n_envs copies of MyCustomGrid(env_config) in parallel,\n",
    "    compute GAE-based 'regret' for each environment, and return the max.\n",
    "    '''\n",
    "\n",
    "    # Create vectorized env with n_envs copies\n",
    "    vec_env = create_vectorized_env(env_config, n_envs=n_envs)\n",
    "    obs_array = vec_env.reset()  # shape: (n_envs, height, width, 3)\n",
    "\n",
    "    # For each environment, we will store transitions to later compute GAE\n",
    "    # We'll keep them in lists, one per environment.\n",
    "    # Alternatively, we can store them in big arrays (n_envs, max_steps), etc.\n",
    "    rewards_list = [[] for _ in range(n_envs)]\n",
    "    values_list = [[] for _ in range(n_envs)]\n",
    "    dones_list = [[] for _ in range(n_envs)]\n",
    "\n",
    "    for t in range(max_steps):\n",
    "        # Model’s predict can handle multiple obs in a single forward pass\n",
    "        actions, _ = model.predict(obs_array, deterministic=True)\n",
    "        \n",
    "        # Also compute the values in one batch\n",
    "        # Convert obs_array to torch tensor\n",
    "        obs_tensor = torch.as_tensor(np.transpose(obs_array, (0, 3, 1, 2))).float().to(device)\n",
    "        with torch.no_grad():\n",
    "            # shape: (n_envs, 1)\n",
    "            value_t = model.policy.predict_values(obs_tensor).cpu().numpy().flatten()\n",
    "\n",
    "        # Step all envs in parallel\n",
    "        next_obs_array, rewards, dones, truncs = vec_env.step(actions)\n",
    "\n",
    "        # Store the results\n",
    "        for i in range(n_envs):\n",
    "            rewards_list[i].append(rewards[i])\n",
    "            values_list[i].append(value_t[i])\n",
    "            dones_list[i].append(bool(dones[i]) or bool(truncs[i]))\n",
    "\n",
    "        obs_array = next_obs_array\n",
    "\n",
    "        # If all envs are done or truncated, we can break early\n",
    "        if all(dones) or all(truncs):\n",
    "            break\n",
    "\n",
    "    # We also need the terminal value for each env\n",
    "    # (0 if done, otherwise model's value at final obs)\n",
    "    obs_tensor = torch.as_tensor(np.transpose(obs_array, (0, 3, 1, 2))).float().to(device)\n",
    "    with torch.no_grad():\n",
    "        final_values = model.policy.predict_values(obs_tensor).cpu().numpy().flatten()\n",
    "\n",
    "    # Now, compute GAE-based \"regret\" for each of the n_envs\n",
    "    regrets = []\n",
    "    for i in range(n_envs):\n",
    "        # If the env ended with done or truncated, terminal value = 0\n",
    "        if dones_list[i][-1]:\n",
    "            values_list[i].append(0.0)\n",
    "        else:\n",
    "            values_list[i].append(final_values[i])\n",
    "\n",
    "        # Compute delta_t and approximate GAE-like metric\n",
    "        env_rewards = rewards_list[i]\n",
    "        env_values = values_list[i]\n",
    "        env_dones = dones_list[i]\n",
    "\n",
    "        env_regrets = []\n",
    "        for t in range(len(env_rewards)):\n",
    "            delta_t = env_rewards[t] + gamma * env_values[t + 1] * (1 - env_dones[t]) - env_values[t]\n",
    "            # accumulate discounted error\n",
    "            discounted_error = (gamma * lam) ** t * delta_t\n",
    "            env_regrets.append(max(0, discounted_error))\n",
    "\n",
    "        # The environment's \"regret\" is the max of its positive GAE deltas\n",
    "        regrets.append(max(env_regrets) if env_regrets else 0.0)\n",
    "\n",
    "    # Return the maximum regret across the parallel envs\n",
    "    return max(regrets) if regrets else 0.0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    def _gen_grid(self, width, height):\n",
    "        '''\n",
    "        Generate the grid layout for a new episode.\n",
    "        We use self.width and self.height from config, even though the underlying\n",
    "        MiniGrid environment might use grid_size for some of its operations.\n",
    "        '''    \n",
    "        \n",
    "        # Create an empty grid of the \"true\" width x height from config\n",
    "        self.grid = Grid(self.width, self.height)\n",
    "        # Surround the grid with walls\n",
    "        self.grid.wall_rect(0, 0, self.width, self.height)\n",
    "        \n",
    "        # Place random walls inside using the custom seed. Only place a wall if the cell is empty.\n",
    "        for _ in range(self.num_blocks):\n",
    "            r = self.rng.integers(1, self.height - 1)\n",
    "            c = self.rng.integers(1, self.width - 1)\n",
    "            if self.grid.get(c, r) is None: #and (c, r) != self.config[\"start_pos\"] and (c, r) != self.config[\"goal_pos\"]:\n",
    "                self.put_obj(Wall(), c, r)\n",
    "        \n",
    "        # Place the goal object in a random position not occupied by any wall\n",
    "        \"\"\"if self.config[\"goal_pos\"] is None:\"\"\"\n",
    "        while True:\n",
    "            r = self.rng.integers(1, self.height - 1)\n",
    "            c = self.rng.integers(1, self.width - 1)\n",
    "            if self.grid.get(c, r) is None: # and (c, r) != self.config[\"start_pos\"]:\n",
    "                self.put_obj(Goal(), c, r)\n",
    "                self.config[\"goal_pos\"] = (c, r)\n",
    "                break\n",
    "        \"\"\"elif self.config[\"goal_pos\"] is not None:\n",
    "            c, r = self.config[\"goal_pos\"]\n",
    "            self.put_obj(Goal(), c, r)\"\"\"\n",
    "\n",
    "        # Place the agent in a random position not occupied by any wall and not on the goal\n",
    "        \n",
    "        \"\"\"if self.config[\"start_pos\"] is None:\"\"\"\n",
    "        while True:\n",
    "            r = self.rng.integers(1, self.height - 1)\n",
    "            c = self.rng.integers(1, self.width - 1)\n",
    "            if self.grid.get(c, r) is None: # and (c, r) != self.config[\"goal_pos\"]:\n",
    "                self.place_agent(top=(c, r), rand_dir=True)\n",
    "                self.config[\"start_pos\"] = (c, r)\n",
    "                break  \n",
    "        \"\"\"elif self.config[\"start_pos\"] is not None:\n",
    "            c, r = self.config[\"start_pos\"]\n",
    "            self.place_agent(top=(c, r), rand_dir=True)\"\"\"'''\n",
    "            \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "accel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
