{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACCEL IMPLEMENTATION TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ACCEL with config:\n",
      "{'num_pretrain_levels': 5, 'pretrain_steps': 10, 'total_iterations': 30, 'replay_prob': 0.7, 'train_steps': 20, 'level_buffer_size': 50, 'initial_fill_size': 10}\n",
      "Initializing teacher and student models PPO...\n",
      "Initializing a level buffer of size 50...\n",
      "Pretraining teacher model...\n",
      "Pretraining teacher on config: {'width': 5, 'height': 5, 'seed_val': 273541}\n",
      "Pretraining teacher on config: {'width': 5, 'height': 5, 'seed_val': 500603}\n",
      "Pretraining teacher on config: {'width': 5, 'height': 5, 'seed_val': 579642}\n",
      "Pretraining teacher on config: {'width': 5, 'height': 5, 'seed_val': 321425}\n",
      "Pretraining teacher on config: {'width': 5, 'height': 5, 'seed_val': 743786}\n",
      "Populating buffer with 10 initial levels and regrets...\n",
      "\n",
      "=== ITERATION 1/30 ===\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'num_blocks'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 313\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning ACCEL with config:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28mprint\u001b[39m(config)\n\u001b[1;32m--> 313\u001b[0m \u001b[43mmain_accel_demo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 281\u001b[0m, in \u001b[0;36mmain_accel_demo\u001b[1;34m(total_iterations, replay_prob, train_steps, num_pretrain_levels, level_buffer_size, initial_fill_size, pretrain_steps)\u001b[0m\n\u001b[0;32m    278\u001b[0m student_model\u001b[38;5;241m.\u001b[39mset_env(env)\n\u001b[0;32m    279\u001b[0m student_model\u001b[38;5;241m.\u001b[39mlearn(total_timesteps\u001b[38;5;241m=\u001b[39mtrain_steps)\n\u001b[1;32m--> 281\u001b[0m new_cfg \u001b[38;5;241m=\u001b[39m \u001b[43medit_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mold_cfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    282\u001b[0m regret \u001b[38;5;241m=\u001b[39m calculate_regret(new_cfg, student_model, teacher_model)\n\u001b[0;32m    283\u001b[0m level_buffer\u001b[38;5;241m.\u001b[39madd(new_cfg, regret)\n",
      "Cell \u001b[1;32mIn[1], line 181\u001b[0m, in \u001b[0;36medit_config\u001b[1;34m(old_config)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21medit_config\u001b[39m(old_config):\n\u001b[0;32m    180\u001b[0m     new_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(old_config)\n\u001b[1;32m--> 181\u001b[0m     new_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[43mold_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_blocks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice([\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m]))\n\u001b[0;32m    182\u001b[0m     new_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed_val\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m999999\u001b[39m)\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_config\n",
      "\u001b[1;31mKeyError\u001b[0m: 'num_blocks'"
     ]
    }
   ],
   "source": [
    "# TODO: fix agent position of the agent based on the config_seed in the MyCustomGrid class\n",
    "# TODO: visualize the progress of the regret in the main_accel_demo function\n",
    "# TODO: If the teacher reaches the goal, the simulation should stop\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import wandb\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "import gymnasium.spaces as spaces\n",
    "\n",
    "from minigrid.core.mission import MissionSpace\n",
    "from minigrid.core.world_object import Goal, Wall\n",
    "from minigrid.minigrid_env import MiniGridEnv, Grid\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ====================================================\n",
    "# 1. Custom MiniGrid Environment that returns only the image\n",
    "#    for SB3's PPO (which expects a Box space).\n",
    "# ====================================================\n",
    "class MyCustomGrid(MiniGridEnv):\n",
    "    \"\"\"\n",
    "    Simple MiniGrid environment that places random wall tiles\n",
    "    according to a config dict, returning only the 'image' observation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config=None, **kwargs):\n",
    "        if config is None:\n",
    "            config = {}\n",
    "        self.config = config\n",
    "\n",
    "        # Extract parameters from config\n",
    "        self.width = config.get(\"width\")\n",
    "        self.height = config.get(\"height\")\n",
    "        self.custom_seed = config.get(\"seed_val\")\n",
    "        \n",
    "\n",
    "        grid_size = max(self.width, self.height)\n",
    "\n",
    "        mission_space = MissionSpace(mission_func=lambda: \"get to the green goal square\")\n",
    "\n",
    "        super().__init__(\n",
    "            grid_size=grid_size,\n",
    "            max_steps=self.width * self.height * 2, # max_steps is typically 2x the grid size\n",
    "            see_through_walls=False,\n",
    "            agent_view_size=5,                      # Size of the agent's view square\n",
    "            mission_space=mission_space,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # Manually define our observation_space as a single Box (the image).\n",
    "        # By default, MiniGrid's image shape is (view_size, view_size, 3) if using partial obs,\n",
    "        # or (height, width, 3) if using full-grid observation. We'll do full-grid here:\n",
    "        # We'll define (self.height, self.width, 3) as the shape.\n",
    "        # In practice, \"image\" shape can vary if partial observations are used.\n",
    "        self.observation_space = Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=(self.height, self.width, 3),\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "\n",
    "    def _gen_grid(self, width, height):\n",
    "        \"\"\"\n",
    "        Generate the grid layout for a new episode.\n",
    "        We use self.width and self.height from config, even though the underlying\n",
    "        MiniGrid environment might use grid_size for some of its operations.\n",
    "        \"\"\"    \n",
    "        \n",
    "        # Create an empty grid of the \"true\" width x height from config\n",
    "        self.grid = Grid(self.width, self.height)\n",
    "        # Surround the grid with walls\n",
    "        self.grid.wall_rect(0, 0, self.width, self.height)\n",
    "        \n",
    "        # Seed the random number generator to create the same level each time given the same seed\n",
    "        rng = np.random.default_rng(seed=self.custom_seed)\n",
    "        \n",
    "        # Determine the maximum number of blocks (leaving at least one cell for the agent and one for the goal)\n",
    "        max_blocks = ((self.width - 1) * (self.height - 1)) / 2\n",
    "        self.num_blocks = rng.integers(1, max_blocks)\n",
    "\n",
    "        # Place random walls inside using the custom seed. Only place a wall if the cell is empty.\n",
    "        for _ in range(self.num_blocks):\n",
    "            r = rng.integers(1, self.height - 1)\n",
    "            c = rng.integers(1, self.width - 1)\n",
    "            if self.grid.get(c, r) is None:\n",
    "                self.put_obj(Wall(), c, r)\n",
    "\n",
    "        # Place the agent in a random position not occupied by any wall\n",
    "        while True:\n",
    "            r = rng.integers(1, self.height - 1)\n",
    "            c = rng.integers(1, self.width - 1)\n",
    "            if self.grid.get(c, r) is None:\n",
    "                self.place_agent(top=(c, r), rand_dir=True)\n",
    "                break\n",
    "\n",
    "        # Place the goal object in a random position not occupied by any wall or the agent\n",
    "        while True:\n",
    "            r = rng.integers(1, self.height - 1)\n",
    "            c = rng.integers(1, self.width - 1)\n",
    "            if self.grid.get(c, r) is None:\n",
    "                self.put_obj(Goal(), c, r)\n",
    "                break\n",
    "            \n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Override reset to ensure we only return the 'image' array\n",
    "        instead of a dict with 'image' and 'mission'.\n",
    "        \"\"\"\n",
    "        obs, info = super().reset(**kwargs)\n",
    "        obs = self._convert_obs(obs)\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Same for step: override to convert the dict observation into an image only.\n",
    "        \"\"\"\n",
    "        obs, reward, done, truncated, info = super().step(action)\n",
    "        obs = self._convert_obs(obs)\n",
    "        return obs, reward, done, truncated, info\n",
    "\n",
    "    def _convert_obs(self, original_obs):\n",
    "        \"\"\"\n",
    "        original_obs is typically {'image':..., 'mission':...}.\n",
    "        We'll just return original_obs['image'] to get a Box(low=0,high=255) shape.\n",
    "        \"\"\"\n",
    "        return original_obs[\"image\"]\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# 2. Simple “level buffer” \n",
    "# ====================================================\n",
    "# class to memorize generated levels and score\n",
    "class LevelBuffer: \n",
    "    def __init__(self, max_size=50):\n",
    "        self.max_size = max_size\n",
    "        self.data = []  # will store (config_dict, score)\n",
    "\n",
    "    def add(self, config, score):\n",
    "        self.data.append((config, score))\n",
    "        if len(self.data) > self.max_size:\n",
    "            self.data.sort(key=lambda x: x[1], reverse=True)\n",
    "            self.data = self.data[: self.max_size]\n",
    "            #it memorize only the highest score for each level\n",
    "\n",
    "    def sample_config(self): \n",
    "        # Samples a level from the buffer, weighting the probabilities \n",
    "        # based on the scores.\n",
    "        if len(self.data) == 0:\n",
    "            return None\n",
    "        scores = [item[1] for item in self.data]\n",
    "        total = sum(scores)\n",
    "        if total <= 1e-9:\n",
    "            # fallback to uniform\n",
    "            idx = np.random.randint(len(self.data))\n",
    "            return self.data[idx][0]\n",
    "        probs = [s / total for s in scores]\n",
    "        idx = np.random.choice(len(self.data), p=probs)\n",
    "        return self.data[idx][0]\n",
    "\n",
    "# ====================================================\n",
    "# 3. Utility Functions\n",
    "# ====================================================\n",
    "def random_config():\n",
    "    return {\n",
    "        \"width\": 5,\n",
    "        \"height\": 5,\n",
    "        \"seed_val\": np.random.randint(0, 999999),\n",
    "        # \"num_blocks\": 3,  # to be set in the MyCustomGrid class depending on the seed_val\n",
    "        # \"agent_start\": (x, y) to be set in the MyCustomGrid class depending on the seed_val\n",
    "    }\n",
    "    \n",
    "# Modify an existing configuration, adding randomness.\n",
    "def edit_config(old_config):\n",
    "    new_config = dict(old_config)\n",
    "    #new_config[\"num_blocks\"] = max(0, old_config[\"num_blocks\"] + np.random.choice([-2, -1, 1, 2]))\n",
    "    offset = np.random.choice([-10, -5, 5, 10])\n",
    "    new_config[\"seed_val\"] = (old_config[\"seed_val\"] + offset) % 1000000\n",
    "    return new_config\n",
    "\n",
    "def calculate_regret(config, student_model,teacher_model, max_steps=50):\n",
    "    \"\"\"\n",
    "    Calculate regret as the difference between the teacher's performance\n",
    "    and the student's performance on the same level.\n",
    "    \"\"\"\n",
    "    env = MyCustomGrid(config)\n",
    "    #Teacher rollout \n",
    "    obs, _ = env.reset()\n",
    "    teacher_total_reward = 0\n",
    "    for _ in range(max_steps):\n",
    "        action, _ = teacher_model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, truncated, _ = env.step(action)\n",
    "        teacher_total_reward += reward\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "    #Student rollout\n",
    "    obs, _ = env.reset()\n",
    "    student_total_reward = 0\n",
    "    for _ in range(max_steps):\n",
    "        action, _ = student_model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, truncated, _ = env.step(action)\n",
    "        student_total_reward += reward\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "    return max(0, teacher_total_reward - student_total_reward)\n",
    "\n",
    "def initialize_ppo(env, learning_rate=1e-4):\n",
    "    return PPO(\n",
    "        \"MlpPolicy\",                    # Multi-layer perceptron policy\n",
    "        env,                            # environment to learn from\n",
    "        verbose=0,                      # Display training output\n",
    "        n_steps=256,                    # Number of steps to run for each environment per update\n",
    "        batch_size=64,                  # Minibatch size for each gradient update\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# 4. Main ACCEL Loop\n",
    "# ====================================================\n",
    "\n",
    "def main_accel_demo(total_iterations, replay_prob, train_steps, num_pretrain_levels, level_buffer_size, initial_fill_size, pretrain_times):\n",
    "    #Create a dummy environment to initialize the model\n",
    "    dummy_env = MyCustomGrid(random_config(), render_mode=\"human\")\n",
    "    #dummy_env.reset()\n",
    "    \n",
    "    # Render the environment\n",
    "    # dummy_env.render()\n",
    "    \n",
    "    # Initialize teacher and student models with PPO\n",
    "    print(\"Initializing teacher and student models PPO...\")\n",
    "    teacher_model = initialize_ppo(dummy_env)\n",
    "    student_model = initialize_ppo(dummy_env)\n",
    "    \n",
    "    \n",
    "    # creates a layer buffer.\n",
    "    print(\"Initializing a level buffer of size 50...\")\n",
    "    level_buffer = LevelBuffer(max_size=level_buffer_size)\n",
    "    iteration_regrets = []\n",
    "    \n",
    "    # Pretrain teacher on a set of random levels\n",
    "    print(\"Pretraining teacher model...\")\n",
    "    for i in range(num_pretrain_levels):\n",
    "        # Create a random config and environment from it\n",
    "        cfg = random_config()\n",
    "        env = MyCustomGrid(cfg, render_mode=\"human\")\n",
    "        \n",
    "        print(f\"Pretraining teacher on config: {cfg} for {pretrain_times} times\")\n",
    "        teacher_model.set_env(env)\n",
    "        teacher_model.learn(total_timesteps=pretrain_times) # Number of times the model is trained in the environment\n",
    "\n",
    "    # Populate buffer with initial levels\n",
    "    print(f\"Populating buffer with {initial_fill_size} initial levels and regrets...\")\n",
    "    for _ in range(initial_fill_size):\n",
    "        cfg = random_config()\n",
    "        regret = calculate_regret(cfg, student_model, teacher_model)\n",
    "        level_buffer.add(cfg, regret)\n",
    "\n",
    "    for iteration in range(total_iterations):\n",
    "        print(f\"\\n=== ITERATION {iteration + 1}/{total_iterations} ===\")\n",
    "        use_replay = np.random.rand() < replay_prob\n",
    "        # Generates new random levels if you don't use replay\n",
    "        if not use_replay or len(level_buffer.data) == 0:\n",
    "            cfg = random_config()\n",
    "            regret = calculate_regret(cfg, student_model, teacher_model)\n",
    "            level_buffer.add(cfg, regret)\n",
    "            print(f\"  Sampled new config, regret={regret:.3f}\")\n",
    "        else:\n",
    "            # Replays an existing layer, edits it, and evaluates the new layer\n",
    "            old_cfg = level_buffer.sample_config()\n",
    "            env = MyCustomGrid(old_cfg)\n",
    "            student_model.set_env(env)\n",
    "            student_model.learn(total_timesteps=train_steps)\n",
    "\n",
    "            new_cfg = edit_config(old_cfg)\n",
    "            regret = calculate_regret(new_cfg, student_model, teacher_model)\n",
    "            level_buffer.add(new_cfg, regret)\n",
    "            print(f\"  Replayed + mutated config, regret={regret:.3f}\")\n",
    "        \n",
    "        iteration_regrets.append(regret)\n",
    "\n",
    "    # TODO: Visualize progress \n",
    "    #plot_progress(total_iterations, iteration_regrets)\n",
    "    \n",
    "    print(\"\\nDone. Final buffer size:\", len(level_buffer.data))\n",
    "    print(\"Top-5 hardest levels (config, regret):\")\n",
    "    level_buffer.data.sort(key=lambda x: x[1], reverse=True)\n",
    "    for i, (cfg, sc) in enumerate(level_buffer.data[:5]):\n",
    "        print(f\"{i + 1}. regret={sc:.3f}, config={cfg}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    config = {\n",
    "        \"num_pretrain_levels\": 5,\n",
    "        \"pretrain_steps\": 10,\n",
    "        \n",
    "        \"total_iterations\": 30,\n",
    "        \"replay_prob\": 0.7,\n",
    "        \"train_steps\": 20,\n",
    "        \"level_buffer_size\": 50,\n",
    "        \"initial_fill_size\": 10\n",
    "    }\n",
    "    \n",
    "    print(\"Running ACCEL with config:\")\n",
    "    print(config)\n",
    "    \n",
    "    main_accel_demo(**config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "accel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
