{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACCEL IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n"
     ]
    }
   ],
   "source": [
    "print(\"Start\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import wandb\n",
    "import imageio\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "from gymnasium.spaces import Box\n",
    "\n",
    "from minigrid.core.mission import MissionSpace\n",
    "from minigrid.core.world_object import Goal, Wall\n",
    "from minigrid.minigrid_env import MiniGridEnv, Grid\n",
    "from minigrid.wrappers import ImgObsWrapper\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "\n",
    "\n",
    "device = 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ====================================================\n",
    "# 1. Custom MiniGrid Environment that returns only the image\n",
    "#    for SB3's PPO (which expects a Box space).\n",
    "# ====================================================\n",
    "class MyCustomGrid(MiniGridEnv):\n",
    "    \"\"\"\n",
    "    Simple MiniGrid environment that places random wall tiles\n",
    "    according to a config dict, returning only the 'image' observation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config=None, solvable_only=False, **kwargs):\n",
    "        if config is None:\n",
    "            config = {}\n",
    "        self.config = config\n",
    "        self.solvable_only = solvable_only\n",
    "\n",
    "        # Create a random number generator with the custom seed\n",
    "        self.rng = np.random.default_rng(seed=self.config.get(\"seed_val\"))\n",
    "\n",
    "        mission_space = MissionSpace(mission_func=lambda: \"get to the green goal square\")\n",
    "\n",
    "        super().__init__(\n",
    "            grid_size=self.config['width'],\n",
    "            max_steps=self.config['width'] * self.config['height'] * 2, # max_steps is typically 2x the grid size\n",
    "            see_through_walls=False,\n",
    "            agent_view_size=5,                      # Size of the agent's view square\n",
    "            mission_space=mission_space,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # Manually define our observation_space as a single Box (the image).\n",
    "        # By default, MiniGrid's image shape is (view_size, view_size, 3) if using partial obs,\n",
    "        # or (height, width, 3) if using full-grid observation. We'll do full-grid here:\n",
    "        # We'll define (self.height, self.width, 3) as the shape.\n",
    "        # In practice, \"image\" shape can vary if partial observations are used.\n",
    "        self.observation_space = Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=(self.agent_view_size, self.agent_view_size, 3),\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "\n",
    "            \n",
    "    def _gen_grid(self, width, height):\n",
    "        \"\"\"\n",
    "        Generate a new environment layout ensuring solvability if required.\n",
    "        \"\"\"\n",
    "        \n",
    "        check_stuck = 0\n",
    "        while True:  # Keep regenerating until a solvable layout is found\n",
    "            self.grid = Grid(width, height)\n",
    "            self.grid.wall_rect(0, 0, width, height)\n",
    "\n",
    "            # Place the goal\n",
    "            goal_pos = self.config.get(\"goal_pos\")\n",
    "            if goal_pos is None:\n",
    "                while True:\n",
    "                    goal_r = self.rng.integers(1, height - 1)\n",
    "                    goal_c = self.rng.integers(1, width - 1)\n",
    "                    if self.grid.get(goal_c, goal_r) is None:\n",
    "                        self.put_obj(Goal(), goal_c, goal_r)\n",
    "                        self.config[\"goal_pos\"] = (goal_c, goal_r)\n",
    "                        break\n",
    "            else:\n",
    "                self.put_obj(Goal(), goal_pos[0], goal_pos[1])\n",
    "\n",
    "            # Place the agent\n",
    "            start_pos = self.config.get(\"start_pos\")\n",
    "            if start_pos is None:\n",
    "                while True:\n",
    "                    start_r = self.rng.integers(1, height - 1)\n",
    "                    start_c = self.rng.integers(1, width - 1)\n",
    "                    if self.grid.get(start_c, start_r) is None and (start_c, start_r) != self.config[\"goal_pos\"]:\n",
    "                        self.agent_pos = (start_c, start_r)\n",
    "                        self.agent_dir = self.rng.integers(0, 4)\n",
    "                        self.config[\"start_pos\"] = (start_c, start_r)\n",
    "                        break\n",
    "            else:\n",
    "                self.agent_pos = start_pos\n",
    "                self.agent_dir = self.rng.integers(0, 4)\n",
    "                self.config[\"start_pos\"] = start_pos\n",
    "            \n",
    "            placed_blocks = 0\n",
    "            \n",
    "            # Maximum number of tries to place the blocks\n",
    "            max_num_tries = 100\n",
    "            \n",
    "            # Place random walls using config parameters\n",
    "            while placed_blocks < self.config[\"num_blocks\"]:\n",
    "                max_num_tries -= 1\n",
    "                r = self.rng.integers(1, height - 1)\n",
    "                c = self.rng.integers(1, width - 1)\n",
    "                if max_num_tries <= 0:\n",
    "                    print(\"Could not place all blocks in the grid.\")\n",
    "                    break\n",
    "                if self.grid.get(c, r) is None and (c, r) != self.config[\"start_pos\"] and (c, r) != self.config[\"goal_pos\"]:\n",
    "                    self.put_obj(Wall(), c, r)\n",
    "                    placed_blocks += 1\n",
    "\n",
    "            # Check solvability if required\n",
    "            if not self.solvable_only or self._is_solvable():\n",
    "                break\n",
    "            \n",
    "            check_stuck += 1\n",
    "            if check_stuck > 50:\n",
    "                print(\"Re-randomizing start and goal positions...\")\n",
    "                self.config.pop(\"start_pos\", None)\n",
    "                self.config.pop(\"goal_pos\", None)\n",
    "                self.rng = np.random.default_rng(seed=self.config.get(\"seed_val\") + check_stuck)\n",
    "\n",
    "        \n",
    "    def _is_solvable(self):\n",
    "        \"\"\"\n",
    "        Uses Breadth-First Search (BFS) to check if there's a path \n",
    "        from the agent's start position to the goal.\n",
    "        \"\"\"\n",
    "        start_pos = self.config[\"start_pos\"]\n",
    "        goal_pos = self.config[\"goal_pos\"]\n",
    "        if not start_pos or not goal_pos:\n",
    "            return False\n",
    "\n",
    "        queue = deque([start_pos])\n",
    "        visited = set()\n",
    "        visited.add(start_pos)\n",
    "\n",
    "        while queue:\n",
    "            x, y = queue.popleft()\n",
    "            if (x, y) == goal_pos:\n",
    "                return True\n",
    "\n",
    "            # Possible moves: up, down, left, right\n",
    "            for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n",
    "                nx, ny = x + dx, y + dy\n",
    "                cell_obj = self.grid.get(nx, ny)\n",
    "                if (\n",
    "                    1 <= nx < self.width - 1 and  # Stay within grid bounds\n",
    "                    1 <= ny < self.height - 1 and\n",
    "                    (nx, ny) not in visited and\n",
    "                    self.grid.get(nx, ny) is None or isinstance(cell_obj, Goal)\n",
    "                ):\n",
    "                    queue.append((nx, ny))\n",
    "                    visited.add((nx, ny))\n",
    "        return False  # No path found\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Override reset to ensure we only return the 'image' array\n",
    "        instead of a dict with 'image' and 'mission'.\n",
    "        \"\"\"\n",
    "        obs, info = super().reset(**kwargs)\n",
    "        obs = self._convert_obs(obs)\n",
    "        \n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Same for step: override to convert the dict observation into an image only.\n",
    "        \"\"\"\n",
    "        obs, reward, done, truncated, info = super().step(action)\n",
    "        obs = self._convert_obs(obs)\n",
    "        return obs, reward, done, truncated, info\n",
    "\n",
    "    def _convert_obs(self, original_obs):\n",
    "        \"\"\"\n",
    "        original_obs is typically {'image':..., 'mission':...}.\n",
    "        We'll just return original_obs['image'] to get a Box(low=0,high=255) shape.\n",
    "        \"\"\"\n",
    "        return original_obs[\"image\"]\n",
    "        #return np.transpose(original_obs[\"image\"], (2, 0, 1))\n",
    "    \n",
    "    def update_config(self, new_config):\n",
    "        self.config = new_config\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "\n",
    "def random_config(grid_size, num_blocks=None, seed=None):\n",
    "    max_blocks = int(((grid_size - 1) * (grid_size - 1)) / 2)\n",
    "    \n",
    "    if num_blocks is None:\n",
    "        num_blocks = np.random.randint(1, max_blocks)\n",
    "    else:\n",
    "        num_blocks = min(num_blocks, max_blocks)\n",
    "        \n",
    "    config = {\n",
    "        \"width\": grid_size,\n",
    "        \"height\": grid_size,\n",
    "        \"num_blocks\": num_blocks,\n",
    "        \"start_pos\": None,\n",
    "        \"goal_pos\": None,\n",
    "        \"edited\": False,\n",
    "        \"seed_val\": seed if seed is not None else np.random.randint(0, 1000)\n",
    "    }\n",
    "    \n",
    "    # Set the start and goal positions\n",
    "    env = MyCustomGrid(config)\n",
    "    \n",
    "    # Reset the environment to get the start and goal positions\n",
    "    env.reset()\n",
    "    \n",
    "    # Get the new config from the environment\n",
    "    config = env.config\n",
    "        \n",
    "    return config\n",
    "\n",
    "def print_level_from_config(config, solvable_only=False):\n",
    "    #print(\"Putting up the level from config:\", config)\n",
    "    env = MyCustomGrid(config, render_mode='rgb_array', solvable_only=solvable_only)\n",
    "    env.reset()\n",
    "    full_level_image = env.render()  # This should return an RGB image of the full grid\n",
    "\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(full_level_image)\n",
    "    plt.title(\"Level Configuration: \" + str(config))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "\"\"\"# Modify an existing configuration, adding randomness.\n",
    "def edit_config(old_config):\n",
    "    max_blocks = int(((old_config[\"width\"] - 1) * (old_config[\"height\"] - 1)) / 2)\n",
    "    \n",
    "    new_config = dict(old_config)\n",
    "    \n",
    "    # Randomly change the number of blocks\n",
    "    new_number_blocks = old_config[\"num_blocks\"] + np.random.choice([-1, 1, 2, 3])\n",
    "    \n",
    "    # Ensure the number of blocks is within bounds\n",
    "    new_config[\"num_blocks\"] = max(1, min(new_number_blocks, max_blocks))    \n",
    "    \n",
    "    # Mark the config as edited\n",
    "    new_config[\"edited\"] = True\n",
    "    \n",
    "    return new_config\"\"\"\n",
    "    \n",
    "\n",
    "def edit_config(old_config, difficulty_level=1):\n",
    "\n",
    "    width, height = old_config[\"width\"], old_config[\"height\"]\n",
    "    total_cells = width * height\n",
    "\n",
    "    # Define a baseline max number of blocks\n",
    "    max_blocks = int(0.6 * total_cells)  # Ensure we don't overcrowd (max 60% coverage)\n",
    "    \n",
    "    # Calculate the new number of blocks using a logarithmic scale\n",
    "    base_growth = int(np.log2(total_cells) * difficulty_level)\n",
    "    \n",
    "    # Introduce some randomness while keeping it within a reasonable range\n",
    "    growth_factor = np.random.randint(base_growth // 2, base_growth + 1)\n",
    "    \n",
    "    # Compute the new block count\n",
    "    new_number_blocks = old_config[\"num_blocks\"] + growth_factor\n",
    "    \n",
    "    # Ensure it's within the allowed range\n",
    "    new_config = dict(old_config)\n",
    "    new_config[\"num_blocks\"] = max(1, min(new_number_blocks, max_blocks))  \n",
    "    \n",
    "    # Mark as edited\n",
    "    new_config[\"edited\"] = True\n",
    "\n",
    "    return new_config\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# 2. Simple “level buffer” \n",
    "# ====================================================\n",
    "# class to memorize generated levels and score\n",
    "class LevelBuffer: \n",
    "    def __init__(self, max_size=50):\n",
    "        self.max_size = max_size\n",
    "        self.data = []  # will store (config_dict, score)\n",
    "\n",
    "    def add(self, config, score):\n",
    "        self.data.append((config, score))\n",
    "        if len(self.data) > self.max_size:\n",
    "            self.data.sort(key=lambda x: x[1], reverse=True)\n",
    "            self.data = self.data[: self.max_size]\n",
    "            #it memorize only the highest score for each level\n",
    "\n",
    "    def sample_config(self): \n",
    "        # Samples a level from the buffer, weighting the probabilities \n",
    "        # based on the scores.\n",
    "        if len(self.data) == 0:\n",
    "            return None\n",
    "        scores = [item[1] for item in self.data]\n",
    "        total = sum(scores)\n",
    "        if total <= 1e-9:\n",
    "            # fallback to uniform\n",
    "            idx = np.random.randint(len(self.data))\n",
    "            return self.data[idx][0]\n",
    "        probs = [s / total for s in scores]\n",
    "        idx = np.random.choice(len(self.data), p=probs)\n",
    "        return self.data[idx][0]\n",
    "\n",
    "# ====================================================\n",
    "# 3. Utility Functions\n",
    "# ====================================================\n",
    "\n",
    "# Calculate regret using Generalized Advantage Estimation (GAE) with Stable-Baselines3's PPO model.\n",
    "# PLR approximates regret using a score function such as the positive value loss.\n",
    "def calculate_regret_gae(env, model, max_steps, gamma, lam):\n",
    "    \"\"\"\n",
    "    Calculate regret using Generalized Advantage Estimation (GAE)\n",
    "    with Stable-Baselines3's PPO model.\n",
    "    \"\"\"\n",
    "    obs, _ = env.reset()\n",
    "    regrets = []\n",
    "    rewards = []\n",
    "    dones = []\n",
    "    values = []\n",
    "\n",
    "    for t in range(max_steps):\n",
    "        # Add batch dimension to the observation tensor\n",
    "        obs_tensor = torch.as_tensor(obs).float().unsqueeze(0).to(device)\n",
    "        \n",
    "        # Use the model's policy to get the value and action.\n",
    "        # For actions, model.predict handles single observations well.\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        \n",
    "        # Compute the value from the policy.\n",
    "        value_t = model.policy.predict_values(obs_tensor).item()\n",
    "        values.append(value_t)\n",
    "        \n",
    "        # Perform the step in the environment\n",
    "        obs, reward, done, truncated, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "    # Add value of the terminal state (0 if done/truncated)\n",
    "    if done or truncated:\n",
    "        terminal_value = 0.0\n",
    "    else:\n",
    "        terminal_obs_tensor = torch.as_tensor(obs).float().unsqueeze(0).to(device)\n",
    "        terminal_value = model.policy.predict_values(terminal_obs_tensor).item()\n",
    "    values.append(terminal_value)\n",
    "\n",
    "    # Compute TD-errors and GAE-like regret score\n",
    "    for t in range(len(rewards)):\n",
    "        delta_t = rewards[t] + gamma * values[t + 1] * (1 - dones[t]) - values[t]\n",
    "        discounted_error = (gamma * lam) ** t * delta_t\n",
    "        regrets.append(max(0, discounted_error))\n",
    "\n",
    "    # Return the maximum positive regret score (or 0 if empty)\n",
    "    return max(regrets) if regrets else 0.0\n",
    "\n",
    "\n",
    "def initialize_ppo(env, learning_rate=1e-4):\n",
    "    return PPO(\n",
    "        \"MlpPolicy\",                    # Multi-layer perceptron policy\n",
    "        env,                            # environment to learn from\n",
    "        verbose=0,                      # Display training output\n",
    "        n_steps=256,                    # Number of steps to run for each environment per update\n",
    "        batch_size=64,                  # Minibatch size for each gradient update\n",
    "        learning_rate=learning_rate,    # Learning rate for optimizer\n",
    "        device=device                   # Use GPU if available\n",
    "    )\n",
    "    \n",
    "# Use vectorized environment\n",
    "def create_vectorized_env(config, n_envs=4, solvable_only=False):\n",
    "    \"\"\"\n",
    "    Create a vectorized environment with n parallel environments.\n",
    "    \"\"\"\n",
    "    return make_vec_env(lambda: MyCustomGrid(config, solvable_only), n_envs=n_envs, vec_env_cls=SubprocVecEnv)\n",
    "\n",
    "\n",
    "\n",
    "def evalute_models(load_dim = -1, grid_size = 6, n_eval_episodes = 5, num_levels_per_difficulty = 10):\n",
    "    \n",
    "    if load_dim > 0:\n",
    "        # Load the models\n",
    "        model_dr = PPO.load(f\"models/dr_model_{load_dim}x{load_dim}\")\n",
    "        model_plr = PPO.load(f\"models/plr_model_{load_dim}x{load_dim}\")\n",
    "        model_accel = PPO.load(f\"models/accel_model_{load_dim}x{load_dim}\")\n",
    "        model_accel_easy = PPO.load(f\"models/accel_model_easy_{load_dim}x{load_dim}\")\n",
    "\n",
    "    # Inseert the models in a dictionary\n",
    "    models = {\"DR\": model_dr, 'PLR': model_plr, 'ACCEL': model_accel, 'ACCEL-EasyStart': model_accel_easy}\n",
    "\n",
    "    # Generate n levels difficulties with increasing complexity, for each level generate m configs\n",
    "    difficulties = 3\n",
    "    num_levels_per_difficulty = num_levels_per_difficulty\n",
    "\n",
    "    levels = []\n",
    "    for i in range(difficulties):\n",
    "        level = []\n",
    "        for _ in range(num_levels_per_difficulty):\n",
    "            cfg = random_config(grid_size, num_blocks=grid_size*(i+1))\n",
    "            #print_level_from_config(cfg, solvable_only=True)\n",
    "            level.append(cfg)\n",
    "        levels.append(level)\n",
    "        \n",
    "    \n",
    "    # Create a dummy config to initialize the vectorized environment\n",
    "    dummy_config = random_config(grid_size)\n",
    "    env = create_vectorized_env(dummy_config, n_envs=4, solvable_only=True)\n",
    "\n",
    "    # Evaluate the model on the generated levels\n",
    "    results = {}\n",
    "    for model_name, model in models.items():\n",
    "        results[model_name] = []\n",
    "        for i, level in enumerate(levels):\n",
    "            print(f\"Evaluating {num_levels_per_difficulty} levels of difficulty {i + 1} with {grid_size*(i+1)} blocks on a {grid_size}x{grid_size} grid for model {model_name}, ratio of blocks to grid size: {grid_size*(i+1) / (grid_size*grid_size):.2f}\")\n",
    "            r = []\n",
    "            for j, cfg in enumerate(level):\n",
    "                # Update the environment with the new config\n",
    "                env.env_method(\"update_config\", cfg)\n",
    "                mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=n_eval_episodes, deterministic=True)\n",
    "                r.append(mean_reward)\n",
    "            results[model_name].append(r)\n",
    "        print()\n",
    "        \n",
    "    # Print mean rewards for each level\n",
    "    for model_name in models.keys():\n",
    "        print(f\"Model: {model_name}\")\n",
    "        for i, level in enumerate(levels):\n",
    "            print(f\"Level {i + 1} - Complexity {grid_size*(i+1)}: {np.mean(results[model_name][i]):.2f}\")\n",
    "        print()\n",
    "    \n",
    "    #Comute the number of xticks based on the number of models\n",
    "    xticks = [i for i in range(1, len(models.keys()) + 1)]\n",
    "\n",
    "    # Boxplot of results, a plot for each level complexity comparing models\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for i, level in enumerate(levels):\n",
    "        plt.subplot(1, difficulties, i + 1)\n",
    "        plt.boxplot([results[model_name][i] for model_name in models.keys()])\n",
    "        plt.xticks([1,2,3,4], [model_name for model_name in models.keys()])\n",
    "        plt.title(f\"Level {i + 1} - Complexity {grid_size*(i+1)}\")\n",
    "        plt.ylabel(\"Mean Reward\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig(f\"plots/boxplot_{load_dim}x{load_dim}.png\")\n",
    "    \n",
    "    \n",
    "def main_accel(total_iterations, replay_prob, train_steps, level_buffer_size,\n",
    "               initial_fill_size, grid_size, n_envs, edit_levels, regret_threshold,\n",
    "               easy_start, domain_randomization, name):\n",
    "    \n",
    "    # Initialize Weights and Biases\n",
    "    wandb.init(project=\"accel\", config=config)\n",
    "    \n",
    "    # Create a level buffer, a personal class to store levels and scores\n",
    "    level_buffer = LevelBuffer(max_size=level_buffer_size)\n",
    "    \n",
    "    # Generate a random configuration {width, height, num_blocks, start_pos, goal_pos}\n",
    "    dummy_config = random_config(grid_size)\n",
    "    \n",
    "    # Create a vectorized environment, so a wrapper for MyCustomGrid that allows interconnection \n",
    "    # between gymnasium and stable-baselines3 to train the model in a vectorized way, since we\n",
    "    # are using DummyVecEnv, it is not true parallelism\n",
    "    vectorized_env = create_vectorized_env(dummy_config, n_envs=n_envs)\n",
    "\n",
    "    # Initialize PPO with vectorized environment\n",
    "    print(\"Initializing student model PPO...\")\n",
    "    student_model = initialize_ppo(vectorized_env)\n",
    "            \n",
    "        \n",
    "\n",
    "    # ====================================================\n",
    "    # Initial buffer fill\n",
    "    # ====================================================\n",
    "    if not domain_randomization:\n",
    "        print(f\"Populating buffer with {initial_fill_size} initial levels with regret > {regret_threshold}...\")\n",
    "        while len(level_buffer.data) < initial_fill_size:\n",
    "            \n",
    "            if easy_start:\n",
    "                cfg = random_config(grid_size, num_blocks=2)\n",
    "            else:\n",
    "                cfg = random_config(grid_size)\n",
    "            \n",
    "            #for monitor in vectorized_env.envs:\n",
    "            #    monitor.env.update_config(cfg)\n",
    "            \n",
    "            vectorized_env.env_method(\"update_config\", cfg)\n",
    "            \n",
    "            student_model.learn(total_timesteps=100)\n",
    "            \n",
    "            regret = calculate_regret_gae(MyCustomGrid(cfg), student_model, max_steps=1000, gamma=0.99, lam=0.95)\n",
    "\n",
    "            # Skip levels with low regret\n",
    "            if regret < regret_threshold: continue\n",
    "\n",
    "            level_buffer.add(cfg, regret)\n",
    "\n",
    "    # ====================================================\n",
    "    # Main ACCEL loop\n",
    "    # ====================================================\n",
    "    \n",
    "    iteration_regrets = []\n",
    "    iteration, skipped = 0, 0\n",
    "    \n",
    "    print(\"\\nMain training loop...\")\n",
    "    while iteration < total_iterations + skipped:\n",
    "        \n",
    "        print(f\"\\n=== ITERATION {iteration + 1}/{total_iterations + skipped} SKIPPED: {skipped} ===\")\n",
    "        \n",
    "        iteration += 1\n",
    "        \n",
    "        if domain_randomization:\n",
    "            cfg = random_config(grid_size)\n",
    "            vectorized_env.env_method(\"update_config\", cfg)\n",
    "            student_model.learn(total_timesteps=train_steps)\n",
    "            regret = calculate_regret_gae(MyCustomGrid(cfg), student_model, max_steps=1000, gamma=0.99, lam=0.95)\n",
    "            print(f\"Regret for current level: {regret}, buffer size: {len(level_buffer.data)}\")\n",
    "            iteration_regrets.append(regret)\n",
    "            \n",
    "            # if regret is below threshold, skip\n",
    "            if regret <= regret_threshold:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        # Decide whether to replay or generate a new level\n",
    "        use_replay = np.random.rand() < replay_prob\n",
    "\n",
    "        if not use_replay or len(level_buffer.data) == 0:\n",
    "            # Create a new random level\n",
    "            cfg = random_config(grid_size)\n",
    "            print(\"Generated new random level:\", cfg)\n",
    "        else:\n",
    "            # Sample a level from the buffer\n",
    "            cfg = level_buffer.sample_config()\n",
    "            print(\"Sampled level from buffer:\", cfg)\n",
    "            \n",
    "        # Update the vectorized environment with the selected config and train the model\n",
    "        #for monitor in vectorized_env.envs:\n",
    "        #    monitor.env.update_config(cfg)\n",
    "        \n",
    "        vectorized_env.env_method(\"update_config\", cfg)\n",
    "        \n",
    "        student_model.learn(total_timesteps=train_steps)\n",
    "        \n",
    "        wandb.log({\n",
    "            \"iteration\": iteration,\n",
    "            \"regret_score\": regret,\n",
    "            \"regret_threshold\": regret_threshold,\n",
    "            \"buffer_size\": len(level_buffer.data),\n",
    "            \"value_loss\": student_model.logger.name_to_value[\"train/value_loss\"],\n",
    "            \"entropy_loss\": student_model.logger.name_to_value[\"train/entropy_loss\"],\n",
    "            \"policy_loss\": student_model.logger.name_to_value[\"train/policy_loss\"],\n",
    "        })\n",
    "\n",
    "        if use_replay and edit_levels:\n",
    "            # Edit the level and calculate regret\n",
    "            cfg = edit_config(cfg)\n",
    "            print(\"Edited level to:\", cfg)\n",
    "\n",
    "        regret = calculate_regret_gae(MyCustomGrid(cfg), student_model, max_steps=1000, gamma=0.99, lam=0.95)\n",
    "        \n",
    "        if regret <= regret_threshold:\n",
    "            print(f\"Regret for current level is {regret:.5f} <= threshold {regret_threshold:.5f}. Skipping...\")\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        print(f\"Regret for current level: {regret}, buffer size: {len(level_buffer.data)}\")\n",
    "        level_buffer.add(cfg, regret)\n",
    "        iteration_regrets.append(regret)\n",
    "        \n",
    "        # Increase the regret threshold slightly\n",
    "        regret_threshold += 0.0001\n",
    "        \n",
    "    \n",
    "    # Plot and display the progress\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(iteration_regrets, marker='o')\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Regret\")\n",
    "    plt.title(\"Regret Progress during Training\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig(f\"regret_progress_{name}_{grid_size}x{grid_size}.png\")\n",
    "    \n",
    "    \n",
    "    print(\"\\nDone. Final buffer size:\", len(level_buffer.data))\n",
    "    print(\"Top-5 hardest levels (config, regret):\")\n",
    "    level_buffer.data.sort(key=lambda x: x[1], reverse=True)\n",
    "    for i, (cfg, sc) in enumerate(level_buffer.data[:5]):\n",
    "        print(f\"{i + 1}. regret={sc:.5f}, config={cfg}\")\n",
    "        #print_level_from_config(cfg)\n",
    "        \n",
    "    print(\"Top-5 easiest levels (config, regret):\")\n",
    "    level_buffer.data.sort(key=lambda x: x[1])\n",
    "    for i, (cfg, sc) in enumerate(level_buffer.data[:5]):\n",
    "        print(f\"{i + 1}. regret={sc:.5f}, config={cfg}\")\n",
    "        #print_level_from_config(cfg)\n",
    "    \n",
    "    wandb.finish()\n",
    "    \n",
    "    # Save the model\n",
    "    student_model.save(f\"models/{name}\")\n",
    "\n",
    "    return student_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Domain Randomization with config: {'name': 'dr_model_10x10', 'grid_size': 10, 'total_iterations': 512, 'train_steps': 1024, 'replay_prob': 0.7, 'level_buffer_size': 128, 'initial_fill_size': 64, 'regret_threshold': 0.0, 'n_envs': 2, 'edit_levels': False, 'easy_start': False, 'domain_randomization': True}\n",
      "Initializing student model PPO...\n",
      "\n",
      "Main training loop...\n",
      "\n",
      "=== ITERATION 1/512 SKIPPED: 0 ===\n",
      "Regret for current level: 0.00018112236633896842, buffer size: 0\n",
      "\n",
      "=== ITERATION 2/512 SKIPPED: 0 ===\n",
      "Regret for current level: 1.5519282314926507e-05, buffer size: 0\n",
      "\n",
      "=== ITERATION 3/512 SKIPPED: 0 ===\n",
      "Regret for current level: 0.00010214682668447522, buffer size: 0\n",
      "\n",
      "=== ITERATION 4/512 SKIPPED: 0 ===\n",
      "Regret for current level: 0, buffer size: 0\n",
      "\n",
      "=== ITERATION 5/513 SKIPPED: 1 ===\n",
      "Regret for current level: 0, buffer size: 0\n",
      "\n",
      "=== ITERATION 6/514 SKIPPED: 2 ===\n",
      "Regret for current level: 0, buffer size: 0\n",
      "\n",
      "=== ITERATION 7/515 SKIPPED: 3 ===\n",
      "Regret for current level: 0.015283386600523158, buffer size: 0\n",
      "\n",
      "=== ITERATION 8/515 SKIPPED: 3 ===\n",
      "Regret for current level: 0.024316527396440496, buffer size: 0\n",
      "\n",
      "=== ITERATION 9/515 SKIPPED: 3 ===\n",
      "Regret for current level: 0, buffer size: 0\n",
      "\n",
      "=== ITERATION 10/516 SKIPPED: 4 ===\n",
      "Regret for current level: 0, buffer size: 0\n",
      "\n",
      "=== ITERATION 11/517 SKIPPED: 5 ===\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124measy_start\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning Domain Randomization with config: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 31\u001b[0m model_dr \u001b[38;5;241m=\u001b[39m \u001b[43mmain_accel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m============================================\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     35\u001b[0m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplr_model_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrid_size\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mx\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrid_size\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[1], line 541\u001b[0m, in \u001b[0;36mmain_accel\u001b[1;34m(total_iterations, replay_prob, train_steps, level_buffer_size, initial_fill_size, grid_size, n_envs, edit_levels, regret_threshold, easy_start, domain_randomization, name)\u001b[0m\n\u001b[0;32m    539\u001b[0m cfg \u001b[38;5;241m=\u001b[39m random_config(grid_size)\n\u001b[0;32m    540\u001b[0m vectorized_env\u001b[38;5;241m.\u001b[39menv_method(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdate_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, cfg)\n\u001b[1;32m--> 541\u001b[0m \u001b[43mstudent_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    542\u001b[0m regret \u001b[38;5;241m=\u001b[39m calculate_regret_gae(MyCustomGrid(cfg), student_model, max_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.99\u001b[39m, lam\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m)\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRegret for current level: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mregret\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, buffer size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(level_buffer\u001b[38;5;241m.\u001b[39mdata)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\noost\\anaconda3\\envs\\accel\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\noost\\anaconda3\\envs\\accel\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:323\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 323\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[0;32m    326\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\noost\\anaconda3\\envs\\accel\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:202\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;66;03m# Convert to pytorch tensor or to TensorDict\u001b[39;00m\n\u001b[0;32m    201\u001b[0m     obs_tensor \u001b[38;5;241m=\u001b[39m obs_as_tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 202\u001b[0m     actions, values, log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    203\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    205\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\noost\\anaconda3\\envs\\accel\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\noost\\anaconda3\\envs\\accel\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\noost\\anaconda3\\envs\\accel\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:645\u001b[0m, in \u001b[0;36mActorCriticPolicy.forward\u001b[1;34m(self, obs, deterministic)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;124;03mForward pass in all the networks (actor and critic)\u001b[39;00m\n\u001b[0;32m    639\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;124;03m:return: action, value and log probability of the action\u001b[39;00m\n\u001b[0;32m    643\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    644\u001b[0m \u001b[38;5;66;03m# Preprocess the observation if needed\u001b[39;00m\n\u001b[1;32m--> 645\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    646\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare_features_extractor:\n\u001b[0;32m    647\u001b[0m     latent_pi, latent_vf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_extractor(features)\n",
      "File \u001b[1;32mc:\\Users\\noost\\anaconda3\\envs\\accel\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:672\u001b[0m, in \u001b[0;36mActorCriticPolicy.extract_features\u001b[1;34m(self, obs, features_extractor)\u001b[0m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;124;03mPreprocess the observation if needed and extract features.\u001b[39;00m\n\u001b[0;32m    665\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;124;03m    features for the actor and the features for the critic.\u001b[39;00m\n\u001b[0;32m    670\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare_features_extractor:\n\u001b[1;32m--> 672\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures_extractor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeatures_extractor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeatures_extractor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m features_extractor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\noost\\anaconda3\\envs\\accel\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:130\u001b[0m, in \u001b[0;36mBaseModel.extract_features\u001b[1;34m(self, obs, features_extractor)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, obs: PyTorchObs, features_extractor: BaseFeaturesExtractor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    123\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03m    Preprocess the observation if needed and extract features.\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    :return: The extracted features\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m     preprocessed_obs \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_obs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize_images\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_images\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m features_extractor(preprocessed_obs)\n",
      "File \u001b[1;32mc:\\Users\\noost\\anaconda3\\envs\\accel\\Lib\\site-packages\\stable_baselines3\\common\\preprocessing.py:120\u001b[0m, in \u001b[0;36mpreprocess_obs\u001b[1;34m(obs, observation_space, normalize_images)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation_space, spaces\u001b[38;5;241m.\u001b[39mBox):\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m normalize_images \u001b[38;5;129;01mand\u001b[39;00m is_image_space(observation_space):\n\u001b[1;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obs\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation_space, spaces\u001b[38;5;241m.\u001b[39mDiscrete):\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;66;03m# One hot encoding and convert to float to avoid errors\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "        \n",
    "    wandb.finish()\n",
    "        \n",
    "    config = {\n",
    "            \"name\": \"model\",\n",
    "            \"grid_size\": 10,\n",
    "            \n",
    "            \"total_iterations\": 512,\n",
    "            \"train_steps\": 1024,\n",
    "\n",
    "            \"replay_prob\": 0.7,            # Probability of replaying a level and editing it vs. generating a new one\n",
    "            \"level_buffer_size\": 128,       # Maximum number of levels to store in the buffer\n",
    "            \"initial_fill_size\": 64,       # Number of levels to pre-fill the buffer with\n",
    "            \"regret_threshold\": 0.00,      # Minimum regret threshold to consider a level for the buffer\n",
    "            \n",
    "            \"n_envs\": 2,                   # Number of parallel environments to use for training\n",
    "            \n",
    "            \"edit_levels\": True,           # Whether to edit levels during training i.e. ACCEL or PLR\n",
    "            \"easy_start\": True,            # Whether to fill the buffer with easy levels first i.e. minimum number of blocks\n",
    "            \"domain_randomization\": False, # Whether to use domain randomization\n",
    "    \n",
    "        }\n",
    "\n",
    "\n",
    "    config[\"name\"] = f\"dr_model_{config['grid_size']}x{config['grid_size']}\"\n",
    "    config[\"domain_randomization\"] = True\n",
    "    config[\"edit_levels\"] = False\n",
    "    config[\"easy_start\"] = False\n",
    "    print(f\"Running Domain Randomization with config: {config}\")\n",
    "    model_dr = main_accel(**config)\n",
    "        \n",
    "    print(\"\\n\\n============================================\\n\\n\")\n",
    "    \n",
    "    config[\"name\"] = f\"plr_model_{config['grid_size']}x{config['grid_size']}\"\n",
    "    config[\"domain_randomization\"] = False\n",
    "    config[\"edit_levels\"] = False\n",
    "    config[\"easy_start\"] = False\n",
    "    print(f\"Running PLR with config: {config}\")\n",
    "    model_plr = main_accel(**config)\n",
    "\n",
    "    print(\"\\n\\n============================================\\n\\n\")\n",
    "\n",
    "\n",
    "    config[\"name\"] = f\"accel_model_{config['grid_size']}x{config['grid_size']}\"\n",
    "    config[\"domain_randomization\"] = False\n",
    "    config[\"edit_levels\"] = True\n",
    "    config[\"easy_start\"] = False\n",
    "    print(f\"Running ACCEL with config: {config}\")\n",
    "    model_accel = main_accel(**config)\n",
    "\n",
    "    print(\"\\n\\n============================================\\n\\n\")\n",
    "    \n",
    "    config[\"name\"] = f\"accel_model_easy_{config['grid_size']}x{config['grid_size']}\"\n",
    "    config[\"domain_randomization\"] = False\n",
    "    config[\"edit_levels\"] = True\n",
    "    config[\"easy_start\"] = True\n",
    "    print(f\"Running ACCEL with easy start with config: {config}\")\n",
    "    model_accel_easy = main_accel(**config)\n",
    "    \n",
    "    print(\"\\n\\n============================================\\n\\n\")\n",
    "\n",
    "    # Evaluate the models\n",
    "    evalute_models(load_dim = config[\"grid_size\"], grid_size = config[\"grid_size\"], n_eval_episodes = 5, num_levels_per_difficulty = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 30 levels of difficulty 1 with 12 blocks on a 12x12 grid for model DR, ratio of blocks to grid size: 0.08\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mevalute_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_levels_per_difficulty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_model\u001b[39m(model, config, gif_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlevel.gif\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      5\u001b[0m     env \u001b[38;5;241m=\u001b[39m MyCustomGrid(config, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m'\u001b[39m, solvable_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[3], line 445\u001b[0m, in \u001b[0;36mevalute_models\u001b[1;34m(load_dim, grid_size, n_eval_episodes, num_levels_per_difficulty)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, cfg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(level):\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;66;03m# Update the environment with the new config\u001b[39;00m\n\u001b[0;32m    444\u001b[0m     env\u001b[38;5;241m.\u001b[39menv_method(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdate_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, cfg)\n\u001b[1;32m--> 445\u001b[0m     mean_reward, _ \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_eval_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    446\u001b[0m     r\u001b[38;5;241m.\u001b[39mappend(mean_reward)\n\u001b[0;32m    447\u001b[0m results[model_name]\u001b[38;5;241m.\u001b[39mappend(r)\n",
      "File \u001b[1;32mc:\\Users\\noost\\anaconda3\\envs\\accel\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:88\u001b[0m, in \u001b[0;36mevaluate_policy\u001b[1;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[0;32m     86\u001b[0m episode_starts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((env\u001b[38;5;241m.\u001b[39mnum_envs,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (episode_counts \u001b[38;5;241m<\u001b[39m episode_count_targets)\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m---> 88\u001b[0m     actions, states \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepisode_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m     new_observations, rewards, dones, infos \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(actions)\n\u001b[0;32m     95\u001b[0m     current_rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rewards\n",
      "File \u001b[1;32mc:\\Users\\noost\\anaconda3\\envs\\accel\\Lib\\site-packages\\stable_baselines3\\common\\base_class.py:556\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[0;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    538\u001b[0m     observation: Union[np\u001b[38;5;241m.\u001b[39mndarray, Dict[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    541\u001b[0m     deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    542\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, Optional[Tuple[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]:\n\u001b[0;32m    543\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;124;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\noost\\anaconda3\\envs\\accel\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:368\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    365\u001b[0m obs_tensor, vectorized_env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_to_tensor(observation)\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 368\u001b[0m     actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;66;03m# Convert to numpy, and reshape to the original action shape\u001b[39;00m\n\u001b[0;32m    370\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mshape))  \u001b[38;5;66;03m# type: ignore[misc, assignment]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\noost\\anaconda3\\envs\\accel\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:717\u001b[0m, in \u001b[0;36mActorCriticPolicy._predict\u001b[1;34m(self, observation, deterministic)\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, observation: PyTorchObs, deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    710\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    711\u001b[0m \u001b[38;5;124;03m    Get the action according to the policy for a given observation.\u001b[39;00m\n\u001b[0;32m    712\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;124;03m    :return: Taken action according to the policy\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mget_actions(deterministic\u001b[38;5;241m=\u001b[39mdeterministic)\n",
      "File \u001b[1;32mc:\\Users\\noost\\anaconda3\\envs\\accel\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:752\u001b[0m, in \u001b[0;36mActorCriticPolicy.get_distribution\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m    750\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mextract_features(obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpi_features_extractor)\n\u001b[0;32m    751\u001b[0m latent_pi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_extractor\u001b[38;5;241m.\u001b[39mforward_actor(features)\n\u001b[1;32m--> 752\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_action_dist_from_latent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_pi\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\noost\\anaconda3\\envs\\accel\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:697\u001b[0m, in \u001b[0;36mActorCriticPolicy._get_action_dist_from_latent\u001b[1;34m(self, latent_pi)\u001b[0m\n\u001b[0;32m    694\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist\u001b[38;5;241m.\u001b[39mproba_distribution(mean_actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_std)\n\u001b[0;32m    695\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist, CategoricalDistribution):\n\u001b[0;32m    696\u001b[0m     \u001b[38;5;66;03m# Here mean_actions are the logits before the softmax\u001b[39;00m\n\u001b[1;32m--> 697\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_dist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproba_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist, MultiCategoricalDistribution):\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# Here mean_actions are the flattened logits\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist\u001b[38;5;241m.\u001b[39mproba_distribution(action_logits\u001b[38;5;241m=\u001b[39mmean_actions)\n",
      "File \u001b[1;32mc:\\Users\\noost\\anaconda3\\envs\\accel\\Lib\\site-packages\\stable_baselines3\\common\\distributions.py:288\u001b[0m, in \u001b[0;36mCategoricalDistribution.proba_distribution\u001b[1;34m(self, action_logits)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mproba_distribution\u001b[39m(\u001b[38;5;28mself\u001b[39m: SelfCategoricalDistribution, action_logits: th\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfCategoricalDistribution:\n\u001b[1;32m--> 288\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribution \u001b[38;5;241m=\u001b[39m \u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction_logits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\noost\\anaconda3\\envs\\accel\\Lib\\site-packages\\torch\\distributions\\categorical.py:68\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[1;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits \u001b[38;5;241m=\u001b[39m logits \u001b[38;5;241m-\u001b[39m logits\u001b[38;5;241m.\u001b[39mlogsumexp(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobs \u001b[38;5;28;01mif\u001b[39;00m probs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m---> 68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_param\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     69\u001b[0m batch_shape \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39mndimension() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mSize()\n\u001b[0;32m     71\u001b[0m )\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(batch_shape, validate_args\u001b[38;5;241m=\u001b[39mvalidate_args)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#evalute_models(load_dim=8, grid_size=12, n_eval_episodes=10, num_levels_per_difficulty=30)\n",
    "\n",
    "\n",
    "def test_model(model, config, gif_path=\"level.gif\"):\n",
    "    env = MyCustomGrid(config, render_mode='rgb_array', solvable_only=True)\n",
    "    obs, _ = env.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    total_reward = 0\n",
    "    i = 0\n",
    "\n",
    "    frames = []  # List to store frames\n",
    "\n",
    "    # Continue until either terminated or truncated is True\n",
    "    while not (terminated or truncated):\n",
    "        frame = env.render()  # Capture frame as an image\n",
    "        frames.append(Image.fromarray(frame))  # Convert to PIL image and store\n",
    "        \n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        i += 1\n",
    "        if i > 20:\n",
    "            break\n",
    "    \n",
    "    # Add to the gif also the last frame\n",
    "    frame = env.render()\n",
    "    frames.append(Image.fromarray(frame))\n",
    "\n",
    "    # Save frames as a GIF\n",
    "    if frames:\n",
    "        frames[0].save(\n",
    "            gif_path, save_all=True, append_images=frames[1:], duration=500, loop=0\n",
    "        )\n",
    "\n",
    "    env.close()\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def load_models(grid_size):\n",
    "    # Load the models\n",
    "    model_dr = PPO.load(f\"models/dr_model_{grid_size}x{grid_size}\")\n",
    "    model_plr = PPO.load(f\"models/plr_model_{grid_size}x{grid_size}\")\n",
    "    model_accel = PPO.load(f\"models/accel_model_{grid_size}x{grid_size}\")\n",
    "    model_accel_easy = PPO.load(f\"models/accel_model_easy_{grid_size}x{grid_size}\")\n",
    "    \n",
    "    return model_dr, model_plr, model_accel, model_accel_easy\n",
    "\n",
    "model_dr, model_plr, model_accel, model_accel_easy = load_models(8)\n",
    "# Test the models on a few ranom levels\n",
    "for i in range(10):\n",
    "    reward = test_model(model_dr, random_config(8), gif_path=f\"gifs/level_{i}_dr.gif\")\n",
    "    print(f\"Reward for level {i} with DR: {reward:.2f}\")\n",
    "    reward =test_model(model_plr, random_config(8), gif_path=f\"gifs/level_{i}_plr.gif\")\n",
    "    print(f\"Reward for level {i} with PLR: {reward:.2f}\")\n",
    "    reward =test_model(model_accel, random_config(8), gif_path=f\"gifs/level_{i}_accel.gif\")\n",
    "    print(f\"Reward for level {i} with ACCEL: {reward:.2f}\")\n",
    "    reward =test_model(model_accel_easy, random_config(8), gif_path=f\"gifs/level_{i}_accel_easy.gif\")\n",
    "    print(f\"Reward for level {i} with ACCEL-EasyStart: {reward:.2f}\")\n",
    "\n",
    "\"\"\"\n",
    "models = {\n",
    "    \"DR\": model_dr,\n",
    "    \"PLR\": model_plr,\n",
    "    \"ACCEL\": model_accel,\n",
    "    \"ACCEL-EasyStart\": model_accel_easy\n",
    "}\n",
    "    \n",
    "# Test the models on previously evaluated levels\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Testing model {model_name} on previously evaluated levels...\")\n",
    "    for i, level in enumerate(levels):\n",
    "        print(f\"Level {i + 1} - Complexity {(i+2)**2}:\")\n",
    "        for j, cfg in enumerate(level):\n",
    "            mean_reward = test_model(model, cfg)\n",
    "            print(f\"  Config {j + 1}: {mean_reward:.2f}\")\n",
    "        print()\n",
    "    print()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_cnf: {'width': 8, 'height': 8, 'num_blocks': 6, 'start_pos': (6, 6), 'goal_pos': (2, 1), 'edited': False, 'seed_val': 963}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABD0AAAFeCAYAAACGrGQrAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAATmhJREFUeJzt3Xd4VFXi//HPJJBCGi0hAQyEgAIBQWIBJIC0qKj0ugoEQZam2FB0laKI2BZFKYo/QHSXLrB0UFCUVaQpVSmCitIJCS0hyfn9gTPfTCYhM0lwwt3363ny8HBzc++ZW8/9zD3n2IwxRgAAAAAAABbj4+0CAAAAAAAAXAuEHgAAAAAAwJIIPQAAAAAAgCURegAAAAAAAEsi9AAAAAAAAJZE6AEAAAAAACyJ0AMAAAAAAFgSoQcAAAAAALAkQg8AAAAAAGBJlgo9bDabRo0a5e1i5OvcuXPq16+fIiMjZbPZNGzYMB06dEg2m00zZszwdvEKpTjvgzNnzqhHjx6KiIiQr6+vDh06dE3WU7VqVfXp0yff+WbMmCGbzeZWOezzbt68ufAFLAZGjRolm82mkydPFtky+/Tpo6pVqxb4b4ODg4usLChanuyfa3kNWr9+vWw2m+bPn39Nlg9I0qZNm+Tn56fDhw97uyiSpN27d6tEiRLauXOnt4tSZApzv7ge3Xvvverfv7+3i+EwZcoURUdHKy0tzdtFcWKvm2Tnbp2uKBTX49J+71u/fr23i1KsFYftVBzKUNRiYmIUGhqqZs2a6YcffijwcjwKPaz04JWSkqLRo0erXr16Cg4OVmBgoOrUqaNnnnlGv//++zVd9yuvvKIZM2Zo4MCBmjVrlh566KFrur6itnz58mIbbFzNP//5T82ePVsPPPCApk2bpvDwcMfv7KGTty8SkyZNKnTw1bx580LdoM+ePavhw4erRo0aCgwMVJUqVfTwww/rl19+KfAyrRDoSdKFCxc0atSoXI+TUaNGFaqykpWVpSlTpqh+/foKDg5WhQoVdM8992jjxo0FXmZhjwVcO3369FHz5s0LtYzU1FQNHz5cMTEx8vf3V6VKldS5c2dduHChQMurWrWqx9f2a3k/uNr5VlwVxTn3/PPPq0ePHqpSpYrT9KysLE2ePFn169dXYGCgypUrpxYtWuj7778v1PrmzJmjRo0aKSgoSKVLl1bjxo31+eefO35fu3ZttW3bVi+++GKh1mOV+8Bfyf4AU5gvab7++mutXr1azzzzjGPa3r17NXz4cNWvX18hISGKiopS27ZtC12/nzNnjh588EHVqFFDNpstz2tcnz59lJ6erqlTpxZ4XUWxbQpi9+7dGjVq1F+23sKcN/bnttx+nn322aIt6F+E64jnClvfOHfunIYNG6bKlSvL399ftWrV0uTJk/Ocf+3atWrRooXCwsIUEhKi+Ph4zZkzx2mexx9/XA0aNFDZsmVVqlQp1apVS6NGjdK5c+dclvfWW2/pmWee0fbt2zVkyJACf44SBf7L69jBgwfVqlUr/fLLL+rSpYseeeQR+fn56YcfftCHH36oTz/9VD/99NM1W//nn3+uhg0bauTIkY5pxhhdvHhRJUuWvGbrLSrLly/Xe++9l2tF9+LFiypRongeVlu3blX58uX1wQcfuCT5RenHH3+Uj0/BXqKaNGmSypcv77UH1aysLLVu3Vq7d+/WoEGDdOONN2r//v2aNGmSVq1apT179igkJMQrZcvPBx98oKysrGu6jgsXLmj06NGSVOgH1pyefvppvfXWW3rwwQc1aNAgJScna+rUqWrWrJm+/vpr3X777UW6Plzfzp49q2bNmum3337TI488ourVq+vEiRPasGGD0tLSVKpUqb+kHFe7HxTWtTzfiqvt27dr7dq1uYadffv21SeffKJevXppyJAhOn/+vLZt26bjx48XeH2jRo3SmDFj1LlzZ/Xp00eXL1/Wzp07deTIEaf5/v73v+vee+/VgQMHFBsbW+D14a/3+uuvq2XLlqpevbpj2rRp0/Thhx+qU6dOGjRokM6ePaupU6eqYcOGWrlypVq1alWgdU2ePFlbtmzRbbfdplOnTuU5X0BAgHr37q233npLQ4cOvaZ1ssLKWafbvXu3Ro8erebNmxfLtzJyM2bMGMXExDhNq1OnjpdKg+tJZmamEhMTtXnzZg0ePFg1atTQqlWrNGjQIJ05c0bPPfec0/zTp0/Xww8/rNatW+uVV16Rr6+vfvzxR/36669O83333XdKSEhQUlKSAgICtG3bNr366qtau3atvvzyS6dzrkOHDurQoYMOHz6s2bNnF/izFM+n02soIyNDHTt21LFjx7R+/Xo1adLE6fdjx47V+PHjr2kZjh8/rtq1aztNs9lsCggIuKbrzcv58+cVFBRUJMvy1mdwx/nz5xUREXHNb67+/v7XdPnX0jfffKPvvvtO7777rgYPHuyYftNNN6lv375au3atOnTo4MUS5u16CAzzkpGRocmTJ6tz586aNWuWY3qXLl1UrVo1ffLJJ4QecDJixAgdPnxYW7dudarMZv8293qVlZWl9PR0bxfDK6ZPn67o6Gg1bNjQafrcuXM1c+ZMLVy4sMiuwd98843GjBmjN998U48//vhV523VqpXKlCmjmTNnasyYMUWyflx7x48f17JlyzRlyhSn6T169NCoUaOcmg327dvX8W1rQUOPWbNmqVKlSvLx8cn3obpr16567bXXtG7dOrVo0aJA6/srXM91Ort77rlHt956q7eLgevQwoULtXHjRn344Yfq27evJGngwIHq3LmzXnrpJfXr108RERGSrrw1P3jwYA0dOlRvv/32VZf71VdfuUyLjY3VU089pU2bNrncAyUpMjJSqampBf4s16RPjyNHjqhv376qUKGC/P39FRcXp//3//6f4/fHjh1TiRIlHN/gZPfjjz/KZrPp3XffdUxLTk7WsGHDdMMNN8jf31/Vq1fX+PHjC/St7oIFC/T999/r+eefdwk8JCk0NFRjx451mjZv3jzFx8crMDBQ5cuX14MPPujyLYi9zfmRI0fUvn17BQcHKzw8XE899ZQyMzMl/d+reD///LOWLVvmeMXs0KFDefbpMW/ePNWuXVsBAQGqU6eOPv30U5c2f3m138ptmfZyHjhwQPfee69CQkL0t7/9TZK0YcMGdenSRdHR0fL399cNN9ygxx9/XBcvXnT6+/fee0+SnF6Ts8utPf22bdt0zz33KDQ0VMHBwWrZsqW++eYbp3nsr+B9/fXXeuKJJxQeHq6goCB16NBBJ06ccJr37Nmz2rt3r86ePStPGGPcDjyWLFkim83m1HZswYIFstls6tixo9O8tWrVUrdu3Rz/z639565du9SiRQsFBgaqcuXKevnll12O36pVq2rXrl364osvHNs157ebaWlp+W6f3Pzyyy/au3dvvvOlpKRIkipUqOA0PSoqSpIUGBiY7zI8kZycrD59+qh06dIKCwtTUlJSrq/mf/zxx45zsGzZsurevbtLapxbW9hTp07poYceUmhoqEqXLq3evXvr+++/z/P1yKudv4cOHXI0iRo9erRjH13tG+6TJ09q7969+TY3uHz5si5evOiy3SMiIuTj41Nk291+rZg7d67Gjh2rypUrKyAgQC1bttT+/fud5s2rHXPz5s2djsvsyxw9erQqVaqkkJAQde7cWWfPnlVaWpqGDRumiIgIBQcHKykpqcDtuA8ePKjExEQFBQWpYsWKGjNmjIwx+f6dO9cg6crx+Pjjj6tq1ary9/dX5cqV1atXr6v2PZOWlqb77rtPYWFhjm/nU1NTNWzYMMdyIiIi1Lp1a23duvWq5fzjjz+0d+9eXb58+arzJScna/r06XrkkUcUExOj9PT0a9I2/vLlyxo9erRq1KihgIAAlStXTk2aNNGaNWsk5X8/eOONN9S4cWOVK1dOgYGBio+Pz7U/FJvNpiFDhuiTTz5RXFyc/P39NWXKFI/Pt+zsx+WcOXP03HPPKTIyUkFBQXrggQdcrh2Se/f5o0ePKikpyfGKb1RUlNq1a5fva+7uXn8ladGiRWrRooXLveqtt97S7bffrg4dOigrK0vnz593a3lXM2HCBEVGRuqxxx6TMSbX14rtSpYsqebNm2vx4sWFXm9269ev16233qqAgADFxsZq6tSpufarkJGRoZdeekmxsbHy9/dX1apV9dxzz7kc94sXL1bbtm1VsWJF+fv7KzY2Vi+99JLjOl4YzZs3V506dbRlyxY1btxYgYGBiomJcQkUpCthw8MPP6wKFSooICBA9erV08yZM13mmz17tuLj4xUSEqLQ0FDVrVs334eFCxcuaO/evW71ibVs2TJlZGS4hBjx8fEu/SSVK1dOCQkJ2rNnT77LzcsNN9zg9puu8fHxKlu2bJEfU99++63uvvtuhYWFqVSpUo63JXP66quvdNtttzkde7nJfi+cMWOGunTpIkm66667HNel7PXvFStWKCEhQUFBQQoJCVHbtm21a9cul+UuWrRIderUcarfu2vv3r2FanJsd/jwYQ0aNEg33XSTo8lcly5d3Gq6s2/fPnXq1EmRkZEKCAhQ5cqV1b17d5e6uTv1t8Jw937r7nGR3zOs3W+//ab27dsrKChIERERevzxxz2+D2/evFk2my3Xa8OqVatks9m0dOlSSYXbV7lxt76xYcMGSVL37t2dpnfv3l2XLl1yOn+nTJmizMxMRzB+7tw5t+podvY6fHJycq6/L+hb9HZF/qbHsWPH1LBhQ0clJjw8XCtWrNDDDz+slJQUDRs2TBUqVFCzZs00d+5cpyYe0pX2gL6+vo6LyoULF9SsWTMdOXJEAwYMUHR0tDZu3KgRI0bojz/+0IQJEzwq35IlSyTJ7X40ZsyYoaSkJN12220aN26cjh07prfffltff/21tm3bptKlSzvmtb8CdMcdd+iNN97Q2rVr9eabbyo2NlYDBw5UrVq1NGvWLD3++OOqXLmynnzySUlSeHh4rg+uy5YtU7du3VS3bl2NGzdOZ86c0cMPP6xKlSp59JlzysjIUGJiopo0aaI33njD8Rr0vHnzdOHCBQ0cOFDlypXTpk2bNHHiRP3222+aN2+eJGnAgAH6/ffftWbNGqdvpPOya9cuJSQkKDQ0VMOHD1fJkiU1depUNW/eXF988YXuuOMOp/mHDh2qMmXKaOTIkTp06JAmTJigIUOGOLUF+/TTT5WUlKTp06d71AwkKyvL7ROmSZMmstls+vLLL3XzzTdLunLi+/j4OKWTJ06c0N69e6/axuzo0aO66667lJGRoWeffVZBQUF6//33XR5kJ0yYoKFDhyo4OFjPP/+8JNfwwZ3tk5tevXrpiy++yPfic+uttyooKEgvvPCCypYtq5tuukn79+/X8OHDddtttxX425+8dO3aVTExMRo3bpy2bt2qadOmKSIiwultq7Fjx+qFF15Q165d1a9fP504cUITJ05U06ZNXc7B7LKysnT//fdr06ZNGjhwoGrWrKnFixerd+/euc6f3/kbHh6uyZMna+DAgerQoYMj/LIfH7l59913NXr0aK1bt+6qr+cHBgbqjjvu0IwZM9SoUSMlJCQoOTlZL730ksqUKaNHHnkk/43pgVdffVU+Pj566qmndPbsWb322mv629/+pm+//bbAyxw3bpwCAwP17LPPav/+/Zo4caJKliwpHx8fnTlzRqNGjdI333yjGTNmKCYmxuP+ATIzM3X33XerYcOGeu2117Ry5UqNHDlSGRkZV/3m2d1r0Llz5xwV/r59+6pBgwY6efKklixZot9++03ly5d3WfbFixfVrl07bd68WWvXrtVtt90m6UpTgPnz52vIkCGqXbu2Tp06pa+++kp79uxRgwYN8izriBEjNHPmTP38889XfW36q6++0qVLl1S9enV17txZixYtUlZWlho1aqT33ntP9evXd2+j5mPUqFEaN26c+vXrp9tvv10pKSnavHmztm7dqtatW+d7P3j77bf1wAMP6G9/+5vS09M1e/ZsdenSRUuXLlXbtm2d5v388881d+5cDRkyROXLl1e9evU8Pt9yM3bsWNlsNj3zzDM6fvy4JkyYoFatWmn79u2Oa7C79/lOnTpp165dGjp0qKpWrarjx49rzZo1+uWXX666v9y9/h45ckS//PKLyzGSkpKiTZs2adCgQXruuec0ceJEnTt3TjExMXr11VfVtWtXj7aJ3WeffabGjRvrnXfe0csvv6xTp04pMjJSzz//fK73tPj4eC1evFgpKSkKDQ0t0Dqz27Ztm+6++25FRUVp9OjRjspy9v627Pr166eZM2eqc+fOevLJJ/Xtt99q3Lhx2rNnj9PD4owZMxQcHKwnnnhCwcHB+vzzz/Xiiy8qJSVFr7/+eqHLfObMGd17773q2rWrevTooblz52rgwIHy8/NzfAt68eJFNW/eXPv379eQIUMUExOjefPmqU+fPkpOTtZjjz0mSVqzZo169Oihli1bOu55e/bs0ddff+2YJzebNm3SXXfdpZEjR+YbAm7cuFHlypVz6R8mL0ePHs31WnetNGjQINcHz4L6/PPPdc899yg+Pl4jR46Uj4+Ppk+frhYtWmjDhg2ONyZ37NihNm3aKDw8XKNGjVJGRoZGjhzpUufKqWnTpnr00Uf1zjvv6LnnnlOtWrUkyfHvrFmz1Lt3byUmJmr8+PG6cOGCJk+erCZNmmjbtm2O68Tq1avVqVMn1a5dW+PGjdOpU6ccgao7atWqpWbNmrnd39HZs2ddQrLy5cvru+++08aNG9W9e3dVrlxZhw4d0uTJk9W8eXPt3r07z2aS6enpSkxMVFpamoYOHarIyEgdOXJES5cuVXJyssLCwiQVvP7mCXfut+4eF+48w0pXzvGWLVvql19+0aOPPqqKFStq1qxZTn0huePWW29VtWrVNHfuXJe66Zw5c1SmTBklJiZKUoH3VV7crW+kpaXJ19dXfn5+TtPt69uyZYujk+S1a9eqZs2aWr58uZ5++mkdOXJEZcqU0eDBgzV69GiXZ7CMjAwlJycrPT1dO3fu1D/+8Q+FhITk+WazPQz35HnOifHA9OnTjSTz3Xff5TnPww8/bKKioszJkyedpnfv3t2EhYWZCxcuGGOMmTp1qpFkduzY4TRf7dq1TYsWLRz/f+mll0xQUJD56aefnOZ79tlnja+vr/nll18c0ySZkSNHXvUz3HLLLSYsLOyq89ilp6ebiIgIU6dOHXPx4kXH9KVLlxpJ5sUXX3RM6927t5FkxowZ47K++Ph4p2lVqlQxbdu2dZr2888/G0lm+vTpjml169Y1lStXNqmpqY5p69evN5JMlSpVHNPWrVtnJJl169blu0x7OZ999lmXz2vfN9mNGzfO2Gw2c/jwYce0wYMHm7wOnZz7oH379sbPz88cOHDAMe333383ISEhpmnTpo5p9mOrVatWJisryzH98ccfN76+viY5Odll3uyfyx2xsbEmISHB7fnj4uJM165dHf9v0KCB6dKli5Fk9uzZY4wxZuHChUaS+f777x3zValSxfTu3dvx/2HDhhlJ5ttvv3VMO378uAkLCzOSzM8//+y0zmbNmrmUxZPtk5tmzZrluc9yWrp0qYmKijKSHD+JiYlOx2FhjRw50kgyffv2dZreoUMHU65cOcf/Dx06ZHx9fc3YsWOd5tuxY4cpUaKE0/TevXs7nRcLFiwwksyECRMc0zIzM02LFi3yPC/yO39PnDjh1nUm5+fMeW7mZt++faZBgwZO271atWpm7969bq3LHfZrRa1atUxaWppj+ttvv+1yPc55HNs1a9bM6Ri1L7NOnTomPT3dMb1Hjx7GZrOZe+65x+nvGzVq5LSf3GHfP0OHDnVMy8rKMm3btjV+fn7mxIkTjukFvQa9+OKLRpJZuHChy/rt55z9s86bN8+kpqaaZs2amfLly5tt27Y5zR8WFmYGDx7s0WfM/jmzXxNy89ZbbxlJply5cub22283n3zyiZk0aZKpUKGCKVOmjPn99989Xndu6tWr53Kvyulq94Oc95T09HRTp04dp3u8MVf2mY+Pj9m1a5fTdE/Pt+zs+6pSpUomJSXFMX3u3LlGknn77bcdZXLnPn/mzBkjybz++usel8Xd6+/atWuNJPOf//zHafrWrVsd+7tChQpm0qRJ5pNPPjG33367sdlsZsWKFR6X6fTp045lBgcHm9dff93MmTPH3H333UaSmTJlisvf/Otf/3K5lxXG/fffb0qVKmWOHDnimLZv3z5TokQJp+21fft2I8n069fP6e+feuopI8l8/vnnjmm51WMGDBhgSpUqZS5duuSYlvN+4Q77fnzzzTcd09LS0kz9+vVNRESE4/o3YcIEI8l8/PHHjvnS09NNo0aNTHBwsON4fOyxx0xoaKjJyMjwqBz2Y9ud86JJkyYuddC8fPnll8Zms5kXXnjBo/LkJa/6THaPPPKICQwMLJL1ZWVlmRo1apjExESnetKFCxdMTEyMad26tWNa+/btTUBAgFPddvfu3cbX19flXM15L5w3b16u9/XU1FRTunRp079/f6fpR48eNWFhYU7T69evb6KiopzqbqtXr3ap3+dFUr7b1pj/qzvm9mNM7ufLf//7XyPJfPTRR45pOZ8ztm3b5rgX5sWT+lth5He/9eS4cPcZ1n6Oz5071zHP+fPnTfXq1d2u89mNGDHClCxZ0pw+fdoxLS0tzZQuXdqpjlzQfZUXd+sbb775ppFkNmzY4DT92WefNZLMfffd55gWGhpqypQpY/z9/c0LL7xg5s+fb3r27Jnnc6e9/Pafm2666arl/uc//2kkmd9+++2qZc5LkTZvMcZowYIFuv/++2WM0cmTJx0/iYmJOnv2rON1o44dO6pEiRJO31Dv3LlTu3fvdmoqMG/ePCUkJKhMmTJOy2vVqpUyMzP15ZdfelTGlJQUtzti3Lx5s44fP65BgwY59VXRtm1b1axZU8uWLXP5m7///e9O/09ISNDBgwc9KqMk/f7779qxY4d69erl9Apis2bNVLduXY+Xl9PAgQNdpmV/8+D8+fM6efKkGjduLGOMtm3b5vE6MjMztXr1arVv317VqlVzTI+KilLPnj311VdfOZpT2D3yyCNOr7UmJCQoMzPTaei+Pn36yBjj1lsely9f1q+//qp33nlHBw4c8OhNhYSEBMdrXampqfr+++/1yCOPqHz58o7pGzZsUOnSpa/adnX58uVq2LChU3IZHh7uaFbkCXe2T27Wr1/v9itm4eHhuuWWWzR27FgtWrRIo0aN0oYNG5SUlORxefOT2/ly6tQpx3GxcOFCZWVlqWvXrk7nf2RkpGrUqKF169blueyVK1eqZMmSTsP0+fj4OPVV4k55CnL+2o0aNUrGGLc6YQwJCVFcXJwGDx6shQsXatKkScrIyFD79u2LdGhfSUpKSnJK7RMSEiSpUJ+1V69eTv2q3HHHHTLGOL79zD79119/VUZGhsfryP7ts/2bmPT0dK1duzbX+T25Bi1YsED16tXLtb+EnK/anz17Vm3atNHevXu1fv16lzcrSpcurW+//dbjkcBmzJghY0y+nePZmyHYbDZ99tln6tmzpwYOHKhFixbpzJkzjiYnhVW6dGnt2rVL+/btK9DfZ7+nnDlzRmfPnlVCQkKuzXyaNWvm0tdVUejVq5fTPb9z586KiorS8uXLJbl/nw8MDJSfn5/Wr1+vM2fOeFQGd6+/9o4fy5Qp4zTdvr9PnTqlxYsXa+DAgerZs6c+++wzlStXTi+//LJH5cm5zGnTpumpp55S165dtWzZMtWuXTvXZdrLVRTXo8zMTK1du1bt27dXxYoVHdOrV6+ue+65x2le+7564oknnKbb35bNXhfLfsylpqbq5MmTSkhIcDQJKawSJUpowIABjv/7+flpwIABOn78uLZs2eIob2RkpHr06OGYr2TJknr00Ud17tw5ffHFF5KunF/nz593NBdzV/PmzWWMcaup16lTp1yOp9wcP35cPXv2VExMjIYPH+5ReQqjTJkyunjxYoFHnMpu+/bt2rdvn3r27KlTp0456gvnz59Xy5Yt9eWXXyorK0uZmZlatWqV2rdvr+joaMff16pVy/GtekGsWbNGycnJ6tGjh1N9xdfXV3fccYejvvLHH39o+/bt6t27t+ONCElq3bq129dAY4xHo1q99957WrNmjdOP5Hy+XL58WadOnVL16tVVunTpqzbHtJd71apVee67wtTfPJHf/dbd48KTZ9jly5crKipKnTt3dqynVKlSBXort1u3brp8+bIWLlzomLZ69WolJyc7PQ8XdF/lxd36Rs+ePRUWFqa+fftqzZo1OnTokN5//31NmjRJkpy6QDh37pzOnDmj0aNHa8yYMerUqZM++eQT3X333Xr77bdd+uOoXbu21qxZo0WLFmn48OEKCgq6ajPLhIQE2Ww2/eMf/9C+ffs8vm4Uaehx4sQJJScn6/3331d4eLjTj/2Byd7LePny5dWyZUvNnTvX8fdz5sxRiRIlnPpM2Ldvn1auXOmyPPvDq6e9loeGhrrdCYr9QfKmm25y+V3NmjVdHjQDAgJcXsssU6aMxxWk7OvO3tu2XW7TPFGiRIlcX6H75Zdf1KdPH5UtW9bRp0GzZs0kyeP+M6Qrx8OFCxdy3X61atVSVlaWS7u+7Dcg6f8qWQXZhtKVodqio6P12GOPqV27do5mI+5ISEjQH3/8of3792vjxo2y2WyOpgfZQ48777zzqq9ZHT58WDVq1HCZntt2yU9Rb5+cDh48qLvuukt9+/bVc889p3bt2mnkyJGaNGmS5s+frxUrVhTJeuzy+zz79u2TMUY1atRwuQbs2bPnquf/4cOHFRUV5fLKX17nT1Gev56yt7kOCwvTu+++qw4dOmjgwIFau3atDhw4UCSvZWd3LY6jnMu0V4puuOEGl+lZWVkeX1N8fHycggtJuvHGGyUpzzatnlyDDhw44HZv9sOGDdN3332ntWvXKi4uzuX3r732mnbu3KkbbrhBt99+u0aNGlWoQCkne+Xn/vvvdwrFGzZsqJiYmEINc5zdmDFjlJycrBtvvFF169bV008/7dTPUX6WLl2qhg0bKiAgQGXLlnU0Ectt3+ccWaCo5Lz22mw2Va9e3XHMuHuf9/f31/jx47VixQpVqFBBTZs21WuvvaajR48WeZlzBiT2/R0TE+PUJDQ4ONjRhM/TENG+zJIlSzpV3H18fNStWzf99ttvLn0G2MtVFJ2BHz9+XBcvXnSrjnP48GH5+Pi4TI+MjFTp0qWd6mK7du1Shw4dFBYWptDQUIWHh+vBBx+UVLB6TE4VK1Z06fg953XIfs/PWS+wN4Gwl9c+Qto999yjypUrq2/fvlq5cmWhy5hTfoHb+fPndd999yk1NVWLFy926evjWirKY8oezvbu3dulvjBt2jSlpaXp7NmzOnHihC5evFhk9bKc62/RooXL+levXu2or9j3f1Gv/2puv/12tWrVyulHuvLA+uKLLzr6TCxfvrzCw8OVnJx81fMlJiZGTzzxhKZNm6by5csrMTFR7733ntPfFKb+5on87reeHBfuPsMePnxY1atXdzluC7L/6tWrp5o1azq9BDBnzhyVL1/eqYPfgu6rwoqMjNSSJUuUlpamNm3aKCYmRk8//bQmTpwoSU7XC/t9JXvga///xYsXXb5ADw0NVatWrdSuXTuNHz9eTz75pNq1a5fnMOzx8fGaMGGCPvroI91444167bXXPPosRdqnh71jxgcffDDPdvPZ2+N2795dSUlJ2r59u+rXr6+5c+eqZcuWTu0J7UNo5pU822827qpZs6a2bdumX3/91aUyXli+vr5Fujx35XWzyKvjLn9/f5ebcWZmplq3bq3Tp0/rmWeeUc2aNRUUFKQjR46oT58+13woULu8tqG7bynkVK9ePS1ZskQrV67UpEmTNHHiREebvPzYO7r98ssvdfDgQTVo0EBBQUFKSEjQO++8o3Pnzmnbtm0uHd9eS0W9fXKaMWOGLl26pPvuu89p+gMPPCDpSoiU81u4wsjv82RlZclms2nFihW5zluUlTNvnb/SlWNs586deuutt5ym16hRQ7Vq1SrSNs+Se8fR1a4ruf19Xsu81sesN7Rr106zZ8/Wq6++qo8++sjletq1a1clJCTo008/1erVq/X6669r/PjxWrhwYZGcP/ZvxnNrfx4REVFkQV3Tpk114MABLV68WKtXr9a0adP0z3/+U1OmTFG/fv2u+rcbNmzQAw88oKZNm2rSpEmKiopSyZIlNX36dP3rX/9ymb+oO0m+FoYNG6b7779fixYt0qpVq/TCCy9o3Lhx+vzzz3XLLbcUevnlypWT5Bo+5re/L1++rPPnzzt9a5yfsmXLKiAgQKVLl3Y5R+098Z85c8YpzLSX66/s8yG7/B6Mk5OT1axZM4WGhmrMmDGKjY1VQECAtm7dqmeeeeYvq8e4KyIiQtu3b9eqVau0YsUKrVixQtOnT1evXr1y7diwIMqVK3fV60F6ero6duyoH374QatWrfrLhzE9c+aMSpUqVSTnv33/vv7663n2axQcHHxNOn3Ovv5Zs2YpMjLS5fclShS/ATOHDh2q6dOna9iwYWrUqJHCwsJks9nUvXv3fM+XN998U3369HHcHx599FGNGzdO33zzjSpXrvyX1d/yu9+6e1zY37Rz9xm2KHXr1k1jx47VyZMnFRISoiVLlqhHjx5Ox0xh9lVhNW3aVAcPHtSOHTt0/vx51atXz/FmTfbn8IoVK2rfvn25dsov5f/FWseOHfXQQw9p9uzZqlevnsvvd+3apWeeeUZ33XWXBg4c6PF9t0jPwPDwcIWEhCgzM9OtZgTt27fXgAEDHOnWTz/9pBEjRjjNExsbq3PnzhVZB4r333+//v3vf+vjjz92WVdO9o6ffvzxR5fhtH788Ue3O4YqCPuyc46okNs0+7e0OXu7za/JQ3Y7duzQTz/9pJkzZ6pXr16O6bm9duluIh8eHq5SpUrpxx9/dPnd3r175ePjU+TBU05lypTR/fffr/vvv1/Lli3TggUL3A49oqOjFR0drQ0bNujgwYOOJgBNmzbVE088oXnz5ikzM1NNmza96nKqVKmS6+vhuW0Xb49Vf+zYMRljXAIze+/OBWmSUBixsbEyxigmJsbjgLNKlSpat26dLly44PS2R27nlLuu1f45duyYpNyDysuXL//l2126cu7k1oP24cOHXd64+CtkZWXp4MGDTsfBTz/9JEl5vp7pyTUoNjZWO3fudKss7du3V5s2bdSnTx+FhIRo8uTJLvNERUVp0KBBGjRokI4fP64GDRpo7NixRRJ6xMfHS5LL6CLSlaaRNWvWLPQ67MqWLaukpCQlJSXp3Llzatq0qUaNGuUIPfI6JxYsWKCAgACtWrXKacjH6dOnu73uovz2184Yo/379zsqr57e52NjY/Xkk0/qySef1L59+1S/fn29+eab+vjjjwtdVvt++/nnn52mV6xY0dFRYE6///67AgIC3G62a+fj46P69evru+++U3p6ulNzN3tlNuebbz///LN8fHw8vhbnJiIiQgEBAW7VcapUqaKsrCzt27fP8baEdOW6mZyc7NhH69ev16lTp7Rw4UKn+3LO7VkYv//+u86fP+/0tkfO61CVKlX0ww8/uHS2Z29ek/2Y8vPzc9RRsrKyNGjQIE2dOlUvvPBCod/qla4cUwsWLMj1d1lZWerVq5c+++wzzZ071/Fm71/p559/dtqnhREbGyvp/749zkt4eLgCAwPdrpfllNd1yb7+iIiIq67fvv8Luv6iNH/+fPXu3VtvvvmmY9qlS5fyHD0jp7p166pu3br6xz/+oY0bN+rOO+/UlClT9PLLLxeq/uapq91vPTku3H2GrVKlinbu3OkyKmRB91+3bt00evRoLViwQBUqVFBKSorLaCmF3VeF5evr6xQa2ZsVZ99W8fHx2rdvn44cOeJUT8zrnpJTWlraVd8EXr16tS5duqQPP/ywQM/gRdq8xdfXV506ddKCBQtyrTzmHKGkdOnSSkxM1Ny5czV79mz5+fmpffv2TvN07dpV//3vf7Vq1SqX5SUnJ3v8MNC5c2fVrVtXY8eO1X//+1+X36empjqaQNx6662KiIjQlClTnJLhFStWaM+ePS69zxelihUrqk6dOvroo4+c2jd98cUX2rFjh9O8VapUka+vr0v/Jvb2Vu6wp7DZv301xuQ6dJr9Zp/fiebr66s2bdpo8eLFTq+fHzt2TP/617/UpEmTAvUAX9Aha6Ojoz2+OCQkJOjzzz/Xpk2bHKFH/fr1FRISoldffdUxDOPV3Hvvvfrmm2+0adMmx7QTJ07ok08+cZk3KCjomlzA3B0y8cYbb5QxxqnZmST9+9//lqQi+TbTEx07dpSvr69Gjx7t8maAMcaRzOcmMTFRly9f1gcffOCYlpWVVaj+Duzhibv7yN0ha+0VgtmzZztN37p1q3788ce/fLtLVypw33zzjdLT0x3Tli5dWqRDzXkq+1Dmxhi9++67KlmypFq2bJnr/J5cgzp16qTvv/8+12EDc3srpVevXnrnnXc0ZcoUPfPMM47pmZmZLtemiIgIVaxYMd9vGN0dQu6mm25SvXr1tHjxYqf+FVavXq1ff/1VrVu3vurfuyvn+RUcHKzq1as7fY687ge+vr6y2WxOQd6hQ4e0aNEit9fv6fmWm48++sipSev8+fP1xx9/OMInd+/zFy5c0KVLl5yWHRsbq5CQkHz3q7vX30qVKumGG27Q5s2bXX7XrVs3/frrr05fRJw8eVKLFy9WixYtCtSTfbdu3ZSZmen0VsGlS5f0ySefqHbt2k59bUhXeumPi4vz6I2SvPj6+qpVq1ZatGiRU1v8/fv3uzSjvPfeeyXJZbQ++5tx9n2UWz0mPT3do7pQfjIyMpyGNk1PT9fUqVMVHh7uqAvce++9Onr0qNPr6hkZGZo4caKCg4Md4ULO88vHx8cRxl3tmPJkyNpGjRrpzJkzuTavGzp0qObMmaNJkyY5NSv/K23dulWNGzcukmXFx8crNjZWb7zxRq79AtifQXx9fZWYmKhFixY5NeHas2dPrs8aOeV1zUtMTFRoaKheeeWVXK/h9vVHRUWpfv36mjlzptO9Ys2aNdq9e3f+H1RFN2Str6+vy/1t4sSJ+Q7xnJKS4vL8VbduXfn4+DiO3cLU39zlzv3Wk+PC3WfYe++9V7///rvTEOwXLlzQ+++/X6DPUatWLdWtW1dz5szRnDlzFBUV5fKFakH3VV7crW/k5sSJExo/frxuvvlmp9DD3gfJhx9+6JiWlZWl6dOnq2zZso5rZHJycq7rnTZtmqQr9+Xc2PtgK+gX5gV60+P//b//l2u7w8cee0yvvvqq1q1bpzvuuEP9+/dX7dq1dfr0aW3dulVr167V6dOnnf6mW7duevDBBzVp0iQlJia6DF/09NNPa8mSJbrvvvvUp08fxcfH6/z589qxY4fmz5+vQ4cOefSqZcmSJbVw4UK1atVKTZs2VdeuXXXnnXeqZMmS2rVrl/71r3+pTJkyGjt2rEqWLKnx48crKSlJzZo1U48ePRxD2VWtWlWPP/54QTaf21555RW1a9dOd955p5KSknTmzBm9++67qlOnjtOJGxYWpi5dumjixImy2WyKjY3V0qVLPWovV7NmTcXGxuqpp57SkSNHFBoaqgULFuT6KpL9oH300UeVmJgoX19fl0TS7uWXX9aaNWvUpEkTDRo0SCVKlNDUqVOVlpbmcVssu4IOWevj4+PxK/UJCQn65JNPZLPZHM1dfH191bhxY61atUrNmzd3GcYpp+HDh2vWrFm6++679dhjjzmGrLV/G5RdfHy8Jk+erJdfflnVq1dXRESEy7ePBeHukIl9+vTRG2+8oQEDBmjbtm2Ki4tzDCUbFxfn1Mnj+vXr3R42r6BiY2P18ssva8SIETp06JDat2+vkJAQ/fzzz/r000/1yCOP6Kmnnsr1b9u3b6/bb79dTz75pPbv36+aNWtqyZIljmtQQb5FDgwMVO3atTVnzhzdeOONKlu2rOrUqZPnK8HuDlkbHx+v1q1ba+bMmUpJSVGbNm30xx9/aOLEiQoMDHR5O8lms3k0XF1B9OvXT/Pnz9fdd9+trl276sCBA/r4448d35r81QICArRy5Ur17t1bd9xxh1asWKFly5bpueeeu+q3B+5eg55++mnNnz9fXbp0Ud++fRUfH6/Tp09ryZIlmjJlSq6vWg4ZMkQpKSl6/vnnFRYWpueee06pqamqXLmyOnfurHr16ik4OFhr167Vd9995/QNTW7cHUJOkv75z3+qdevWatKkiQYMGKCzZ8/qrbfe0o033ujUUfWhQ4cUExOj3r17a8aMGVddZk61a9dW8+bNFR8fr7Jly2rz5s2OoQHt8roftG3bVm+99Zbuvvtu9ezZU8ePH9d7772n6tWru90viKfnW27Kli2rJk2aKCkpSceOHdOECRNUvXp1RwfH7t7nf/rpJ7Vs2VJdu3ZV7dq1VaJECX366ac6duxYnvc/O3evv9KVplOffvqpyzeII0aM0Ny5c9WpUyc98cQTCgsL05QpU3T58mW98sorTsuwHzt59XVjN2DAAE2bNk2DBw/WTz/9pOjoaM2aNUuHDx/Wf/7zH6d5L1++rC+++EKDBg1yml6Y+8CoUaO0evVq3XnnnRo4cKAyMzMddZzt27c75qtXr5569+6t999/39GEZdOmTZo5c6bat2+vu+66S5LUuHFjlSlTRr1799ajjz4qm82mWbNmFWlTuooVK2r8+PE6dOiQbrzxRs2ZM0fbt2/X+++/7+jI+ZFHHtHUqVPVp08fbdmyRVWrVtX8+fP19ddfa8KECY63cvr166fTp0+rRYsWqly5sg4fPqyJEyeqfv36V337wZMha9u2basSJUpo7dq1Tp0sTpgwQZMmTVKjRo1UqlQplzeVOnTo4Hi492Qff/nll44v4E6cOKHz5887OsVt2rSp04Pcli1bdPr0abVr185pGaNGjXLrvpmTj4+Ppk2bpnvuuUdxcXFKSkpSpUqVdOTIEa1bt06hoaGO43r06NFauXKlEhISNGjQIEcoFRcXl+/1qX79+vL19dX48eN19uxZ+fv7q0WLFoqIiNDkyZP10EMPqUGDBurevbvCw8P1yy+/aNmyZbrzzjsdwf24cePUtm1bNWnSRH379tXp06cd679aR452ng5Zm5f77rtPs2bNUlhYmGrXrq3//ve/Wrt2raOpXV4+//xzDRkyRF26dNGNN96ojIwMzZo1yxEcSJ7V3wp6HXHnfuvJceHuM2z//v317rvvqlevXtqyZYuioqI0a9Ysj4eNza5bt2568cUXFRAQoIcfftglyC7ovsqLJ/WNZs2aqVGjRqpevbqOHj2q999/X+fOndPSpUudytmuXTu1bNlS48aN08mTJ1WvXj0tWrRIX331laZOnep463P9+vV69NFH1blzZ9WoUUPp6enasGGDFi5cqFtvvdXRD1NO9mt5gYar/XMBbrva0EeSzK+//mqMMebYsWNm8ODB5oYbbjAlS5Y0kZGRpmXLlub99993WWZKSooJDAx0Gd4ru9TUVDNixAhTvXp14+fnZ8qXL28aN25s3njjDachEuXB0HZnzpwxL774oqlbt64pVaqUCQgIMHXq1DEjRowwf/zxh9O8c+bMMbfccovx9/c3ZcuWNX/7299chsvp3bu3CQoKclmPfcjK7NwdstYYY2bPnm1q1qxp/P39TZ06dcySJUtMp06dTM2aNZ3mO3HihOnUqZMpVaqUKVOmjBkwYIDZuXOnyzLzKqcxV4bratWqlQkODjbly5c3/fv3N99//73LMjIyMszQoUNNeHi4sdlsTp8vt32wdetWk5iYaIKDg02pUqXMXXfdZTZu3Og0T17DIec29FJBh6xt0aKFiY2N9ehvdu3aZfTnEJ/Zvfzyy0ZSrkO75TbU5w8//GCaNWtmAgICTKVKlcxLL71kPvzwQ5fhoo4ePWratm1rQkJCnIYk82T75MaTIWt/++0307dvXxMTE2P8/PxMVFSU6d+/v9OwoMYY85///CfPoQ3zYz8vci7T/jlzDqG1YMEC06RJExMUFGSCgoJMzZo1zeDBg82PP/7omCe3IQhPnDhhevbsaUJCQkxYWJjp06eP+frrr40kM3v2bKe/dff83bhxo4mPjzd+fn75XnM8GbL2woULZsyYMaZ27domMDDQhIWFmfvuu89lONTU1FQjyXTv3j3fZeaUfcjV7PK6/rz55pumUqVKxt/f39x5551m8+bNeQ5Zm3OZeR2zee37q7HvnwMHDpg2bdqYUqVKmQoVKpiRI0eazMxMp3kLeg0yxphTp06ZIUOGmEqVKhk/Pz9TuXJl07t3b8fwdXl91uHDhxtJ5t133zVpaWnm6aefNvXq1TMhISEmKCjI1KtXz0yaNMmtz5nb8Z+XNWvWmIYNG5qAgABTtmxZ89BDD7ncv3bs2JHncHH5efnll83tt99uSpcubQIDA03NmjXN2LFjne67V7sffPjhh6ZGjRrG39/f1KxZ00yfPj3Xc0pSnkMOenK+ZWffV//+97/NiBEjTEREhAkMDDRt27Z1GqbSLr/7/MmTJ83gwYNNzZo1TVBQkAkLCzN33HGH05CFefHk+msfnjbn8IDGGHPgwAHToUMHExoaagIDA02LFi3Mpk2bXOYrX768adiwoVvrO3bsmOndu7cpW7as8ff3N3fccYdZuXKly3wrVqwwksy+ffucphfmPmCMMZ999pm55ZZbjJ+fn4mNjTXTpk0zTz75pAkICHCa7/Lly2b06NEmJibGlCxZ0txwww1mxIgRTsPQGmPM119/bRo2bGgCAwNNxYoVzfDhw82qVatcrsMFHbI2Li7ObN682TRq1MgEBASYKlWqmHfffddl3mPHjpmkpCRTvnx54+fnZ+rWretyfZ0/f75p06aNiYiIMH5+fiY6OtoMGDDA5RzOyZMha40x5oEHHjAtW7Z0mma/1uT1k/0a5Mk+tp/fuf3kLO8zzzxjoqOjnYYRNcaYJ5980thsNrNnzx63Pl9O27ZtMx07djTlypUz/v7+pkqVKqZr167ms88+c5rviy++cFxbqlWrZqZMmZJnnT1nne6DDz4w1apVcwxxm/3YWrdunUlMTDRhYWEmICDAxMbGmj59+pjNmzc7LWPBggWmVq1axt/f39SuXdssXLjQ7eMye/3wavK6D9udOXPGcZwGBwebxMREs3fvXpfPnLOuefDgQdO3b18TGxvruP/cddddZu3atS7rcKf+VtDriCf3W3ePC3efYQ8fPmweeOABU6pUKVO+fHnz2GOPmZUrV7pd58tp3759jnPlq6++cvl9QfdVXjypbzz++OOmWrVqxt/f34SHh5uePXuaAwcO5Dpvamqqeeyxx0xkZKTj2pfz+X7//v2mV69eplq1aiYwMNAEBASYuLg4M3LkSHPu3Lk8yzF8+HDj6+ubb3nz4lHogeKhXr16plWrVt4uxnXnoYceMr6+vmbp0qXmjz/+cHlYgmeefvppU7lyZZdKZ3H36aef5nlTuR4sW7bM2Gw288MPP3i7KLgOvPfeeyYoKMgcPXrU20X5S+UVUF0PWrRoYR588MEC/a09qF+6dGmRlqldu3amffv2LtOvxX2gXbt2pnr16kW2vKJiDz2uN19++aXx8fExP/30U4H+/lrs40uXLpnIyEgzYcIEl9/ddtttpnPnzkW2LhR/12t9En+NU6dOmR9++MHcfPPNJjo6usDLKdI+PVC0cuvAcP369fr+++89euUPVwwcOFDBwcG67777FBUVVSTtIf+XrVu3Ti+88IJTJ4XFTfbxw6Ur7T8nTpyo0NBQNWjQwEulKpx169ape/fuqlu3rreLguvAunXr9Oijj+Y68geKp1deeUVz5szxqDNyu3Xr1qlRo0ZF2ufYnj17tHTpUr300ku5rq8w94Gc1+h9+/Zp+fLl1HGKUEJCgtq0aVPgJsXX4l4/ffp0lSxZUn//+9+dpqekpOj777/XmDFjimxdKP6uh/okvKdBgwa6+eabtWvXLj399NMFXo7NmOt43ECLO3TokFq1aqUHH3xQFStW1N69ezVlyhSFhYVp586dBW7H9b/s0qVL2r17t5KTk9W4cWMFBAR4u0i4hvr166eLFy+qUaNGSktL08KFC7Vx40a98sor+Y7ehGvv7NmzLg89OeU29B/+d6Wnp7v0DZZTWFiYvv32W911112aN2+eOnfu/BeVDp6KiopSnz59VK1aNR0+fFiTJ09WWlqatm3bpho1avwlZTh9+rRTh805+fr6Kjw8XM2bN9fJkyfdHuUJwP+uixcv5jvgQtmyZfPtFxDSxo0bZbPZVLNmTceIpQVR/AaNhkOZMmUUHx+vadOm6cSJEwoKClLbtm316quvEngUUEBAwHX7DT8816JFC7355ptaunSpLl26pOrVq2vixIlOHTHCex577DGnkSNyQy6P7DZu3OjouDIv06dPz7djNhQPd999t/7973/r6NGj8vf3V6NGjfTKK6/8ZYGHdGWUiS+++CLP31epUiXfTmEBILs5c+YoKSnpqvN42lnv/6qiGuGJNz0AAF6xe/dup+Eqc5N9ODTgzJkz2rJly1XniYuLU1RU1F9UIlzvtmzZkutIdXaBgYG68847/8ISAbje/fHHH9q1a9dV54mPjy/UmwvwDKEHAAAAAACwJDoyBQAAAAAAlkToAQAAAAAALImOTAEADv379/d2EQCgSHzwwQfeLgIAoBjgTQ8AAAAAAGBJhB4AAAAAAMCSCD0AAAAAAIAlEXoAAAAAAABLIvQAAAAAAACWROgBAAAAAAAsidADAAAAAABYEqEHAAAAAACwJEIPAAAAAABgSYQeAAAAAADAkgg9AAAAAACAJRF6AAAAAAAASyL0AAAAAAAAlkToAQAAAAAALInQAwAAAAAAWBKhBwAAAAAAsCRCDwAAAAAAYEmEHgAAAAAAwJIIPQAAAAAAgCURegAAAAAAAEsi9AAAAAAAAJZE6AEAAAAAACyJ0AMAAAAAAFgSoQcAAAAAALAkQg8AAAAAAGBJhB4AAAAAAMCSCD0AAAAAAIAlEXoAAAAAAABLIvQAAAAAAACWROgBAAAAAAAsidADAAAAAABYEqEHAAAAAACwJEIPAAAAAABgSYQeAAAAAADAkgg9AAAAAACAJRF6AAAAAAAASyL0AAAAAAAAllTC2wUAAOB6ZWxG2+7bJmMz3i6Ki4p7KyrqpyhvFwMAAMCrCD0AANeFtLQ0nTp1ytvFcGJ8jba13SbjW/xCD1uWTWV3lC1228xmsykyMlInTpxQRkaGt4vjJCQkRH5+fsV2m9lsNm8XBQCA6w6hBwDgunDq1CktX77c28VwVkJSliRfbxckd8Vxm9lsNiUlJWn9+vVKSUnxdnGcNGjQQJGRkcV2mxF6AADgOfr0AAAAAAAAlkToAQAAAAAALInQAwAAAAAAWBKhBwAAAAAAsCRCDwAAAAAAYEmEHgAAAAAAwJIIPQAAAAAAgCURegAAAAAAAEsi9AAAAAAAAJZE6AEAAAAAACyJ0AMAAAAAAFgSoQcAAAAAALAkQg8AAAAAAGBJhB4AAAAAAMCSCD0AAAAAAIAlEXoAAAAAAABLIvQAAAAAAACWROgBAAAAAAAsidADAAAAAABYEqEHAAAAAACwJEIPAAAAAABgSYQeAAAAAADAkgg9AAAAAACAJRF6AAAAAAAASyL0AAAAAAAAlkToAQAAAAAALInQAwAAAAAAWBKhBwAAAAAAsCRCDwAAAAAAYEmEHgAAAAAAwJIIPQAAAAAAgCURegAAAAAAAEsi9AAAAAAAAJZE6AEAAAAAACyJ0AMAAAAAAFhSCW8XAAAAd9hsNtlsNm8Xw5mPpExJxaxYkmQztmK5zXx8fBz/Frey2bdXcSuXfZsBAADP2YwxxtuFAAAUD/379/d2EfJkjFGxvGUV068PbMYmZalYbjObzVZsyyUV321W3MKY4u6DDz7wdhEAAMVAMa2qAQDg7MSJE1q/fr23i+HEx8dHHTt21KpVq5Samurt4jiJi4tTXFxcsX1QLq7lOn78eLE9zorrNgMAoDgj9AAAXBcyMjKUkpLi7WI4sT+EpqamFruypaWlebsI16XifJwBAADP0UgUAAAAAABYEqEHAAAAAACwJEIPAAAAAABgSYQeAAAAAADAkgg9AAAAAACAJRF6AAAAAAAASyL0AAAAAAAAlkToAQAAAAAALInQAwAAAAAAWBKhBwAAAAAAsCRCDwAAAAAAYEmEHgAAAAAAwJIIPQAAAAAAgCURegAAAAAAAEsi9AAAAAAAAJZE6AEAAAAAACyJ0AMAAAAAAFgSoQcAAAAAALAkQg8AAAAAAGBJhB4AAAAAAMCSCD0AAAAAAIAlEXoAAAAAAABLIvQAAAAAAACWROgBAAAAAAAsidADAAAAAABYEqEHAAAAAACwJEIPAAAAAABgSYQeAAAAAADAkgg9AAAAAACAJRF6AAAAAAAASyL0AAAAAAAAlkToAQAAAAAALInQAwAAAAAAWBKhBwAAAAAAsCRCDwAAAAAAYEklvF0AAADcERISogYNGni7GE5sNptsNpvi4uKUlpbm7eI4iYqK8nYRrkvF+TgDAACeI/QAAFwX/Pz8FBkZ6e1iOLE/iIaHhysjI8PLpXEWHBzs7SJc1fHjx4vdNgsJCSnWx1lx3WYhISHeLgYAAHki9AAAXBdOnTql5cuXe7sYTmw2m5KSkrR+/XqlpKR4uzhOGjRoUOzeWMiuuG6zyMhIjjMPFPfjDAAA+vQAAAAAAACWROgBAAAAAAAsidADAAAAAABYEqEHAAAAAACwJEIPAAAAAABgSYQeAAAAAADAkgg9AAAAAACAJRF6AAAAAAAASyL0AAAAAAAAlkToAQAAAAAALInQAwAAAAAAWBKhBwAAAAAAsCRCDwAAAAAAYEmEHgAAAAAAwJIIPQAAAAAAgCURegAAAAAAAEsi9AAAAAAAAJZE6AEAAAAAACyJ0AMAAAAAAFgSoQcAAAAAALAkQg8AAAAAAGBJhB4AAAAAAMCSCD0AAAAAAIAlEXoAAAAAAABLIvQAAAAAAACWROgBAAAAAAAsidADAAAAAABYEqEHAAAAAACwJEIPAAAAAABgSYQeAAAAAADAkgg9AAAAAACAJRF6AAAAAAAASyL0AAAAAAAAlkToAQAAAAAALInQAwAAAAAAWFIJbxcAAAB32Gw22Ww2bxfDiY+Pj+Pf4la24laenIrrNiuux5nNZpOvr6/jmCsuilt5AADIyWaMMd4uBACgeOjfv7+3i5AnY4yK4y3LZrMV23IVt4f37LKysrxdBBf27VXc9qfNZlPdunWLXbmk4n2cPfroo94uAgCgGOBNDwDAdeHEiRNav369t4vhxMfHRx07dtSqVauUmprq7eI4iYuLU3h4ONvMA8V1m/n6+qpOnTratm2bLl265O3iOImOjlZ0dLS3iwEAQJ4IPQAA14WMjAylpKR4uxhO7N9wp6amFruypaWlsc08VFy3mb0JyaVLl3ThwgUvl8bZ5cuXvV0EAACuioaYAAAAAADAkgg9AAAAAACAJRF6AAAAAAAASyL0AAAAAAAAlkToAQAAAAAALInQAwAAAAAAWBKhBwAAAAAAsCRCDwAAAAAAYEmEHgAAAAAAwJIIPQAAAAAAgCURegAAAAAAAEsi9AAAAAAAAJZE6AEAAAAAACyJ0AMAAAAAAFgSoQcAAAAAALAkQg8AAAAAAGBJhB4AAAAAAMCSCD0AAAAAAIAlEXoAAAAAAABLIvQAAAAAAACWROgBAAAAAAAsidADAAAAAABYEqEHAAAAAACwJEIPAAAAAABgSYQeAAAAAADAkgg9AAAAAACAJRF6AAAAAAAASyL0AAAAAAAAlkToAQAAAAAALInQAwAAAAAAWBKhBwAAAAAAsCRCDwAAAAAAYEmEHgAAAAAAwJIIPQAAAAAAgCURegAAAAAAAEsq4e0CAADgjpCQEDVo0MDbxXBis9lks9kUFxentLQ0bxfHSVRUlIKDg9lmHiiu28zHx0c2m03R0dG6fPmyt4vjpEyZMt4uAgAAV0XoAQC4Lvj5+SkyMtLbxXBis9kkSeHh4crIyPByaZwFBwezzTxU3LdZaGiosrKyvFwaZ4GBgd4uAgAAV0XoAQC4Lpw6dUrLly/3djGc2Gw2JSUlaf369UpJSfF2cZw0aNBAkZGRbDMPFNdt5uPjo1atWmnnzp26cOGCt4vjJDY2VrGxsd4uBgAAeaJPDwAAAAAAYEmEHgAAAAAAwJIIPQAAAAAAgCURegAAAAAAAEsi9AAAAAAAAJZE6AEAAAAAACyJ0AMAAAAAAFgSoQcAAAAAALAkQg8AAAAAAGBJhB4AAAAAAMCSCD0AAAAAAIAlEXoAAAAAAABLIvQAAAAAAACWROgBAAAAAAAsidADAAAAAABYEqEHAAAAAACwJEIPAAAAAABgSYQeAAAAAADAkgg9AAAAAACAJRF6AAAAAAAASyL0AAAAAAAAlkToAQAAAAAALInQAwAAAAAAWBKhBwAAAAAAsCRCDwAAAAAAYEmEHgAAAAAAwJIIPQAAAAAAgCURegAAAAAAAEsi9AAAAAAAAJZE6AEAAAAAACyJ0AMAAAAAAFgSoQcAAAAAALAkQg8AAAAAAGBJhB4AAAAAAMCSCD0AAAAAAIAllfB2AQAAcIfNZpPNZvN2MZz4+Pg4/i1uZbNvr+JWLraZ5+zbrDiWrbiVBwCAnGzGGOPtQgAAiof+/ft7uwh5MsaoON6ybDZbsS2XpGJbtuJaLqn4bTObzaabb75ZUvEsW3ENPoYOHertIgAAigHe9AAAXBdOnDih9evXe7sYTnx8fNSxY0fHN/HFzfHjx4vtNlu1apVSU1O9XRwncXFxCg8PL3bbzNfXV3Xq1NG2bdt06dIlbxfHSXR0tKKjo71dDAAA8kToAQC4LmRkZCglJcXbxXBSXL/htivO2yw1NbXYlS0tLa1YbjN7qHbp0iVduHDBy6VxdvnyZW8XAQCAqyqeX00BAAAAAAAUEqEHAAAAAACwJEIPAAAAAABgSYQeAAAAAADAkgg9AAAArjMhkh6WVE1SSS+XBQCA4ozRWwAAAK4zAZIaSqoo6aCkXyUd/vMHAAD8H0IPAACA64yvpMg/f6pI+k3Sz5J+knRU0klJ5yVleauAAAAUE4QeAAAA17EKf/7ES7oo6UtJW3QlBEmVlCYpw2ulAwDAuwg9AAAALCJQUuKfPz9L2qQrAcheScaL5QIAwFsIPQAAACyoqqRoSffpSnOX7yQtl3RaUqb3igUAwF+K0AMAAMCCbLrS90egrnR42kZXOj/dJWm7rvT/cdxbhQMA4C9C6AEAAGBhPn/+lJQUKslfUiVJt+pKp6e7dWUEmAui41MAgPUQegAAAPwPCf/z5yZJKbrSCWpFSYcknZJ0Tlc6RAUAwAoIPQAAAP4HlZRUTlKrP3/+K2mnpAO6MgRumqRLXisdAABFg9ADAAAAavTnzzldGfnlW0nLxHC3AIDrG6EHAAAAHIIkxUmqJamtpB90ZejbPZJSvVguAAAKgtADAAAADrZsP366Uln09WqJAAAoOEIPAAAAOKTpSoemhyVtkXRMV0Z5SfNmoQAAKCBCDwAAAOikpLN//ntI0k+SNothbAEA1zdCDwAAgP9BRlcCjUuS0nWl49IdujJ6y1EvlgsAgKJE6AEAAPA/xPz5b7qk45JW6spwtaclZXqrUAAAXCOEHgAAAP9D7KOx7JB0RFeCDsIOAIBVEXoAAABYWLqkZEm7JW2Q9IekFEkXJF32XrEAAPhLEHoAAABY0ClJv+pKp6SHdaWfjgO60oeHyfvPAACwFEIPAAAAi8iUdEJX3uz4WdIeSfsk/ebFMgEA4E2EHgAAANexTEkZutJUJVXSOl0ZavawrjRtAQDgfxmhBwAAwHXI3kTlsKRdkrb8+QMAAP4PoQcAAMB15pKuDDO7XNLBP//PCCwAALgi9AAAALjOpEqaKemM6JgUAICrIfQAAAC4zmRK+t3bhQAA4Drg4+0CAAAAAAAAXAuEHgAAAAAAwJIIPQAAAAAAgCURegAAAAAAAEsi9AAAAAAAAJbE6C0AgOtCSEiIGjRo4O1iOLHZbLLZbN4uRp6K8zaLi4tTWlqat4vjJCoqSsHBwcVum/n4+Mhmsyk6OlqXL1/2dnGclClTxttFAADgqgg9AADXheL4AF/cFedtFhcX5+0i5Kk4bjN76AEAADxD8xYAAAAAAGBJhB4AAAAAAMCSCD0AAAAAAIAlEXoAAAAAAABLIvQAAAAAAACWROgBAAAAAAAsidADAAAAAABYEqEHAAAAAACwJEIPAAAAAABgSYQeAAAAAADAkgg9AAAAAACAJRF6AAAAAAAASyL0AAAAAAAAlkToAQAAAAAALInQAwAAAAAAWBKhBwAAAAAAsCRCDwAAAAAAYEmEHgAAAAAAwJIIPQAAAAAAgCURegAAAAAAAEsi9AAAAAAAAJZE6AEAAAAAACyJ0AMAAAAAAFgSoQcAAAAAALAkQg8AAAAAAGBJhB4AAAAAAMCSCD0AAAAAAIAlEXoAAAAAAABLIvQAAAAAAACWROgBAAAAAAAsidADAAAAAABYEqEHAAAAAACwJEIPAAAAAABgSYQeAAAAAADAkgg9AAAAAACAJRF6AAAAAAAASyL0AAAAAAAAlmQzxhhvFwIAAAAAAKCo8aYHAAAAAACwJEIPAAAAAABgSYQeAAAAAADAkgg9AAAAAACAJRF6AAAAAAAASyL0AAAAAAAAlkToAQAAAAAALInQAwAAAAAAWBKhBwAAAAAAsKT/D6Dr3A6qSLHxAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate a random level and visualize it\n",
    "random_cnf = random_config(8)\n",
    "print(\"random_cnf:\", random_cnf)\n",
    "\n",
    "print_level_from_config(random_cnf)\n",
    "print_level_from_config(random_cnf)\n",
    "print_level_from_config(random_cnf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rewards:\n",
      "DR: 13.45\n",
      "PLR: 0.99\n",
      "ACCEL: 8.73\n",
      "ACCEL-EasyStart: 11.57\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "def test_model(model, config, gif_path=\"level.gif\"):\n",
    "    \"\"\"Evaluate a model on a given environment instance.\"\"\"\n",
    "    env = MyCustomGrid(config, render_mode='rgb_array', solvable_only=True)\n",
    "    obs, _ = env.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    total_reward = 0\n",
    "    i = 0\n",
    "\n",
    "    frames = []  # List to store frames\n",
    "\n",
    "    # Continue until either terminated or truncated is True\n",
    "    while not (terminated or truncated):\n",
    "        frame = env.render()  # Capture frame as an image\n",
    "        frames.append(Image.fromarray(frame))  # Convert to PIL image and store\n",
    "        \n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        i += 1\n",
    "        if i > 20:\n",
    "            break\n",
    "    \n",
    "    # Add to the gif also the last frame\n",
    "    frame = env.render()\n",
    "    frames.append(Image.fromarray(frame))\n",
    "\n",
    "    # Save frames as a GIF\n",
    "    if frames:\n",
    "        frames[0].save(\n",
    "            gif_path, save_all=True, append_images=frames[1:], duration=500, loop=0\n",
    "        )\n",
    "\n",
    "    return total_reward  # Do not close env here, as we reuse it\n",
    "\n",
    "\n",
    "def load_models(grid_size):\n",
    "    \"\"\"Load the RL models.\"\"\"\n",
    "    model_dr = PPO.load(f\"models/dr_model_{grid_size}x{grid_size}\")\n",
    "    model_plr = PPO.load(f\"models/plr_model_{grid_size}x{grid_size}\")\n",
    "    model_accel = PPO.load(f\"models/accel_model_{grid_size}x{grid_size}\")\n",
    "    model_accel_easy = PPO.load(f\"models/accel_model_easy_{grid_size}x{grid_size}\")\n",
    "    \n",
    "    return model_dr, model_plr, model_accel, model_accel_easy\n",
    "\n",
    "\n",
    "model_dr, model_plr, model_accel, model_accel_easy = load_models(8)\n",
    "\n",
    "rewards = {\"DR\": 0, \"PLR\": 0, \"ACCEL\": 0, \"ACCEL-EasyStart\": 0}\n",
    "\n",
    "# Test the models on the same exact environment instance\n",
    "for i in range(20):\n",
    "    config = random_config(10, seed=42 + i)\n",
    "    #print_level_from_config(config)\n",
    "    # Inizialize the environment for each model    \n",
    "\n",
    "    reward = test_model(model_dr, config, gif_path=f\"gifs/level_{i}_dr.gif\")\n",
    "    #print(f\"Reward for level {i} with DR: {reward:.2f}\")\n",
    "    rewards[\"DR\"] += reward\n",
    "    \n",
    "    reward = test_model(model_plr, config, gif_path=f\"gifs/level_{i}_plr.gif\")\n",
    "    #print(f\"Reward for level {i} with PLR: {reward:.2f}\")\n",
    "    rewards[\"PLR\"] += reward\n",
    "    \n",
    "    reward = test_model(model_accel, config, gif_path=f\"gifs/level_{i}_accel.gif\")\n",
    "    #print(f\"Reward for level {i} with ACCEL: {reward:.2f}\")\n",
    "    rewards[\"ACCEL\"] += reward\n",
    "    \n",
    "    reward = test_model(model_accel_easy, config, gif_path=f\"gifs/level_{i}_accel_easy.gif\")\n",
    "    #print(f\"Reward for level {i} with ACCEL-EasyStart: {reward:.2f}\")\n",
    "    rewards[\"ACCEL-EasyStart\"] += reward\n",
    "\n",
    "print(\"Total rewards:\")\n",
    "for model_name, reward in rewards.items():\n",
    "    print(f\"{model_name}: {reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bipedal Walker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://gymnasium.farama.org/environments/box2d/bipedal_walker/\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.utils import obs_as_tensor\n",
    "import random\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "class BipedalWalkerParamWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    A wrapper around BipedalWalker (or BipedalWalkerHardcore) \n",
    "    that sets custom parameters for terrain generation at reset time.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env_id=\"BipedalWalker-v3\", hardcore=False):\n",
    "        super().__init__(gym.make(env_id))\n",
    "        self.hardcore = hardcore\n",
    "\n",
    "        # The environment's internal parameters. \n",
    "        # We will override these at reset to control terrain generation.\n",
    "        self.config = {\n",
    "            \"seed_val\": None,\n",
    "            \"stump_height\": 0.0,\n",
    "            \"stair_height\": 0.0,\n",
    "            \"stair_steps\": 1,\n",
    "            \"roughness\": 0.0,\n",
    "            \"pit_gap\": 0.0,\n",
    "        }\n",
    "\n",
    "        # If we want hardcore version, you can do so with:\n",
    "        if self.hardcore:\n",
    "            self.env = gym.make(\"BipedalWalkerHardcore-v3\")\n",
    "\n",
    "    def set_config(self, config_dict):\n",
    "        \"\"\"Update environment parameters.\"\"\"\n",
    "        for k, v in config_dict.items():\n",
    "            self.config[k] = v\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Modify the environment's terrain parameters right before reset.\n",
    "        This monkey-patches internal Box2D variables, if needed,\n",
    "        or sets seeds to randomize terrain accordingly.\n",
    "        \"\"\"\n",
    "        # 1) Seed:\n",
    "        #if self.config[\"seed_val\"] is not None:\n",
    "        #    self.env.seed(self.config[\"seed_val\"])\n",
    "\n",
    "        # 2) Override terrain parameters in the underlying env\n",
    "        #    (We rely on the environment reading these at reset or having\n",
    "        #     references in the terrain generation code. Depending on \n",
    "        #     your exact BipedalWalker implementation, you might need \n",
    "        #     to modify the source or do partial overrides.)\n",
    "        self.env.unwrapped.stump_height = self.config[\"stump_height\"]\n",
    "        self.env.unwrapped.stair_height = self.config[\"stair_height\"]\n",
    "        self.env.unwrapped.stair_steps = int(self.config[\"stair_steps\"])\n",
    "        self.env.unwrapped.roughness = self.config[\"roughness\"]\n",
    "        self.env.unwrapped.pit_gap = self.config[\"pit_gap\"]\n",
    "\n",
    "        obs = self.env.reset(**kwargs)\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "    \n",
    "\n",
    "\n",
    "# The min and max values for each terrain parameter.\n",
    "# For simplicity, define them here, but you can store these in a table/dict.\n",
    "PARAM_BOUNDS = {\n",
    "    \"stump_height\":  (0.0, 5.0),\n",
    "    \"stair_height\":  (0.0, 5.0),\n",
    "    \"stair_steps\":   (1,   9),\n",
    "    \"roughness\":     (0.0, 10.0),\n",
    "    \"pit_gap\":       (0.0, 10.0)\n",
    "}\n",
    "\n",
    "def random_config(easy_init=False):\n",
    "    \"\"\"\n",
    "    Sample a random environment config. If easy_init=True,\n",
    "    you might restrict sampling to small values, so the agent\n",
    "    starts with simpler terrain.\n",
    "    \"\"\"\n",
    "    cfg = {}\n",
    "    \n",
    "    # Optionally set a random seed \n",
    "    cfg[\"seed_val\"] = random.randint(0, 1_000_000)\n",
    "\n",
    "    def sample_uniform(low, high):\n",
    "        return random.uniform(low, high)\n",
    "\n",
    "    if easy_init:\n",
    "        # Possibly narrower range for simpler (initial) levels\n",
    "        cfg[\"stump_height\"] = sample_uniform(0.0, 0.4)\n",
    "        cfg[\"stair_height\"] = sample_uniform(0.0, 0.4)\n",
    "        cfg[\"stair_steps\"]  = random.randint(1, 2)\n",
    "        cfg[\"roughness\"]    = sample_uniform(0.0, 0.6)\n",
    "        cfg[\"pit_gap\"]      = sample_uniform(0.0, 0.8)\n",
    "    else:\n",
    "        cfg[\"stump_height\"] = sample_uniform(*PARAM_BOUNDS[\"stump_height\"])\n",
    "        cfg[\"stair_height\"] = sample_uniform(*PARAM_BOUNDS[\"stair_height\"])\n",
    "        cfg[\"stair_steps\"]  = random.randint(*PARAM_BOUNDS[\"stair_steps\"])\n",
    "        cfg[\"roughness\"]    = sample_uniform(*PARAM_BOUNDS[\"roughness\"])\n",
    "        cfg[\"pit_gap\"]      = sample_uniform(*PARAM_BOUNDS[\"pit_gap\"])\n",
    "\n",
    "    return cfg\n",
    "\n",
    "def edit_config(old_cfg):\n",
    "    \"\"\"\n",
    "    Make a small 'mutation' to the old_cfg.\n",
    "    This can be random increments/decrements to each parameter, \n",
    "    or randomly choose one parameter to mutate.\n",
    "    \"\"\"\n",
    "    new_cfg = dict(old_cfg)\n",
    "    param_to_edit = random.choice([\"stump_height\", \"stair_height\", \"stair_steps\", \"roughness\", \"pit_gap\"])\n",
    "    # pick small delta \n",
    "    delta = random.uniform(0.1, 1.0)\n",
    "\n",
    "    # Add or subtract\n",
    "    sign = random.choice([-1, 1])\n",
    "    new_val = new_cfg[param_to_edit] + sign * delta\n",
    "    \n",
    "    # Clip to valid range\n",
    "    low, high = PARAM_BOUNDS[param_to_edit]\n",
    "    new_cfg[param_to_edit] = np.clip(new_val, low, high)\n",
    "\n",
    "    # Possibly update the seed too \n",
    "    new_cfg[\"seed_val\"] = random.randint(0, 1_000_000)\n",
    "    return new_cfg\n",
    "\n",
    "\n",
    "\n",
    "class LevelReplayBuffer:\n",
    "    \"\"\"\n",
    "    Stores (config, score). Score is e.g. a 'regret' approximation.\n",
    "    We keep the highest-score configs up to max_size.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_size=100):\n",
    "        self.max_size = max_size\n",
    "        self.data = []\n",
    "\n",
    "    def add(self, cfg, score):\n",
    "        self.data.append((cfg, score))\n",
    "        # keep only top K by score\n",
    "        self.data.sort(key=lambda x: x[1], reverse=True)\n",
    "        self.data = self.data[:self.max_size]\n",
    "\n",
    "    def sample(self):\n",
    "        if len(self.data) == 0:\n",
    "            return None\n",
    "        # Weighted sample by score or just pick the top \n",
    "        # For simplicity, pick randomly from top half:\n",
    "        half = len(self.data) // 2\n",
    "        idx = np.random.randint(0, max(1, half))\n",
    "        return self.data[idx][0]\n",
    "\n",
    "def estimate_regret(env, model, max_steps=1000, gamma=0.99, lam=0.95):\n",
    "    \"\"\"\n",
    "    Calculate regret using Generalized Advantage Estimation (GAE)\n",
    "    with Stable-Baselines3's PPO model.\n",
    "    \"\"\"\n",
    "    obs = env.reset()[0]\n",
    "    regrets = []\n",
    "    rewards = []\n",
    "    dones = []\n",
    "    values = []\n",
    "    \n",
    "\n",
    "    for t in range(max_steps):\n",
    "        # Add batch dimension to the observation tensor\n",
    "        obs_tensor = torch.as_tensor(obs).float().unsqueeze(0).to(device)\n",
    "        \n",
    "        # Use the model's policy to get the value and action.\n",
    "        # For actions, model.predict handles single observations well.\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        \n",
    "        # Compute the value from the policy.\n",
    "        value_t = model.policy.predict_values(obs_tensor).item()\n",
    "        values.append(value_t)\n",
    "        \n",
    "        # Perform the step in the environment\n",
    "        obs, reward, done, truncated, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "    # Add value of the terminal state (0 if done/truncated)\n",
    "    if done or truncated:\n",
    "        terminal_value = 0.0\n",
    "    else:\n",
    "        terminal_obs_tensor = torch.as_tensor(obs).float().unsqueeze(0).to(device)\n",
    "        terminal_value = model.policy.predict_values(terminal_obs_tensor).item()\n",
    "    values.append(terminal_value)\n",
    "\n",
    "    # Compute TD-errors and GAE-like regret score\n",
    "    for t in range(len(rewards)):\n",
    "        delta_t = rewards[t] + gamma * values[t + 1] * (1 - dones[t]) - values[t]\n",
    "        discounted_error = (gamma * lam) ** t * delta_t\n",
    "        regrets.append(max(0, discounted_error))\n",
    "\n",
    "    # Return the maximum positive regret score (or 0 if empty)\n",
    "    return max(regrets) if regrets else 0.0\n",
    "\n",
    "def run_accel_bipedal(\n",
    "    total_iterations=50,\n",
    "    steps_per_iteration=10000,\n",
    "    replay_prob=0.8,\n",
    "    regret_threshold=1.0,\n",
    "    max_buf_size=100,\n",
    "    easy_start=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Minimal ACCEL training loop on BipedalWalker.\n",
    "    \"\"\"\n",
    "    # 1) Create the environment wrapper + vectorize\n",
    "    def make_env():\n",
    "        env = BipedalWalkerParamWrapper(env_id=\"BipedalWalkerHardcore-v3\", hardcore=True)\n",
    "        return env\n",
    "    \n",
    "    # (Alternatively, do SubprocVecEnv if you want parallel CPU rollouts.)\n",
    "    vec_env = DummyVecEnv([make_env])\n",
    "\n",
    "    # 2) Initialize PPO\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\", \n",
    "        vec_env, \n",
    "        n_steps=2048,        # must be multiple of vec_env.num_envs\n",
    "        batch_size=64,\n",
    "        learning_rate=3e-4,\n",
    "        gamma=0.99,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    # 3) Create LevelReplayBuffer\n",
    "    level_buffer = LevelReplayBuffer(max_size=max_buf_size)\n",
    "\n",
    "    # 4) [Optional] Pre-fill buffer with some easy or random levels\n",
    "    for _ in range(10):  # pre-fill 10 levels\n",
    "        print(\"Pre-filling buffer...\")\n",
    "        cfg = random_config(easy_init=easy_start)\n",
    "        env = make_env()  # fresh environment\n",
    "        env.set_config(cfg)\n",
    "        # Train on it briefly to get a partial updated policy \n",
    "        model.set_env(DummyVecEnv([lambda: env]))\n",
    "        model.learn(total_timesteps=2000)\n",
    "\n",
    "        # Evaluate regret\n",
    "        rew_env = make_env()\n",
    "        rew_env.set_config(cfg)\n",
    "        regret = estimate_regret(rew_env, model)\n",
    "\n",
    "        if regret >= regret_threshold:\n",
    "            level_buffer.add(cfg, regret)\n",
    "        \n",
    "    \n",
    "    print(\"Buffer pre-filled. Starting main loop...\")\n",
    "\n",
    "    # 5) ACCEL main loop\n",
    "    for it in range(total_iterations):\n",
    "        # Decide: replay from buffer or create new\n",
    "        use_replay = (np.random.rand() < replay_prob) and (len(level_buffer.data) > 0)\n",
    "        if use_replay:\n",
    "            cfg = level_buffer.sample()\n",
    "            # Optionally edit the config to keep pushing frontier\n",
    "            cfg = edit_config(cfg)\n",
    "        else:\n",
    "            cfg = random_config(easy_init=False)\n",
    "\n",
    "        # Train on that config\n",
    "        env = make_env()\n",
    "        env.set_config(cfg)\n",
    "        model.set_env(DummyVecEnv([lambda: env]))\n",
    "        model.learn(total_timesteps=steps_per_iteration)\n",
    "\n",
    "        # Now measure regret\n",
    "        rew_env = make_env()\n",
    "        rew_env.set_config(cfg)\n",
    "        regret = estimate_regret(rew_env, model)\n",
    "\n",
    "        if regret >= regret_threshold:\n",
    "            level_buffer.add(cfg, regret)\n",
    "\n",
    "        print(f\"[Iteration {it+1}/{total_iterations}] => Config = {cfg}, Regret = {regret:.3f}, \"\n",
    "              f\"Buffer size = {len(level_buffer.data)}\")\n",
    "\n",
    "    return model, level_buffer\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trained_model, buffer_data = run_accel_bipedal(\n",
    "        total_iterations=250,\n",
    "        steps_per_iteration=1000,\n",
    "        replay_prob=0.8,\n",
    "        regret_threshold=1.0,\n",
    "        max_buf_size=100,\n",
    "        easy_start=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Evaluate final performance on some reference environment\n",
    "    test_env = gym.make(\"BipedalWalkerHardcore-v3\", render_mode='human')\n",
    "    mean_return = 0.0\n",
    "    N = 10\n",
    "    for _ in range(N):\n",
    "        obs = test_env.reset()[0]\n",
    "        done = False\n",
    "        episode_reward = 0.0\n",
    "        while not done:\n",
    "            action, _ = trained_model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, truncated, info = test_env.step(action)\n",
    "            episode_reward += reward\n",
    "            if truncated:\n",
    "                done = True\n",
    "        mean_return += episode_reward\n",
    "    mean_return /= N\n",
    "    print(f\"Avg Return on BipedalWalkerHardcore-v3 over {N} trials: {mean_return}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maze generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomMaze(MiniGridEnv):\n",
    "    \"\"\"\n",
    "    Simple MiniGrid environment that places random wall tiles\n",
    "    according to a config dict, returning only the 'image' observation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config=None, **kwargs):\n",
    "        if config is None:\n",
    "            config = {}\n",
    "        self.config = config\n",
    "\n",
    "        # Extract parameters from config\n",
    "        self.width = config.get(\"width\")\n",
    "        self.height = config.get(\"height\")\n",
    "        self.num_blocks = config.get(\"num_blocks\")\n",
    "        self.custom_seed = config.get(\"seed_val\")\n",
    "        \n",
    "        \n",
    "        # Create a random number generator with the custom seed\n",
    "        self.rng = np.random.default_rng(seed=self.custom_seed)\n",
    "\n",
    "        grid_size = max(self.width, self.height)\n",
    "\n",
    "        mission_space = MissionSpace(mission_func=lambda: \"get to the green goal square\")\n",
    "\n",
    "        super().__init__(\n",
    "            grid_size=grid_size,\n",
    "            max_steps=self.width * self.height * 2, # max_steps is typically 2x the grid size\n",
    "            see_through_walls=False,\n",
    "            agent_view_size=5,                      # Size of the agent's view square\n",
    "            mission_space=mission_space,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # Manually define our observation_space as a single Box (the image).\n",
    "        # By default, MiniGrid's image shape is (view_size, view_size, 3) if using partial obs,\n",
    "        # or (height, width, 3) if using full-grid observation. We'll do full-grid here:\n",
    "        # We'll define (self.height, self.width, 3) as the shape.\n",
    "        # In practice, \"image\" shape can vary if partial observations are used.\n",
    "        self.observation_space = Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=(self.agent_view_size, self.agent_view_size, 3),\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "\n",
    "    \n",
    "    def _gen_grid(self, width, height):\n",
    "        \"\"\"\n",
    "        Generate the grid layout for a new episode using the DFS Maze Generation Algorithm.\n",
    "        \"\"\"\n",
    "        # Create an empty grid of the \"true\" width x height from config\n",
    "        self.grid = Grid(self.width, self.height)\n",
    "        # Surround the grid with walls\n",
    "        self.grid.wall_rect(0, 0, self.width, self.height)\n",
    "\n",
    "        # Initialize the maze as walls\n",
    "        maze = [[1 for _ in range(self.width)] for _ in range(self.height)]\n",
    "\n",
    "        # Define directions for DFS\n",
    "        directions = [(0, 2), (0, -2), (2, 0), (-2, 0)]\n",
    "\n",
    "        def is_valid(x, y):\n",
    "            \"\"\"Check if a cell is valid for carving.\"\"\"\n",
    "            return 0 < x < self.height - 1 and 0 < y < self.width - 1 and maze[x][y] == 1\n",
    "\n",
    "        def carve(x, y):\n",
    "            \"\"\"Carve passages in the maze using DFS.\"\"\"\n",
    "            maze[x][y] = 0  # Mark the cell as part of the maze\n",
    "            self.grid.set(x, y, None)  # Clear the wall in the grid\n",
    "            self.rng.shuffle(directions)\n",
    "            for dx, dy in directions:\n",
    "                nx, ny = x + dx, y + dy\n",
    "                if is_valid(nx, ny):\n",
    "                    # Remove the wall between cells\n",
    "                    maze[x + dx // 2][y + dy // 2] = 0\n",
    "                    self.grid.set(x + dx // 2, y + dy // 2, None)\n",
    "                    carve(nx, ny)\n",
    "\n",
    "        # Start carving from the top-left corner\n",
    "        carve(1, 1)\n",
    "\n",
    "        # Place the goal object in a random position not occupied by a wall\n",
    "        while True:\n",
    "            r = self.rng.integers(1, self.height - 1)\n",
    "            c = self.rng.integers(1, self.width - 1)\n",
    "            if self.grid.get(c, r) is None:\n",
    "                self.put_obj(Goal(), c, r)\n",
    "                break\n",
    "\n",
    "        # Place the agent in a random position not occupied by a wall and not on the goal\n",
    "        while True:\n",
    "            r = self.rng.integers(1, self.height - 1)\n",
    "            c = self.rng.integers(1, self.width - 1)\n",
    "            if self.grid.get(c, r) is None:\n",
    "                self.place_agent(top=(c, r), rand_dir=True)\n",
    "                break\n",
    "    \n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Override reset to ensure we only return the 'image' array\n",
    "        instead of a dict with 'image' and 'mission'.\n",
    "        \"\"\"\n",
    "        obs, info = super().reset(**kwargs)\n",
    "        obs = self._convert_obs(obs)\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Same for step: override to convert the dict observation into an image only.\n",
    "        \"\"\"\n",
    "        obs, reward, done, truncated, info = super().step(action)\n",
    "        obs = self._convert_obs(obs)\n",
    "        return obs, reward, done, truncated, info\n",
    "\n",
    "    def _convert_obs(self, original_obs):\n",
    "        \"\"\"\n",
    "        original_obs is typically {'image':..., 'mission':...}.\n",
    "        We'll just return original_obs['image'] to get a Box(low=0,high=255) shape.\n",
    "        \"\"\"\n",
    "        return original_obs[\"image\"]\n",
    "        #return np.transpose(original_obs[\"image\"], (2, 0, 1))\n",
    "\n",
    "\n",
    "\n",
    "def print_maze_from_config(config):\n",
    "    env = MyCustomMaze(config, render_mode='rgb_array')\n",
    "    env.reset()\n",
    "    full_level_image = env.render()  # This should return an RGB image of the full grid\n",
    "\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(full_level_image)\n",
    "    plt.title(\"Maze Configuration: \" + str(config))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# Generate a random maze and visualize it\n",
    "random_maze = random_config(6)\n",
    "print_maze_from_config(random_maze)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD CODE, NOT VECORIZED, REDUNDANT CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "# ====================================================\n",
    "# 4. Main ACCEL Loop\n",
    "# ====================================================\n",
    "\n",
    "def main_accel_demo(total_iterations, replay_prob, train_steps, level_buffer_size,\n",
    "                    initial_fill_size, grid_size):\n",
    "    \n",
    "    \n",
    "    # Create a level buffer to store generated levels and their scores\n",
    "    level_buffer = LevelBuffer(max_size=level_buffer_size)\n",
    "    iteration_regrets = []\n",
    "        \n",
    "    #Create a dummy environment to initialize the model\n",
    "    dummy_env = MyCustomGrid(random_config(grid_size))\n",
    "    vectorized_env = create_vectorized_env(dummy_env, n_envs=4)\n",
    "\n",
    "    # Initialize student model with PPO\n",
    "    print(\"Initializing student model PPO...\")\n",
    "    student_model = initialize_ppo(dummy_env)\n",
    "\n",
    "    skipped = 0\n",
    "\n",
    "    # Populate buffer with initial levels\n",
    "    print(f\"Populating buffer with {initial_fill_size} initial levels with regret != 0...\")\n",
    "    for _ in range(initial_fill_size + skipped):\n",
    "        cfg = random_config(grid_size)\n",
    "        regret = calculate_regret_gae(MyCustomGrid(cfg), student_model, max_steps=1000, gamma=0.99, lam=0.95)\n",
    "        \n",
    "        # Skip levels with 0 regret\n",
    "        if regret == 0:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        level_buffer.add(cfg, regret)\n",
    "        \n",
    "    \n",
    "    print(\"\\nDone. Number of skipped levels with zero regret:\", skipped)\n",
    "    \n",
    "    skipped = 0\n",
    "    iteration = 0\n",
    "    # Main ACCEL loop\n",
    "    print(\"\\nMain ACCEL loop...\")\n",
    "    while iteration < total_iterations + skipped:\n",
    "        print(f\"\\n=== ITERATION {iteration + 1}/{total_iterations} SKIPPED {skipped} ===\")\n",
    "        \n",
    "        iteration += 1\n",
    "        \n",
    "        # Decide whether to use replay or generate a new level\n",
    "        use_replay = np.random.rand() < replay_prob\n",
    "        \n",
    "        # Generates new random levels if you don't use replay\n",
    "        if not use_replay or len(level_buffer.data) == 0:\n",
    "            cfg = random_config(grid_size)\n",
    "            regret = calculate_regret_gae(MyCustomGrid(cfg), student_model, max_steps=100, gamma=0.99, lam=0.95)\n",
    "            \n",
    "            if regret == 0:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            level_buffer.add(cfg, regret)\n",
    "            print(f\"  Sampled new config, regret={regret:.3f}\")\n",
    "        else:\n",
    "            # Replays an existing layer, edits it, and evaluates the new layer\n",
    "            old_cfg = level_buffer.sample_config()\n",
    "            env = MyCustomGrid(old_cfg)\n",
    "            \n",
    "            student_model.set_env(env)\n",
    "            student_model.learn(total_timesteps=train_steps)\n",
    "\n",
    "            new_cfg = edit_config(old_cfg)\n",
    "            regret = calculate_regret_gae(MyCustomGrid(new_cfg), student_model, max_steps=100, gamma=0.99, lam=0.95)\n",
    "            \n",
    "            if regret == 0:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            level_buffer.add(new_cfg, regret)\n",
    "            print(f\"  Replayed + mutated config, regret={regret:.3f}\")\n",
    "        \n",
    "        iteration_regrets.append(regret)\n",
    "    \n",
    "    print(\"\\nDone. Number of skipped levels with zero regret:\", skipped)\n",
    "\n",
    "    # Visualize progress of the regret over iterations.\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(iteration_regrets, marker='o')\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Regret\")\n",
    "    plt.title(\"Regret Progress during ACCEL Training\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    \n",
    "    config = {\n",
    "        \"grid_size\": 8,\n",
    "        \n",
    "        \"total_iterations\": 64,\n",
    "        \"train_steps\": 1024,\n",
    "\n",
    "        \"replay_prob\": 0.7,           # Probability of replaying a level and editing it vs. generating a new one\n",
    "        \"level_buffer_size\": 128,     # Maximum number of levels to store in the buffer\n",
    "        \"initial_fill_size\": 64,      # Number of levels to pre-fill the buffer with\n",
    "        \"regret_threshold\": 0.0,      # Minimum regret threshold to consider a level for the buffer\n",
    "        \n",
    "        \"n_envs\": 8,                  # Number of parallel environments to use for training\n",
    "        \n",
    "        \"edit_levels\": True,          # Whether to edit levels during training i.e. ACCEL or PLR\n",
    "        \"easy_start\": True            # Whether to fill the buffer with easy levels first i.e. minimum number of blocks\n",
    "    }\n",
    "    \n",
    "    print(\"Running ACCEL with config:\")\n",
    "    print(config, \"\\n\")\n",
    "    \n",
    "    main_accel(**config)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_regret_gae_parallel(env_config, model, max_steps=1000, gamma=0.99, lam=0.95, n_envs=4):\n",
    "    '''\n",
    "    Roll out n_envs copies of MyCustomGrid(env_config) in parallel,\n",
    "    compute GAE-based 'regret' for each environment, and return the max.\n",
    "    '''\n",
    "\n",
    "    # Create vectorized env with n_envs copies\n",
    "    vec_env = create_vectorized_env(env_config, n_envs=n_envs)\n",
    "    obs_array = vec_env.reset()  # shape: (n_envs, height, width, 3)\n",
    "\n",
    "    # For each environment, we will store transitions to later compute GAE\n",
    "    # We'll keep them in lists, one per environment.\n",
    "    # Alternatively, we can store them in big arrays (n_envs, max_steps), etc.\n",
    "    rewards_list = [[] for _ in range(n_envs)]\n",
    "    values_list = [[] for _ in range(n_envs)]\n",
    "    dones_list = [[] for _ in range(n_envs)]\n",
    "\n",
    "    for t in range(max_steps):\n",
    "        # Model’s predict can handle multiple obs in a single forward pass\n",
    "        actions, _ = model.predict(obs_array, deterministic=True)\n",
    "        \n",
    "        # Also compute the values in one batch\n",
    "        # Convert obs_array to torch tensor\n",
    "        obs_tensor = torch.as_tensor(np.transpose(obs_array, (0, 3, 1, 2))).float().to(device)\n",
    "        with torch.no_grad():\n",
    "            # shape: (n_envs, 1)\n",
    "            value_t = model.policy.predict_values(obs_tensor).cpu().numpy().flatten()\n",
    "\n",
    "        # Step all envs in parallel\n",
    "        next_obs_array, rewards, dones, truncs = vec_env.step(actions)\n",
    "\n",
    "        # Store the results\n",
    "        for i in range(n_envs):\n",
    "            rewards_list[i].append(rewards[i])\n",
    "            values_list[i].append(value_t[i])\n",
    "            dones_list[i].append(bool(dones[i]) or bool(truncs[i]))\n",
    "\n",
    "        obs_array = next_obs_array\n",
    "\n",
    "        # If all envs are done or truncated, we can break early\n",
    "        if all(dones) or all(truncs):\n",
    "            break\n",
    "\n",
    "    # We also need the terminal value for each env\n",
    "    # (0 if done, otherwise model's value at final obs)\n",
    "    obs_tensor = torch.as_tensor(np.transpose(obs_array, (0, 3, 1, 2))).float().to(device)\n",
    "    with torch.no_grad():\n",
    "        final_values = model.policy.predict_values(obs_tensor).cpu().numpy().flatten()\n",
    "\n",
    "    # Now, compute GAE-based \"regret\" for each of the n_envs\n",
    "    regrets = []\n",
    "    for i in range(n_envs):\n",
    "        # If the env ended with done or truncated, terminal value = 0\n",
    "        if dones_list[i][-1]:\n",
    "            values_list[i].append(0.0)\n",
    "        else:\n",
    "            values_list[i].append(final_values[i])\n",
    "\n",
    "        # Compute delta_t and approximate GAE-like metric\n",
    "        env_rewards = rewards_list[i]\n",
    "        env_values = values_list[i]\n",
    "        env_dones = dones_list[i]\n",
    "\n",
    "        env_regrets = []\n",
    "        for t in range(len(env_rewards)):\n",
    "            delta_t = env_rewards[t] + gamma * env_values[t + 1] * (1 - env_dones[t]) - env_values[t]\n",
    "            # accumulate discounted error\n",
    "            discounted_error = (gamma * lam) ** t * delta_t\n",
    "            env_regrets.append(max(0, discounted_error))\n",
    "\n",
    "        # The environment's \"regret\" is the max of its positive GAE deltas\n",
    "        regrets.append(max(env_regrets) if env_regrets else 0.0)\n",
    "\n",
    "    # Return the maximum regret across the parallel envs\n",
    "    return max(regrets) if regrets else 0.0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    def _gen_grid(self, width, height):\n",
    "        '''\n",
    "        Generate the grid layout for a new episode.\n",
    "        We use self.width and self.height from config, even though the underlying\n",
    "        MiniGrid environment might use grid_size for some of its operations.\n",
    "        '''    \n",
    "        \n",
    "        # Create an empty grid of the \"true\" width x height from config\n",
    "        self.grid = Grid(self.width, self.height)\n",
    "        # Surround the grid with walls\n",
    "        self.grid.wall_rect(0, 0, self.width, self.height)\n",
    "        \n",
    "        # Place random walls inside using the custom seed. Only place a wall if the cell is empty.\n",
    "        for _ in range(self.num_blocks):\n",
    "            r = self.rng.integers(1, self.height - 1)\n",
    "            c = self.rng.integers(1, self.width - 1)\n",
    "            if self.grid.get(c, r) is None: #and (c, r) != self.config[\"start_pos\"] and (c, r) != self.config[\"goal_pos\"]:\n",
    "                self.put_obj(Wall(), c, r)\n",
    "        \n",
    "        # Place the goal object in a random position not occupied by any wall\n",
    "        \"\"\"if self.config[\"goal_pos\"] is None:\"\"\"\n",
    "        while True:\n",
    "            r = self.rng.integers(1, self.height - 1)\n",
    "            c = self.rng.integers(1, self.width - 1)\n",
    "            if self.grid.get(c, r) is None: # and (c, r) != self.config[\"start_pos\"]:\n",
    "                self.put_obj(Goal(), c, r)\n",
    "                self.config[\"goal_pos\"] = (c, r)\n",
    "                break\n",
    "        \"\"\"elif self.config[\"goal_pos\"] is not None:\n",
    "            c, r = self.config[\"goal_pos\"]\n",
    "            self.put_obj(Goal(), c, r)\"\"\"\n",
    "\n",
    "        # Place the agent in a random position not occupied by any wall and not on the goal\n",
    "        \n",
    "        \"\"\"if self.config[\"start_pos\"] is None:\"\"\"\n",
    "        while True:\n",
    "            r = self.rng.integers(1, self.height - 1)\n",
    "            c = self.rng.integers(1, self.width - 1)\n",
    "            if self.grid.get(c, r) is None: # and (c, r) != self.config[\"goal_pos\"]:\n",
    "                self.place_agent(top=(c, r), rand_dir=True)\n",
    "                self.config[\"start_pos\"] = (c, r)\n",
    "                break  \n",
    "        \"\"\"elif self.config[\"start_pos\"] is not None:\n",
    "            c, r = self.config[\"start_pos\"]\n",
    "            self.place_agent(top=(c, r), rand_dir=True)\"\"\"'''\n",
    "            \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "accel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
