{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACCEL IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DANGER: ACCEL does not use a teacher, but a single student that learns from levels selected by a curator, in this impmenetation the curator is a sampler that gives te studemt with a probabilistic manner, the highest regret-based score level, but instaed of discarding the levels during training, it makes small edits to the most diffucult ones, gibing the change to rely on already good but hard leves without the need of a teacher.\n",
    "# TODO: the start_pos and goal_pos of edited levels are uncorrectly marked, if I mutate the level no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Putting up the level from config: {'width': 6, 'height': 6, 'num_blocks': 11, 'start_pos': (1, 3), 'goal_pos': (3, 4), 'edited': False, 'seed_val': 572723}\n",
      "No path found\n",
      "No path found\n",
      "No path found\n",
      "No path found\n",
      "No path found\n",
      "No path found\n",
      "Found a path\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = random_config(6)\n",
    "\n",
    "print_level_from_config(config, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "from gymnasium.spaces import Box\n",
    "\n",
    "from minigrid.core.mission import MissionSpace\n",
    "from minigrid.core.world_object import Goal, Wall\n",
    "from minigrid.minigrid_env import MiniGridEnv, Grid\n",
    "\n",
    "from minigrid.wrappers import ImgObsWrapper\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import wandb\n",
    "\n",
    "device = 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ====================================================\n",
    "# 1. Custom MiniGrid Environment that returns only the image\n",
    "#    for SB3's PPO (which expects a Box space).\n",
    "# ====================================================\n",
    "class MyCustomGrid(MiniGridEnv):\n",
    "    \"\"\"\n",
    "    Simple MiniGrid environment that places random wall tiles\n",
    "    according to a config dict, returning only the 'image' observation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config=None, solvable_only=False, **kwargs):\n",
    "        if config is None:\n",
    "            config = {}\n",
    "        self.config = config\n",
    "        self.solvable_only = solvable_only\n",
    "\n",
    "        \n",
    "        # Create a random number generator with the custom seed\n",
    "        self.rng = np.random.default_rng(seed=self.config.get(\"seed_val\"))\n",
    "\n",
    "        mission_space = MissionSpace(mission_func=lambda: \"get to the green goal square\")\n",
    "\n",
    "        super().__init__(\n",
    "            grid_size=self.config['width'],\n",
    "            max_steps=self.config['width'] * self.config['height'] * 2, # max_steps is typically 2x the grid size\n",
    "            see_through_walls=False,\n",
    "            agent_view_size=5,                      # Size of the agent's view square\n",
    "            mission_space=mission_space,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # Manually define our observation_space as a single Box (the image).\n",
    "        # By default, MiniGrid's image shape is (view_size, view_size, 3) if using partial obs,\n",
    "        # or (height, width, 3) if using full-grid observation. We'll do full-grid here:\n",
    "        # We'll define (self.height, self.width, 3) as the shape.\n",
    "        # In practice, \"image\" shape can vary if partial observations are used.\n",
    "        self.observation_space = Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=(self.agent_view_size, self.agent_view_size, 3),\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "\n",
    "            \n",
    "    def _gen_grid(self, width, height):\n",
    "        \"\"\"\n",
    "        Generate a new environment layout ensuring solvability if required.\n",
    "        \"\"\"\n",
    "\n",
    "        while True:  # Keep regenerating until a solvable layout is found\n",
    "            self.grid = Grid(width, height)\n",
    "            self.grid.wall_rect(0, 0, width, height)\n",
    "\n",
    "            # Place the goal\n",
    "            goal_pos = self.config.get(\"goal_pos\")\n",
    "            if goal_pos is None:\n",
    "                while True:\n",
    "                    goal_r = self.rng.integers(1, height - 1)\n",
    "                    goal_c = self.rng.integers(1, width - 1)\n",
    "                    if self.grid.get(goal_c, goal_r) is None:\n",
    "                        self.put_obj(Goal(), goal_c, goal_r)\n",
    "                        self.config[\"goal_pos\"] = (goal_c, goal_r)\n",
    "                        break\n",
    "            else:\n",
    "                self.put_obj(Goal(), goal_pos[0], goal_pos[1])\n",
    "\n",
    "            # Place the agent\n",
    "            start_pos = self.config.get(\"start_pos\")\n",
    "            if start_pos is None:\n",
    "                while True:\n",
    "                    start_r = self.rng.integers(1, height - 1)\n",
    "                    start_c = self.rng.integers(1, width - 1)\n",
    "                    if self.grid.get(start_c, start_r) is None and (start_c, start_r) != self.config[\"goal_pos\"]:\n",
    "                        self.agent_pos = (start_c, start_r)\n",
    "                        self.agent_dir = self.rng.integers(0, 4)\n",
    "                        self.config[\"start_pos\"] = (start_c, start_r)\n",
    "                        break\n",
    "            else:\n",
    "                self.agent_pos = start_pos\n",
    "                self.agent_dir = self.rng.integers(0, 4)\n",
    "                self.config[\"start_pos\"] = start_pos\n",
    "            \n",
    "            placed_blocks = 0\n",
    "            \n",
    "            # Maximum number of tries to place the blocks\n",
    "            max_num_tries = 1000\n",
    "            \n",
    "            # Place random walls using config parameters\n",
    "            while placed_blocks < self.config[\"num_blocks\"]:\n",
    "                max_num_tries -= 1\n",
    "                r = self.rng.integers(1, height - 1)\n",
    "                c = self.rng.integers(1, width - 1)\n",
    "                if self.grid.get(c, r) is None and (c, r) != self.config[\"start_pos\"] and (c, r) != self.config[\"goal_pos\"]:\n",
    "                    self.put_obj(Wall(), c, r)\n",
    "                    placed_blocks += 1\n",
    "                if max_num_tries <= 0:\n",
    "                    print(\"Could not place all blocks in the grid.\")\n",
    "                    break\n",
    "                \n",
    "\n",
    "            # Check solvability if required\n",
    "            if not self.solvable_only or self._is_solvable():\n",
    "                break\n",
    "            \n",
    "            # If not solvable, change seed and try again\n",
    "            self.rng = np.random.default_rng(seed=self.rng.integers(0, 999999))\n",
    "        \n",
    "    def _is_solvable(self):\n",
    "        \"\"\"\n",
    "        Uses Breadth-First Search (BFS) to check if there's a path \n",
    "        from the agent's start position to the goal.\n",
    "        \"\"\"\n",
    "        start_pos = self.config[\"start_pos\"]\n",
    "        goal_pos = self.config[\"goal_pos\"]\n",
    "        if not start_pos or not goal_pos:\n",
    "            return False\n",
    "\n",
    "        queue = deque([start_pos])\n",
    "        visited = set()\n",
    "        visited.add(start_pos)\n",
    "\n",
    "        while queue:\n",
    "            x, y = queue.popleft()\n",
    "            if (x, y) == goal_pos:\n",
    "                return True\n",
    "\n",
    "            # Possible moves: up, down, left, right\n",
    "            for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n",
    "                nx, ny = x + dx, y + dy\n",
    "                cell_obj = self.grid.get(nx, ny)\n",
    "                if (\n",
    "                    1 <= nx < self.width - 1 and  # Stay within grid bounds\n",
    "                    1 <= ny < self.height - 1 and\n",
    "                    (nx, ny) not in visited and\n",
    "                    self.grid.get(nx, ny) is None or isinstance(cell_obj, Goal)\n",
    "                ):\n",
    "                    queue.append((nx, ny))\n",
    "                    visited.add((nx, ny))\n",
    "        return False  # No path found\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Override reset to ensure we only return the 'image' array\n",
    "        instead of a dict with 'image' and 'mission'.\n",
    "        \"\"\"\n",
    "        obs, info = super().reset(**kwargs)\n",
    "        obs = self._convert_obs(obs)\n",
    "        \n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Same for step: override to convert the dict observation into an image only.\n",
    "        \"\"\"\n",
    "        obs, reward, done, truncated, info = super().step(action)\n",
    "        obs = self._convert_obs(obs)\n",
    "        return obs, reward, done, truncated, info\n",
    "\n",
    "    def _convert_obs(self, original_obs):\n",
    "        \"\"\"\n",
    "        original_obs is typically {'image':..., 'mission':...}.\n",
    "        We'll just return original_obs['image'] to get a Box(low=0,high=255) shape.\n",
    "        \"\"\"\n",
    "        return original_obs[\"image\"]\n",
    "        #return np.transpose(original_obs[\"image\"], (2, 0, 1))\n",
    "    \n",
    "    def update_config(self, new_config):\n",
    "        self.config = new_config\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "\n",
    "def random_config(grid_size, num_blocks=None):\n",
    "    max_blocks = int(((grid_size - 1) * (grid_size - 1)) / 2)\n",
    "    \n",
    "    if num_blocks is None:\n",
    "        num_blocks = np.random.randint(1, max_blocks)\n",
    "    else:\n",
    "        num_blocks = min(num_blocks, max_blocks)\n",
    "        \n",
    "    config = {\n",
    "        \"width\": grid_size,\n",
    "        \"height\": grid_size,\n",
    "        \"num_blocks\": num_blocks,\n",
    "        \"start_pos\": None,\n",
    "        \"goal_pos\": None,\n",
    "        \"edited\": False,\n",
    "        \"seed_val\": np.random.randint(0, 999999),\n",
    "    }\n",
    "    \n",
    "    # Set the start and goal positions\n",
    "    env = MyCustomGrid(config)\n",
    "    \n",
    "    # Reset the environment to get the start and goal positions\n",
    "    env.reset()\n",
    "    \n",
    "    # Get the new config from the environment\n",
    "    config = env.config\n",
    "        \n",
    "    return config\n",
    "\n",
    "def print_level_from_config(config, solvable_only=False):\n",
    "    print(\"Putting up the level from config:\", config)\n",
    "    env = MyCustomGrid(config, render_mode='rgb_array', solvable_only=solvable_only)\n",
    "    env.reset()\n",
    "    full_level_image = env.render()  # This should return an RGB image of the full grid\n",
    "\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(full_level_image)\n",
    "    plt.title(\"Level Configuration: \" + str(config))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "\"\"\"# Modify an existing configuration, adding randomness.\n",
    "def edit_config(old_config):\n",
    "    max_blocks = int(((old_config[\"width\"] - 1) * (old_config[\"height\"] - 1)) / 2)\n",
    "    \n",
    "    new_config = dict(old_config)\n",
    "    \n",
    "    # Randomly change the number of blocks\n",
    "    new_number_blocks = old_config[\"num_blocks\"] + np.random.choice([1, 2, 3])\n",
    "    \n",
    "    # Ensure the number of blocks is within bounds\n",
    "    new_config[\"num_blocks\"] = max(1, min(new_number_blocks, max_blocks))    \n",
    "    \n",
    "    # Mark the config as edited\n",
    "    new_config[\"edited\"] = True\n",
    "    \n",
    "    return new_config\"\"\"\n",
    "    \n",
    "import numpy as np\n",
    "\n",
    "def edit_config(old_config, difficulty_level=1):\n",
    "\n",
    "    width, height = old_config[\"width\"], old_config[\"height\"]\n",
    "    total_cells = width * height\n",
    "\n",
    "    # Define a baseline max number of blocks\n",
    "    max_blocks = int(0.6 * total_cells)  # Ensure we don't overcrowd (max 60% coverage)\n",
    "    \n",
    "    # Calculate the new number of blocks using a logarithmic scale\n",
    "    base_growth = int(np.log2(total_cells) * difficulty_level)\n",
    "    \n",
    "    # Introduce some randomness while keeping it within a reasonable range\n",
    "    growth_factor = np.random.randint(base_growth // 2, base_growth + 1)\n",
    "    \n",
    "    # Compute the new block count\n",
    "    new_number_blocks = old_config[\"num_blocks\"] + growth_factor\n",
    "    \n",
    "    # Ensure it's within the allowed range\n",
    "    new_config = dict(old_config)\n",
    "    new_config[\"num_blocks\"] = max(1, min(new_number_blocks, max_blocks))  \n",
    "    \n",
    "    # Mark as edited\n",
    "    new_config[\"edited\"] = True\n",
    "\n",
    "    return new_config\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# 2. Simple “level buffer” \n",
    "# ====================================================\n",
    "# class to memorize generated levels and score\n",
    "class LevelBuffer: \n",
    "    def __init__(self, max_size=50):\n",
    "        self.max_size = max_size\n",
    "        self.data = []  # will store (config_dict, score)\n",
    "\n",
    "    def add(self, config, score):\n",
    "        self.data.append((config, score))\n",
    "        if len(self.data) > self.max_size:\n",
    "            self.data.sort(key=lambda x: x[1], reverse=True)\n",
    "            self.data = self.data[: self.max_size]\n",
    "            #it memorize only the highest score for each level\n",
    "\n",
    "    def sample_config(self): \n",
    "        # Samples a level from the buffer, weighting the probabilities \n",
    "        # based on the scores.\n",
    "        if len(self.data) == 0:\n",
    "            return None\n",
    "        scores = [item[1] for item in self.data]\n",
    "        total = sum(scores)\n",
    "        if total <= 1e-9:\n",
    "            # fallback to uniform\n",
    "            idx = np.random.randint(len(self.data))\n",
    "            return self.data[idx][0]\n",
    "        probs = [s / total for s in scores]\n",
    "        idx = np.random.choice(len(self.data), p=probs)\n",
    "        return self.data[idx][0]\n",
    "\n",
    "# ====================================================\n",
    "# 3. Utility Functions\n",
    "# ====================================================\n",
    "\n",
    "# Calculate regret using Generalized Advantage Estimation (GAE) with Stable-Baselines3's PPO model.\n",
    "# PLR approximates regret using a score function such as the positive value loss.\n",
    "def calculate_regret_gae(env, model, max_steps, gamma, lam):\n",
    "    \"\"\"\n",
    "    Calculate regret using Generalized Advantage Estimation (GAE)\n",
    "    with Stable-Baselines3's PPO model.\n",
    "    \"\"\"\n",
    "    obs, _ = env.reset()\n",
    "    regrets = []\n",
    "    rewards = []\n",
    "    dones = []\n",
    "    values = []\n",
    "\n",
    "    for t in range(max_steps):\n",
    "        # Add batch dimension to the observation tensor\n",
    "        obs_tensor = torch.as_tensor(obs).float().unsqueeze(0).to(device)\n",
    "        \n",
    "        # Use the model's policy to get the value and action.\n",
    "        # For actions, model.predict handles single observations well.\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        \n",
    "        # Compute the value from the policy.\n",
    "        value_t = model.policy.predict_values(obs_tensor).item()\n",
    "        values.append(value_t)\n",
    "        \n",
    "        # Perform the step in the environment\n",
    "        obs, reward, done, truncated, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "    # Add value of the terminal state (0 if done/truncated)\n",
    "    if done or truncated:\n",
    "        terminal_value = 0.0\n",
    "    else:\n",
    "        terminal_obs_tensor = torch.as_tensor(obs).float().unsqueeze(0).to(device)\n",
    "        terminal_value = model.policy.predict_values(terminal_obs_tensor).item()\n",
    "    values.append(terminal_value)\n",
    "\n",
    "    # Compute TD-errors and GAE-like regret score\n",
    "    for t in range(len(rewards)):\n",
    "        delta_t = rewards[t] + gamma * values[t + 1] * (1 - dones[t]) - values[t]\n",
    "        discounted_error = (gamma * lam) ** t * delta_t\n",
    "        regrets.append(max(0, discounted_error))\n",
    "\n",
    "    # Return the maximum positive regret score (or 0 if empty)\n",
    "    return max(regrets) if regrets else 0.0\n",
    "\n",
    "\n",
    "def initialize_ppo(env, learning_rate=1e-4):\n",
    "    return PPO(\n",
    "        \"MlpPolicy\",                    # Multi-layer perceptron policy\n",
    "        env,                            # environment to learn from\n",
    "        verbose=0,                      # Display training output\n",
    "        n_steps=256,                    # Number of steps to run for each environment per update\n",
    "        batch_size=64,                  # Minibatch size for each gradient update\n",
    "        learning_rate=learning_rate,    # Learning rate for optimizer\n",
    "        device=device                   # Use GPU if available\n",
    "    )\n",
    "    \n",
    "# Use vectorized environment\n",
    "def create_vectorized_env(config, n_envs=4, solvable_only=False):\n",
    "    \"\"\"\n",
    "    Create a vectorized environment with n parallel environments.\n",
    "    \"\"\"\n",
    "    return make_vec_env(lambda: MyCustomGrid(config, solvable_only), n_envs=n_envs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalute_models(load=False):\n",
    "    \n",
    "    if load:\n",
    "        # Load the models\n",
    "        model_plr = PPO.load(\"models/plr_model\")\n",
    "        model_accel = PPO.load(\"models/accel_model\")\n",
    "        model_accel_easy = PPO.load(\"models/accel_model_easy\")\n",
    "\n",
    "    # Inseert the models in a dictionary\n",
    "    models = {'PLR': model_plr, 'ACCEL': model_accel, 'ACCEL-EasyStart': model_accel_easy}\n",
    "\n",
    "    # Generate n levels difficulties with increasing complexity, for each level generate m configs\n",
    "    difficulties = 3\n",
    "    config[\"grid_size\"] = 11\n",
    "\n",
    "    levels = []\n",
    "    for i in range(difficulties):\n",
    "        level = []\n",
    "        for _ in range(10):\n",
    "            cfg = random_config(config[\"grid_size\"], num_blocks=config[\"grid_size\"]*(i+1))\n",
    "            #print_level_from_config(cfg)\n",
    "            level.append(cfg)\n",
    "        levels.append(level)\n",
    "\n",
    "    # Evaluate the model on the generated levels\n",
    "    results = {}\n",
    "    for model_name, model in models.items():\n",
    "        results[model_name] = []\n",
    "        for i, level in enumerate(levels):\n",
    "            print(f\"Evaluating level {i + 1} with {config['grid_size']*(i+1)} blocks for model {model_name}...\")\n",
    "            r = []\n",
    "            for j, cfg in enumerate(level):\n",
    "                # Create vectorized environment\n",
    "                env = create_vectorized_env(cfg, n_envs=4, solvable_only=True)\n",
    "                mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=5, deterministic=True)\n",
    "                r.append(mean_reward)\n",
    "            results[model_name].append(r)\n",
    "        print()\n",
    "        \n",
    "    # Print mean rewards for each level\n",
    "    for model_name in models.keys():\n",
    "        print(f\"Model: {model_name}\")\n",
    "        for i, level in enumerate(levels):\n",
    "            print(f\"Level {i + 1} - Complexity {config['grid_size']*(i+1)}: {np.mean(results[model_name][i]):.2f}\")\n",
    "        print()\n",
    "\n",
    "    # Boxplot of results, a plot for each level complexity comparing models\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for i, level in enumerate(levels):\n",
    "        plt.subplot(1, difficulties, i + 1)\n",
    "        plt.boxplot([results[model_name][i] for model_name in models.keys()])\n",
    "        plt.xticks([1,2,3], [model_name for model_name in models.keys()])\n",
    "        plt.title(f\"Level {i + 1} - Complexity {config['grid_size']*(i+1)}\")\n",
    "        plt.ylabel(\"Mean Reward\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_accel(total_iterations, replay_prob, train_steps, level_buffer_size,\n",
    "               initial_fill_size, grid_size, n_envs, edit_levels, regret_threshold,\n",
    "               easy_start):\n",
    "    \n",
    "    # Initialize Weights and Biases\n",
    "    wandb.init(project=\"accel\", config=config)\n",
    "    \n",
    "    # Create a level buffer, a personal class to store levels and scores\n",
    "    level_buffer = LevelBuffer(max_size=level_buffer_size)\n",
    "    \n",
    "    # Generate a random configuration {width, height, num_blocks, start_pos, goal_pos}\n",
    "    dummy_config = random_config(grid_size)\n",
    "    \n",
    "    # Create a vectorized environment, so a wrapper for MyCustomGrid that allows interconnection \n",
    "    # between gymnasium and stable-baselines3 to train the model in a vectorized way, since we\n",
    "    # are using DummyVecEnv, it is not true parallelism\n",
    "    vectorized_env = create_vectorized_env(dummy_config, n_envs=n_envs)\n",
    "\n",
    "    # Initialize PPO with vectorized environment\n",
    "    print(\"Initializing student model PPO...\")\n",
    "    student_model = initialize_ppo(vectorized_env)\n",
    "\n",
    "    # ====================================================\n",
    "    # Initial buffer fill\n",
    "    # ====================================================\n",
    "    \n",
    "    print(f\"Populating buffer with {initial_fill_size} initial levels with regret > {regret_threshold}...\")\n",
    "    while len(level_buffer.data) < initial_fill_size:\n",
    "        \n",
    "        if easy_start:\n",
    "            cfg = random_config(grid_size, num_blocks=2)\n",
    "        else:\n",
    "            cfg = random_config(grid_size)\n",
    "        \n",
    "        for monitor in vectorized_env.envs:\n",
    "            monitor.env.update_config(cfg)\n",
    "        \n",
    "        student_model.learn(total_timesteps=100)\n",
    "        \n",
    "        regret = calculate_regret_gae(MyCustomGrid(cfg), student_model, max_steps=1000, gamma=0.99, lam=0.95)\n",
    "\n",
    "        # Skip levels with low regret\n",
    "        if regret < regret_threshold: continue\n",
    "\n",
    "        level_buffer.add(cfg, regret)\n",
    "\n",
    "    # ====================================================\n",
    "    # Main ACCEL loop\n",
    "    # ====================================================\n",
    "    \n",
    "    iteration_regrets = []\n",
    "    iteration, skipped = 0, 0\n",
    "    \n",
    "    print(\"\\nMain training loop...\")\n",
    "    while iteration < total_iterations + skipped:\n",
    "        print(f\"\\n=== ITERATION {iteration + 1}/{total_iterations + skipped} SKIPPED: {skipped} ===\")\n",
    "        \n",
    "        iteration += 1\n",
    "        \n",
    "        # Decide whether to replay or generate a new level\n",
    "        use_replay = np.random.rand() < replay_prob\n",
    "\n",
    "        if not use_replay or len(level_buffer.data) == 0:\n",
    "            # Create a new random level\n",
    "            cfg = random_config(grid_size)\n",
    "            print(\"Generated new random level:\", cfg)\n",
    "        else:\n",
    "            # Sample a level from the buffer\n",
    "            cfg = level_buffer.sample_config()\n",
    "            print(\"Sampled level from buffer:\", cfg)\n",
    "            \n",
    "        # Update the vectorized environment with the selected config and train the model\n",
    "        for monitor in vectorized_env.envs:\n",
    "            monitor.env.update_config(cfg)\n",
    "        \n",
    "        student_model.learn(total_timesteps=train_steps)\n",
    "        \n",
    "        wandb.log({\n",
    "            \"iteration\": iteration,\n",
    "            \"regret_score\": regret,\n",
    "            \"regret_threshold\": regret_threshold,\n",
    "            \"buffer_size\": len(level_buffer.data),\n",
    "            \"value_loss\": student_model.logger.name_to_value[\"train/value_loss\"],\n",
    "            \"entropy_loss\": student_model.logger.name_to_value[\"train/entropy_loss\"],\n",
    "        })\n",
    "\n",
    "        if use_replay and edit_levels:\n",
    "            # Edit the level and calculate regret\n",
    "            cfg = edit_config(cfg)\n",
    "            print(\"Edited level to:\", cfg)\n",
    "\n",
    "        regret = calculate_regret_gae(MyCustomGrid(cfg), student_model, max_steps=1000, gamma=0.99, lam=0.95)\n",
    "        \n",
    "        if regret <= regret_threshold:\n",
    "            print(f\"Regret for current level is {regret:.5f} <= threshold {regret_threshold:.5f}. Skipping...\")\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "\n",
    "        print(f\"Regret for current level: {regret}, buffer size: {len(level_buffer.data)}\")\n",
    "        level_buffer.add(cfg, regret)\n",
    "        iteration_regrets.append(regret)\n",
    "        \n",
    "        # Increase the regret threshold slightly\n",
    "        regret_threshold += 0.0001\n",
    "        \n",
    "    \n",
    "    # Plot and display the progress\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(iteration_regrets, marker='o')\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Regret\")\n",
    "    plt.title(\"Regret Progress during Training\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nDone. Final buffer size:\", len(level_buffer.data))\n",
    "    print(\"Top-5 hardest levels (config, regret):\")\n",
    "    level_buffer.data.sort(key=lambda x: x[1], reverse=True)\n",
    "    for i, (cfg, sc) in enumerate(level_buffer.data[:5]):\n",
    "        print(f\"{i + 1}. regret={sc:.5f}, config={cfg}\")\n",
    "        #print_level_from_config(cfg)\n",
    "        \n",
    "    print(\"Top-5 easiest levels (config, regret):\")\n",
    "    level_buffer.data.sort(key=lambda x: x[1])\n",
    "    for i, (cfg, sc) in enumerate(level_buffer.data[:5]):\n",
    "        print(f\"{i + 1}. regret={sc:.5f}, config={cfg}\")\n",
    "        #print_level_from_config(cfg)\n",
    "    \n",
    "    wandb.finish()\n",
    "    \n",
    "    return student_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>buffer_size</td><td>▁▂▃▅▆▇██████████████████████████████████</td></tr><tr><td>entropy_loss</td><td>▁▃▃▄▃▃▃▄▅▄▅▅▇▇▆▆▆██▇▇▇███▇▇█▇█▇████████▇</td></tr><tr><td>iteration</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>regret_score</td><td>▁█▁▁█▁▆▂▅▄▃▂▅▃▂▇▃▃▂▃▄▆▂▂▃▁▂▂▂▂▂▁▁▂▂▂▃▂▂▁</td></tr><tr><td>regret_threshold</td><td>▁▁▂▂▃▄▄▄▅▅▅▆▇▇▇▇▇▇▇▇████████████████████</td></tr><tr><td>value_loss</td><td>▃▁▃▃▃▁█▂▄▂▁▄▁▂▂▃▁▃▂▂▁▁▁▁▁▁▃▁▃▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>buffer_size</td><td>64</td></tr><tr><td>entropy_loss</td><td>-0.05633</td></tr><tr><td>iteration</td><td>301</td></tr><tr><td>regret_score</td><td>0.07296</td></tr><tr><td>regret_threshold</td><td>0.48</td></tr><tr><td>value_loss</td><td>0.00086</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vivid-plasma-24</strong> at: <a href='https://wandb.ai/noostale-organization/accel/runs/51p35iwu' target=\"_blank\">https://wandb.ai/noostale-organization/accel/runs/51p35iwu</a><br> View project at: <a href='https://wandb.ai/noostale-organization/accel' target=\"_blank\">https://wandb.ai/noostale-organization/accel</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250117_015915-51p35iwu\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running PLR with config: {'grid_size': 8, 'total_iterations': 500, 'train_steps': 1000, 'replay_prob': 0.5, 'level_buffer_size': 64, 'initial_fill_size': 32, 'regret_threshold': 0.0, 'n_envs': 3, 'edit_levels': False, 'easy_start': False}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Emanuele\\Documents\\GitHub\\adaptive-rl-env-curricula\\wandb\\run-20250117_020822-8nrp0rlj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/noostale-organization/accel/runs/8nrp0rlj' target=\"_blank\">earnest-morning-25</a></strong> to <a href='https://wandb.ai/noostale-organization/accel' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/noostale-organization/accel' target=\"_blank\">https://wandb.ai/noostale-organization/accel</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/noostale-organization/accel/runs/8nrp0rlj' target=\"_blank\">https://wandb.ai/noostale-organization/accel/runs/8nrp0rlj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing student model PPO...\n",
      "Populating buffer with 32 initial levels with regret > 0.0...\n",
      "\n",
      "Main training loop...\n",
      "\n",
      "=== ITERATION 1/500 SKIPPED: 0 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 3, 'start_pos': (3, 2), 'goal_pos': (2, 3), 'edited': False, 'seed_val': 574048}\n",
      "Regret for current level: 0.03939574703574181, buffer size: 32\n",
      "\n",
      "=== ITERATION 2/500 SKIPPED: 0 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 3, 'start_pos': (3, 2), 'goal_pos': (2, 3), 'edited': False, 'seed_val': 574048}\n",
      "Regret for current level: 0.046449637711048125, buffer size: 33\n",
      "\n",
      "=== ITERATION 3/500 SKIPPED: 0 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 16, 'start_pos': (6, 5), 'goal_pos': (2, 5), 'edited': False, 'seed_val': 836153}\n",
      "Regret for current level: 0.06878476411104201, buffer size: 34\n",
      "\n",
      "=== ITERATION 4/500 SKIPPED: 0 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 16, 'start_pos': (5, 4), 'goal_pos': (4, 1), 'edited': False, 'seed_val': 120064}\n",
      "Regret for current level: 0.001872336789965623, buffer size: 35\n",
      "\n",
      "=== ITERATION 5/500 SKIPPED: 0 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 13, 'start_pos': (5, 2), 'goal_pos': (6, 4), 'edited': False, 'seed_val': 857444}\n",
      "Regret for current level: 0.1024972255527973, buffer size: 36\n",
      "\n",
      "=== ITERATION 6/500 SKIPPED: 0 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 12, 'start_pos': (1, 3), 'goal_pos': (3, 6), 'edited': False, 'seed_val': 276562}\n",
      "Regret for current level: 0.0036467968761920953, buffer size: 37\n",
      "\n",
      "=== ITERATION 7/500 SKIPPED: 0 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 17, 'start_pos': (5, 5), 'goal_pos': (2, 6), 'edited': False, 'seed_val': 70008}\n",
      "Regret for current level is 0.00000 <= threshold 0.00060. Skipping...\n",
      "\n",
      "=== ITERATION 8/501 SKIPPED: 1 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 8, 'start_pos': (4, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 775458}\n",
      "Regret for current level: 0.061378397792577755, buffer size: 38\n",
      "\n",
      "=== ITERATION 9/501 SKIPPED: 1 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 15, 'start_pos': (4, 1), 'goal_pos': (1, 1), 'edited': False, 'seed_val': 234574}\n",
      "Regret for current level is 0.00000 <= threshold 0.00070. Skipping...\n",
      "\n",
      "=== ITERATION 10/502 SKIPPED: 2 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 8, 'start_pos': (4, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 775458}\n",
      "Regret for current level: 0.07515109926462171, buffer size: 39\n",
      "\n",
      "=== ITERATION 11/502 SKIPPED: 2 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 20, 'start_pos': (3, 1), 'goal_pos': (5, 1), 'edited': False, 'seed_val': 804510}\n",
      "Regret for current level is 0.00000 <= threshold 0.00080. Skipping...\n",
      "\n",
      "=== ITERATION 12/503 SKIPPED: 3 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.7210813879966735, buffer size: 40\n",
      "\n",
      "=== ITERATION 13/503 SKIPPED: 3 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 13, 'start_pos': (1, 1), 'goal_pos': (4, 1), 'edited': False, 'seed_val': 559342}\n",
      "Regret for current level is 0.00000 <= threshold 0.00090. Skipping...\n",
      "\n",
      "=== ITERATION 14/504 SKIPPED: 4 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 3, 'start_pos': (4, 6), 'goal_pos': (4, 1), 'edited': False, 'seed_val': 464171}\n",
      "Regret for current level: 0.29710089934810996, buffer size: 41\n",
      "\n",
      "=== ITERATION 15/504 SKIPPED: 4 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 16, 'start_pos': (2, 5), 'goal_pos': (4, 1), 'edited': False, 'seed_val': 404388}\n",
      "Regret for current level is 0.00000 <= threshold 0.00100. Skipping...\n",
      "\n",
      "=== ITERATION 16/505 SKIPPED: 5 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 3, 'start_pos': (4, 6), 'goal_pos': (4, 1), 'edited': False, 'seed_val': 464171}\n",
      "Regret for current level: 0.29174381007760763, buffer size: 42\n",
      "\n",
      "=== ITERATION 17/505 SKIPPED: 5 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 8, 'start_pos': (4, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 775458}\n",
      "Regret for current level: 0.14716366827487948, buffer size: 43\n",
      "\n",
      "=== ITERATION 18/505 SKIPPED: 5 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 8, 'start_pos': (4, 3), 'goal_pos': (2, 5), 'edited': False, 'seed_val': 519997}\n",
      "Regret for current level: 0.4059567841887474, buffer size: 44\n",
      "\n",
      "=== ITERATION 19/505 SKIPPED: 5 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.7005951523780822, buffer size: 45\n",
      "\n",
      "=== ITERATION 20/505 SKIPPED: 5 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.565509033203125, buffer size: 46\n",
      "\n",
      "=== ITERATION 21/505 SKIPPED: 5 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 14, 'start_pos': (2, 6), 'goal_pos': (1, 4), 'edited': False, 'seed_val': 593498}\n",
      "Regret for current level: 0.0330710405111313, buffer size: 47\n",
      "\n",
      "=== ITERATION 22/505 SKIPPED: 5 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 17, 'start_pos': (4, 4), 'goal_pos': (6, 3), 'edited': False, 'seed_val': 809963}\n",
      "Regret for current level is 0.00000 <= threshold 0.00160. Skipping...\n",
      "\n",
      "=== ITERATION 23/506 SKIPPED: 6 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 3, 'start_pos': (4, 6), 'goal_pos': (4, 1), 'edited': False, 'seed_val': 464171}\n",
      "Regret for current level: 0.2674853143197298, buffer size: 48\n",
      "\n",
      "=== ITERATION 24/506 SKIPPED: 6 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.6760439693927764, buffer size: 49\n",
      "\n",
      "=== ITERATION 25/506 SKIPPED: 6 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 15, 'start_pos': (6, 5), 'goal_pos': (3, 6), 'edited': False, 'seed_val': 109028}\n",
      "Regret for current level is 0.00000 <= threshold 0.00180. Skipping...\n",
      "\n",
      "=== ITERATION 26/507 SKIPPED: 7 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 13, 'start_pos': (2, 6), 'goal_pos': (1, 4), 'edited': False, 'seed_val': 942949}\n",
      "Regret for current level: 0.023870278298854825, buffer size: 50\n",
      "\n",
      "=== ITERATION 27/507 SKIPPED: 7 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 2, 'start_pos': (4, 4), 'goal_pos': (5, 6), 'edited': False, 'seed_val': 760853}\n",
      "Regret for current level: 0.10338861733675003, buffer size: 51\n",
      "\n",
      "=== ITERATION 28/507 SKIPPED: 7 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 2, 'start_pos': (3, 2), 'goal_pos': (2, 6), 'edited': False, 'seed_val': 488162}\n",
      "Regret for current level: 0.07720179468393323, buffer size: 52\n",
      "\n",
      "=== ITERATION 29/507 SKIPPED: 7 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 13, 'start_pos': (3, 4), 'goal_pos': (3, 5), 'edited': False, 'seed_val': 156782}\n",
      "Regret for current level is 0.00000 <= threshold 0.00210. Skipping...\n",
      "\n",
      "=== ITERATION 30/508 SKIPPED: 8 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 16, 'start_pos': (5, 3), 'goal_pos': (2, 5), 'edited': False, 'seed_val': 492117}\n",
      "Regret for current level is 0.00000 <= threshold 0.00210. Skipping...\n",
      "\n",
      "=== ITERATION 31/509 SKIPPED: 9 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.6545353055000305, buffer size: 53\n",
      "\n",
      "=== ITERATION 32/509 SKIPPED: 9 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.5936176896095275, buffer size: 54\n",
      "\n",
      "=== ITERATION 33/509 SKIPPED: 9 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 8, 'start_pos': (4, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 775458}\n",
      "Regret for current level: 0.15037001192569732, buffer size: 55\n",
      "\n",
      "=== ITERATION 34/509 SKIPPED: 9 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 3, 'start_pos': (4, 6), 'goal_pos': (4, 1), 'edited': False, 'seed_val': 464171}\n",
      "Regret for current level: 0.2584763720065355, buffer size: 56\n",
      "\n",
      "=== ITERATION 35/509 SKIPPED: 9 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 9, 'start_pos': (4, 6), 'goal_pos': (2, 1), 'edited': False, 'seed_val': 26318}\n",
      "Regret for current level: 0.09684044688940047, buffer size: 57\n",
      "\n",
      "=== ITERATION 36/509 SKIPPED: 9 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 18, 'start_pos': (1, 2), 'goal_pos': (6, 6), 'edited': False, 'seed_val': 706839}\n",
      "Regret for current level is 0.00000 <= threshold 0.00260. Skipping...\n",
      "\n",
      "=== ITERATION 37/510 SKIPPED: 10 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.829689872264862, buffer size: 58\n",
      "\n",
      "=== ITERATION 38/510 SKIPPED: 10 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 4, 'start_pos': (3, 6), 'goal_pos': (5, 5), 'edited': False, 'seed_val': 325453}\n",
      "Regret for current level: 0.03176110841743649, buffer size: 59\n",
      "\n",
      "=== ITERATION 39/510 SKIPPED: 10 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.6680152833461761, buffer size: 60\n",
      "\n",
      "=== ITERATION 40/510 SKIPPED: 10 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 20, 'start_pos': (6, 2), 'goal_pos': (5, 1), 'edited': False, 'seed_val': 205285}\n",
      "Regret for current level is 0.00000 <= threshold 0.00290. Skipping...\n",
      "\n",
      "=== ITERATION 41/511 SKIPPED: 11 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.6214318096637725, buffer size: 61\n",
      "\n",
      "=== ITERATION 42/511 SKIPPED: 11 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 8, 'start_pos': (4, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 775458}\n",
      "Regret for current level: 0.15378792643547057, buffer size: 62\n",
      "\n",
      "=== ITERATION 43/511 SKIPPED: 11 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.5605418801307678, buffer size: 63\n",
      "\n",
      "=== ITERATION 44/511 SKIPPED: 11 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 19, 'start_pos': (3, 2), 'goal_pos': (4, 1), 'edited': False, 'seed_val': 874987}\n",
      "Regret for current level is 0.00000 <= threshold 0.00320. Skipping...\n",
      "\n",
      "=== ITERATION 45/512 SKIPPED: 12 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.6192052781581878, buffer size: 64\n",
      "\n",
      "=== ITERATION 46/512 SKIPPED: 12 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 13, 'start_pos': (1, 6), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 588047}\n",
      "Regret for current level is 0.00000 <= threshold 0.00330. Skipping...\n",
      "\n",
      "=== ITERATION 47/513 SKIPPED: 13 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (2, 2), 'goal_pos': (6, 1), 'edited': False, 'seed_val': 929292}\n",
      "Regret for current level is 0.00000 <= threshold 0.00330. Skipping...\n",
      "\n",
      "=== ITERATION 48/514 SKIPPED: 14 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.6389871060848236, buffer size: 64\n",
      "\n",
      "=== ITERATION 49/514 SKIPPED: 14 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 2, 'start_pos': (4, 3), 'goal_pos': (1, 3), 'edited': False, 'seed_val': 544505}\n",
      "Regret for current level: 0.10079659253358841, buffer size: 64\n",
      "\n",
      "=== ITERATION 50/514 SKIPPED: 14 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 2, 'start_pos': (1, 4), 'goal_pos': (1, 1), 'edited': False, 'seed_val': 504030}\n",
      "Regret for current level: 0.11808756871530042, buffer size: 64\n",
      "\n",
      "=== ITERATION 51/514 SKIPPED: 14 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 3, 'start_pos': (4, 6), 'goal_pos': (4, 1), 'edited': False, 'seed_val': 464171}\n",
      "Regret for current level: 0.24955283038884402, buffer size: 64\n",
      "\n",
      "=== ITERATION 52/514 SKIPPED: 14 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.6610951542854309, buffer size: 64\n",
      "\n",
      "=== ITERATION 53/514 SKIPPED: 14 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 8, 'start_pos': (4, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 775458}\n",
      "Regret for current level: 0.15521268129348753, buffer size: 64\n",
      "\n",
      "=== ITERATION 54/514 SKIPPED: 14 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 23, 'start_pos': (4, 4), 'goal_pos': (3, 2), 'edited': False, 'seed_val': 195825}\n",
      "Regret for current level is 0.00000 <= threshold 0.00390. Skipping...\n",
      "\n",
      "=== ITERATION 55/515 SKIPPED: 15 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 2, 'start_pos': (1, 4), 'goal_pos': (1, 1), 'edited': False, 'seed_val': 504030}\n",
      "Regret for current level: 0.09229539566099482, buffer size: 64\n",
      "\n",
      "=== ITERATION 56/515 SKIPPED: 15 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 8, 'start_pos': (2, 4), 'goal_pos': (6, 3), 'edited': False, 'seed_val': 515233}\n",
      "Regret for current level is 0.00185 <= threshold 0.00400. Skipping...\n",
      "\n",
      "=== ITERATION 57/516 SKIPPED: 16 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 4, 'start_pos': (4, 1), 'goal_pos': (1, 1), 'edited': False, 'seed_val': 229217}\n",
      "Regret for current level is 0.00000 <= threshold 0.00400. Skipping...\n",
      "\n",
      "=== ITERATION 58/517 SKIPPED: 17 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 11, 'start_pos': (1, 6), 'goal_pos': (5, 6), 'edited': False, 'seed_val': 743185}\n",
      "Regret for current level: 0.05275363484030142, buffer size: 64\n",
      "\n",
      "=== ITERATION 59/517 SKIPPED: 17 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.8271778643131256, buffer size: 64\n",
      "\n",
      "=== ITERATION 60/517 SKIPPED: 17 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 8, 'start_pos': (4, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 775458}\n",
      "Regret for current level: 0.15251222729682923, buffer size: 64\n",
      "\n",
      "=== ITERATION 61/517 SKIPPED: 17 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 14, 'start_pos': (4, 1), 'goal_pos': (6, 4), 'edited': False, 'seed_val': 358950}\n",
      "Regret for current level is 0.00000 <= threshold 0.00430. Skipping...\n",
      "\n",
      "=== ITERATION 62/518 SKIPPED: 18 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 8, 'start_pos': (4, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 775458}\n",
      "Regret for current level: 0.14810543835163115, buffer size: 64\n",
      "\n",
      "=== ITERATION 63/518 SKIPPED: 18 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.7774329304695129, buffer size: 64\n",
      "\n",
      "=== ITERATION 64/518 SKIPPED: 18 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 16, 'start_pos': (4, 2), 'goal_pos': (3, 6), 'edited': False, 'seed_val': 576067}\n",
      "Regret for current level is 0.00000 <= threshold 0.00450. Skipping...\n",
      "\n",
      "=== ITERATION 65/519 SKIPPED: 19 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.7320251584053039, buffer size: 64\n",
      "\n",
      "=== ITERATION 66/519 SKIPPED: 19 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.6885473251342773, buffer size: 64\n",
      "\n",
      "=== ITERATION 67/519 SKIPPED: 19 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.5872453272342681, buffer size: 64\n",
      "\n",
      "=== ITERATION 68/519 SKIPPED: 19 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 6, 'start_pos': (5, 2), 'goal_pos': (4, 1), 'edited': False, 'seed_val': 510981}\n",
      "Regret for current level: 0.02171582341194156, buffer size: 64\n",
      "\n",
      "=== ITERATION 69/519 SKIPPED: 19 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.6569754004478454, buffer size: 64\n",
      "\n",
      "=== ITERATION 70/519 SKIPPED: 19 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.5551236391067504, buffer size: 64\n",
      "\n",
      "=== ITERATION 71/519 SKIPPED: 19 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 20, 'start_pos': (6, 2), 'goal_pos': (2, 5), 'edited': False, 'seed_val': 289914}\n",
      "Regret for current level: 0.06252772361040115, buffer size: 64\n",
      "\n",
      "=== ITERATION 72/519 SKIPPED: 19 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 8, 'start_pos': (2, 6), 'goal_pos': (1, 4), 'edited': False, 'seed_val': 958014}\n",
      "Regret for current level: 0.07551373004913331, buffer size: 64\n",
      "\n",
      "=== ITERATION 73/519 SKIPPED: 19 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 12, 'start_pos': (2, 1), 'goal_pos': (6, 5), 'edited': False, 'seed_val': 795401}\n",
      "Regret for current level is 0.00000 <= threshold 0.00530. Skipping...\n",
      "\n",
      "=== ITERATION 74/520 SKIPPED: 20 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 12, 'start_pos': (5, 5), 'goal_pos': (5, 6), 'edited': False, 'seed_val': 207656}\n",
      "Regret for current level is 0.00000 <= threshold 0.00530. Skipping...\n",
      "\n",
      "=== ITERATION 75/521 SKIPPED: 21 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 9, 'start_pos': (2, 4), 'goal_pos': (2, 6), 'edited': False, 'seed_val': 664834}\n",
      "Regret for current level is 0.00000 <= threshold 0.00530. Skipping...\n",
      "\n",
      "=== ITERATION 76/522 SKIPPED: 22 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 10, 'start_pos': (1, 3), 'goal_pos': (6, 3), 'edited': False, 'seed_val': 942911}\n",
      "Regret for current level: 0.29564758191539703, buffer size: 64\n",
      "\n",
      "=== ITERATION 77/522 SKIPPED: 22 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 11, 'start_pos': (2, 2), 'goal_pos': (2, 3), 'edited': False, 'seed_val': 411352}\n",
      "Regret for current level: 0.7242274701595306, buffer size: 64\n",
      "\n",
      "=== ITERATION 78/522 SKIPPED: 22 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 18, 'start_pos': (4, 3), 'goal_pos': (6, 6), 'edited': False, 'seed_val': 940138}\n",
      "Regret for current level is 0.00000 <= threshold 0.00550. Skipping...\n",
      "\n",
      "=== ITERATION 79/523 SKIPPED: 23 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.6449468851089477, buffer size: 64\n",
      "\n",
      "=== ITERATION 80/523 SKIPPED: 23 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 2, 'start_pos': (3, 1), 'goal_pos': (2, 6), 'edited': False, 'seed_val': 85100}\n",
      "Regret for current level: 0.3503779762983322, buffer size: 64\n",
      "\n",
      "=== ITERATION 81/523 SKIPPED: 23 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.5030056655406951, buffer size: 64\n",
      "\n",
      "=== ITERATION 82/523 SKIPPED: 23 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 17, 'start_pos': (1, 1), 'goal_pos': (6, 5), 'edited': False, 'seed_val': 626444}\n",
      "Regret for current level is 0.00000 <= threshold 0.00580. Skipping...\n",
      "\n",
      "=== ITERATION 83/524 SKIPPED: 24 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 14, 'start_pos': (2, 3), 'goal_pos': (4, 2), 'edited': False, 'seed_val': 133942}\n",
      "Regret for current level: 0.36133776533424855, buffer size: 64\n",
      "\n",
      "=== ITERATION 84/524 SKIPPED: 24 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 1, 'start_pos': (3, 5), 'goal_pos': (4, 2), 'edited': False, 'seed_val': 354949}\n",
      "Regret for current level: 0.283496643478896, buffer size: 64\n",
      "\n",
      "=== ITERATION 85/524 SKIPPED: 24 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.5493816256523132, buffer size: 64\n",
      "\n",
      "=== ITERATION 86/524 SKIPPED: 24 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.4680674791336059, buffer size: 64\n",
      "\n",
      "=== ITERATION 87/524 SKIPPED: 24 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 23, 'start_pos': (4, 4), 'goal_pos': (3, 3), 'edited': False, 'seed_val': 171067}\n",
      "Regret for current level: 0.2700437328869104, buffer size: 64\n",
      "\n",
      "=== ITERATION 88/524 SKIPPED: 24 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 2, 'start_pos': (3, 1), 'goal_pos': (2, 6), 'edited': False, 'seed_val': 85100}\n",
      "Regret for current level: 0.294885892868042, buffer size: 64\n",
      "\n",
      "=== ITERATION 89/524 SKIPPED: 24 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.5291613757610321, buffer size: 64\n",
      "\n",
      "=== ITERATION 90/524 SKIPPED: 24 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 3), 'goal_pos': (3, 1), 'edited': False, 'seed_val': 916702}\n",
      "Regret for current level is 0.00000 <= threshold 0.00650. Skipping...\n",
      "\n",
      "=== ITERATION 91/525 SKIPPED: 25 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 11, 'start_pos': (2, 2), 'goal_pos': (2, 3), 'edited': False, 'seed_val': 411352}\n",
      "Regret for current level: 0.3798198223114013, buffer size: 64\n",
      "\n",
      "=== ITERATION 92/525 SKIPPED: 25 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 2, 'start_pos': (1, 4), 'goal_pos': (1, 1), 'edited': False, 'seed_val': 504030}\n",
      "Regret for current level: 0.1732050157158246, buffer size: 64\n",
      "\n",
      "=== ITERATION 93/525 SKIPPED: 25 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 9, 'start_pos': (6, 4), 'goal_pos': (5, 4), 'edited': False, 'seed_val': 530097}\n",
      "Regret for current level: 0.451800000667572, buffer size: 64\n",
      "\n",
      "=== ITERATION 94/525 SKIPPED: 25 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 8, 'start_pos': (4, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 775458}\n",
      "Regret for current level: 0.14821664217428449, buffer size: 64\n",
      "\n",
      "=== ITERATION 95/525 SKIPPED: 25 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 4, 'start_pos': (6, 5), 'goal_pos': (5, 2), 'edited': False, 'seed_val': 205922}\n",
      "Regret for current level: 0.3327109139869679, buffer size: 64\n",
      "\n",
      "=== ITERATION 96/525 SKIPPED: 25 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.33346713781356807, buffer size: 64\n",
      "\n",
      "=== ITERATION 97/525 SKIPPED: 25 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.3150639653205871, buffer size: 64\n",
      "\n",
      "=== ITERATION 98/525 SKIPPED: 25 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 9, 'start_pos': (2, 4), 'goal_pos': (4, 4), 'edited': False, 'seed_val': 736918}\n",
      "Regret for current level: 0.09087981475867392, buffer size: 64\n",
      "\n",
      "=== ITERATION 99/525 SKIPPED: 25 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 7, 'start_pos': (3, 4), 'goal_pos': (1, 6), 'edited': False, 'seed_val': 574415}\n",
      "Regret for current level: 0.07362550151448592, buffer size: 64\n",
      "\n",
      "=== ITERATION 100/525 SKIPPED: 25 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 21, 'start_pos': (3, 5), 'goal_pos': (5, 3), 'edited': False, 'seed_val': 683716}\n",
      "Regret for current level: 0.2305370828962326, buffer size: 64\n",
      "\n",
      "=== ITERATION 101/525 SKIPPED: 25 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.2859667062759399, buffer size: 64\n",
      "\n",
      "=== ITERATION 102/525 SKIPPED: 25 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 12, 'start_pos': (1, 6), 'goal_pos': (4, 2), 'edited': False, 'seed_val': 31669}\n",
      "Regret for current level: 0.12930039283395264, buffer size: 64\n",
      "\n",
      "=== ITERATION 103/525 SKIPPED: 25 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 11, 'start_pos': (5, 4), 'goal_pos': (6, 6), 'edited': False, 'seed_val': 536402}\n",
      "Regret for current level: 0.1880594783869395, buffer size: 64\n",
      "\n",
      "=== ITERATION 104/525 SKIPPED: 25 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 19, 'start_pos': (5, 4), 'goal_pos': (4, 5), 'edited': False, 'seed_val': 753696}\n",
      "Regret for current level is 0.00000 <= threshold 0.00780. Skipping...\n",
      "\n",
      "=== ITERATION 105/526 SKIPPED: 26 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 14, 'start_pos': (5, 3), 'goal_pos': (5, 6), 'edited': False, 'seed_val': 676685}\n",
      "Regret for current level: 0.14813513319420712, buffer size: 64\n",
      "\n",
      "=== ITERATION 106/526 SKIPPED: 26 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 4, 'start_pos': (6, 2), 'goal_pos': (1, 3), 'edited': False, 'seed_val': 938431}\n",
      "Regret for current level: 0.08631100466658768, buffer size: 64\n",
      "\n",
      "=== ITERATION 107/526 SKIPPED: 26 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 2, 'start_pos': (1, 4), 'goal_pos': (1, 1), 'edited': False, 'seed_val': 504030}\n",
      "Regret for current level: 0.45716931811585454, buffer size: 64\n",
      "\n",
      "=== ITERATION 108/526 SKIPPED: 26 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 10, 'start_pos': (1, 3), 'goal_pos': (5, 6), 'edited': False, 'seed_val': 898701}\n",
      "Regret for current level is 0.00000 <= threshold 0.00810. Skipping...\n",
      "\n",
      "=== ITERATION 109/527 SKIPPED: 27 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 2, 'start_pos': (1, 4), 'goal_pos': (1, 1), 'edited': False, 'seed_val': 504030}\n",
      "Regret for current level: 0.5107426821247639, buffer size: 64\n",
      "\n",
      "=== ITERATION 110/527 SKIPPED: 27 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 2, 'start_pos': (1, 4), 'goal_pos': (1, 1), 'edited': False, 'seed_val': 504030}\n",
      "Regret for current level: 0.4669652471170083, buffer size: 64\n",
      "\n",
      "=== ITERATION 111/527 SKIPPED: 27 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.30481721162796016, buffer size: 64\n",
      "\n",
      "=== ITERATION 112/527 SKIPPED: 27 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 10, 'start_pos': (6, 6), 'goal_pos': (2, 5), 'edited': False, 'seed_val': 349597}\n",
      "Regret for current level: 0.09450404763221742, buffer size: 64\n",
      "\n",
      "=== ITERATION 113/527 SKIPPED: 27 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.2872874855995178, buffer size: 64\n",
      "\n",
      "=== ITERATION 114/527 SKIPPED: 27 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 17, 'start_pos': (3, 1), 'goal_pos': (1, 1), 'edited': False, 'seed_val': 36931}\n",
      "Regret for current level is 0.00000 <= threshold 0.00860. Skipping...\n",
      "\n",
      "=== ITERATION 115/528 SKIPPED: 28 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 13, 'start_pos': (3, 5), 'goal_pos': (5, 5), 'edited': False, 'seed_val': 733272}\n",
      "Regret for current level: 0.3117594162991901, buffer size: 64\n",
      "\n",
      "=== ITERATION 116/528 SKIPPED: 28 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.23143113851547237, buffer size: 64\n",
      "\n",
      "=== ITERATION 117/528 SKIPPED: 28 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 2, 'start_pos': (5, 1), 'goal_pos': (4, 3), 'edited': False, 'seed_val': 421243}\n",
      "Regret for current level: 0.11193243744045497, buffer size: 64\n",
      "\n",
      "=== ITERATION 118/528 SKIPPED: 28 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 5, 'start_pos': (5, 1), 'goal_pos': (2, 1), 'edited': False, 'seed_val': 56608}\n",
      "Regret for current level: 0.28075226281398186, buffer size: 64\n",
      "\n",
      "=== ITERATION 119/528 SKIPPED: 28 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (2, 5), 'goal_pos': (3, 6), 'edited': False, 'seed_val': 450907}\n",
      "Regret for current level: 0.33164524112233673, buffer size: 64\n",
      "\n",
      "=== ITERATION 120/528 SKIPPED: 28 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.2496912598609924, buffer size: 64\n",
      "\n",
      "=== ITERATION 121/528 SKIPPED: 28 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.20932800769805904, buffer size: 64\n",
      "\n",
      "=== ITERATION 122/528 SKIPPED: 28 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 17, 'start_pos': (4, 1), 'goal_pos': (3, 3), 'edited': False, 'seed_val': 958220}\n",
      "Regret for current level is 0.00000 <= threshold 0.00930. Skipping...\n",
      "\n",
      "=== ITERATION 123/529 SKIPPED: 29 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.18831033706665035, buffer size: 64\n",
      "\n",
      "=== ITERATION 124/529 SKIPPED: 29 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (4, 1), 'goal_pos': (5, 6), 'edited': False, 'seed_val': 287439}\n",
      "Regret for current level: 0.0196318833822012, buffer size: 64\n",
      "\n",
      "=== ITERATION 125/529 SKIPPED: 29 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (2, 5), 'goal_pos': (3, 6), 'edited': False, 'seed_val': 450907}\n",
      "Regret for current level: 0.26070367630057023, buffer size: 64\n",
      "\n",
      "=== ITERATION 126/529 SKIPPED: 29 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 14, 'start_pos': (5, 3), 'goal_pos': (4, 2), 'edited': False, 'seed_val': 41498}\n",
      "Regret for current level is 0.00000 <= threshold 0.00960. Skipping...\n",
      "\n",
      "=== ITERATION 127/530 SKIPPED: 30 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.17212398052215572, buffer size: 64\n",
      "\n",
      "=== ITERATION 128/530 SKIPPED: 30 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.16287000179290767, buffer size: 64\n",
      "\n",
      "=== ITERATION 129/530 SKIPPED: 30 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 8, 'start_pos': (4, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 775458}\n",
      "Regret for current level: 0.13385009185902536, buffer size: 64\n",
      "\n",
      "=== ITERATION 130/530 SKIPPED: 30 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 2, 'start_pos': (5, 2), 'goal_pos': (1, 2), 'edited': False, 'seed_val': 201310}\n",
      "Regret for current level: 0.22755195738384343, buffer size: 64\n",
      "\n",
      "=== ITERATION 131/530 SKIPPED: 30 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.1691640734672546, buffer size: 64\n",
      "\n",
      "=== ITERATION 132/530 SKIPPED: 30 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.1446310997009277, buffer size: 64\n",
      "\n",
      "=== ITERATION 133/530 SKIPPED: 30 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 23, 'start_pos': (4, 2), 'goal_pos': (3, 4), 'edited': False, 'seed_val': 915812}\n",
      "Regret for current level: 0.09560345277503654, buffer size: 64\n",
      "\n",
      "=== ITERATION 134/530 SKIPPED: 30 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 2, 'start_pos': (1, 4), 'goal_pos': (1, 1), 'edited': False, 'seed_val': 504030}\n",
      "Regret for current level: 0.3723855205041234, buffer size: 64\n",
      "\n",
      "=== ITERATION 135/530 SKIPPED: 30 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.15578300952911373, buffer size: 64\n",
      "\n",
      "=== ITERATION 136/530 SKIPPED: 30 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.14893974065780635, buffer size: 64\n",
      "\n",
      "=== ITERATION 137/530 SKIPPED: 30 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 22, 'start_pos': (1, 2), 'goal_pos': (2, 2), 'edited': False, 'seed_val': 47910}\n",
      "Regret for current level: 0.13549555540084834, buffer size: 64\n",
      "\n",
      "=== ITERATION 138/530 SKIPPED: 30 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 11, 'start_pos': (4, 3), 'goal_pos': (3, 5), 'edited': False, 'seed_val': 826911}\n",
      "Regret for current level: 0.23325510187178847, buffer size: 64\n",
      "\n",
      "=== ITERATION 139/530 SKIPPED: 30 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 8, 'start_pos': (2, 4), 'goal_pos': (3, 1), 'edited': False, 'seed_val': 310962}\n",
      "Regret for current level: 0.08609474746179645, buffer size: 64\n",
      "\n",
      "=== ITERATION 140/530 SKIPPED: 30 ===\n",
      "Sampled level from buffer: {'width': 8, 'height': 8, 'num_blocks': 1, 'start_pos': (3, 5), 'goal_pos': (4, 2), 'edited': False, 'seed_val': 354949}\n",
      "Regret for current level: 0.1330484354496002, buffer size: 64\n",
      "\n",
      "=== ITERATION 141/530 SKIPPED: 30 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 16, 'start_pos': (6, 6), 'goal_pos': (3, 4), 'edited': False, 'seed_val': 839044}\n",
      "Regret for current level: 0.11901107334315772, buffer size: 64\n",
      "\n",
      "=== ITERATION 142/530 SKIPPED: 30 ===\n",
      "Generated new random level: {'width': 8, 'height': 8, 'num_blocks': 6, 'start_pos': (5, 6), 'goal_pos': (1, 5), 'edited': False, 'seed_val': 665721}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "        \n",
    "    wandb.finish()\n",
    "        \n",
    "    config = {\n",
    "            \"grid_size\": 8,\n",
    "            \n",
    "            \"total_iterations\": 500,\n",
    "            \"train_steps\": 1000,\n",
    "\n",
    "            \"replay_prob\": 0.5,            # Probability of replaying a level and editing it vs. generating a new one\n",
    "            \"level_buffer_size\": 64,       # Maximum number of levels to store in the buffer\n",
    "            \"initial_fill_size\": 32,       # Number of levels to pre-fill the buffer with\n",
    "            \"regret_threshold\": 0.00,      # Minimum regret threshold to consider a level for the buffer\n",
    "            \n",
    "            \"n_envs\": 3,                   # Number of parallel environments to use for training\n",
    "            \n",
    "            \"edit_levels\": True,           # Whether to edit levels during training i.e. ACCEL or PLR\n",
    "            \"easy_start\": True,            # Whether to fill the buffer with easy levels first i.e. minimum number of blocks\n",
    "    \n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "    config[\"edit_levels\"] = False\n",
    "    config[\"easy_start\"] = False\n",
    "    print(f\"Running PLR with config: {config}\")\n",
    "    model_plr = main_accel(**config)\n",
    "\n",
    "    # Save the model\n",
    "    model_plr.save(f\"models/plr_model_{config['grid_size']}x{config['grid_size']}\")\n",
    "\n",
    "    print(\"\\n\\n============================================\\n\\n\")\n",
    "\n",
    "    config[\"edit_levels\"] = True\n",
    "    config[\"easy_start\"] = False\n",
    "    print(f\"Running ACCEL with config: {config}\")\n",
    "    model_accel = main_accel(**config)\n",
    "\n",
    "    # Save the model\n",
    "    model_accel.save(f\"models/accel_model_{config['grid_size']}x{config['grid_size']}\")\n",
    "\n",
    "    print(\"\\n\\n============================================\\n\\n\")\n",
    "    \n",
    "\n",
    "    config[\"edit_levels\"] = True\n",
    "    config[\"easy_start\"] = True\n",
    "    print(f\"Running ACCEL with easy start with config: {config}\")\n",
    "    model_accel_easy = main_accel(**config)\n",
    "\n",
    "    # Save the model\n",
    "    model_accel_easy.save(f\"models/accel_model_easy_{config['grid_size']}x{config['grid_size']}\")\n",
    "    \n",
    "    # Evaluate the models\n",
    "    evalute_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 25\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Test the models on a few ranom levels\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m30\u001b[39m):\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m#test_model(model_plr, random_config(6))\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m#test_model(model_accel, random_config(6))\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m     \u001b[43mtest_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_accel_easy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03mmodels = {\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03m    \"PLR\": model_plr,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;124;03m        print()\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;124;03m    print()\"\"\"\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[66], line 3\u001b[0m, in \u001b[0;36mtest_model\u001b[1;34m(model, config)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_model\u001b[39m(model, config):\n\u001b[0;32m      2\u001b[0m     env \u001b[38;5;241m=\u001b[39m MyCustomGrid(config, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m, solvable_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 3\u001b[0m     obs, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     terminated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     truncated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[53], line 176\u001b[0m, in \u001b[0;36mMyCustomGrid.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    172\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;124;03m    Override reset to ensure we only return the 'image' array\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;124;03m    instead of a dict with 'image' and 'mission'.\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 176\u001b[0m     obs, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m     obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_obs(obs)\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obs, info\n",
      "File \u001b[1;32mc:\\Users\\Emanuele\\miniconda3\\envs\\accel\\Lib\\site-packages\\minigrid\\minigrid_env.py:132\u001b[0m, in \u001b[0;36mMiniGridEnv.reset\u001b[1;34m(self, seed, options)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;66;03m# Generate a new random grid at the start of each episode\u001b[39;00m\n\u001b[1;32m--> 132\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gen_grid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# These fields should be defined by _gen_grid\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_pos \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_pos, \u001b[38;5;28mtuple\u001b[39m)\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_pos \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_dir \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    139\u001b[0m )\n",
      "Cell \u001b[1;32mIn[53], line 136\u001b[0m, in \u001b[0;36mMyCustomGrid._gen_grid\u001b[1;34m(self, width, height)\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# If not solvable, change seed and try again\u001b[39;00m\n\u001b[1;32m--> 136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrng \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mdefault_rng(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrng\u001b[38;5;241m.\u001b[39mintegers(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m999999\u001b[39m))\n",
      "File \u001b[1;32mnumpy\\\\random\\\\_generator.pyx:4957\u001b[0m, in \u001b[0;36mnumpy.random._generator.default_rng\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pcg64.pyx:132\u001b[0m, in \u001b[0;36mnumpy.random._pcg64.PCG64.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Emanuele\\miniconda3\\envs\\accel\\Lib\\contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 81\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def test_model(model, config):\n",
    "    env = MyCustomGrid(config, render_mode='human', solvable_only=True)\n",
    "    obs, _ = env.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    total_reward = 0\n",
    "    i = 0\n",
    "\n",
    "    # Continue until either terminated or truncated is True\n",
    "    while not (terminated or truncated):\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        i += 1\n",
    "        if i > 20:\n",
    "            break\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "# Test the models on a few ranom levels\n",
    "for i in range(30):\n",
    "    #test_model(model_plr, random_config(6))\n",
    "    #test_model(model_accel, random_config(6))\n",
    "    test_model(model_accel_easy, random_config(6))\n",
    "\"\"\"\n",
    "models = {\n",
    "    \"PLR\": model_plr,\n",
    "    \"ACCEL\": model_accel,\n",
    "    \"ACCEL-EasyStart\": model_accel_easy\n",
    "}\n",
    "    \n",
    "# Test the models on previously evaluated levels\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Testing model {model_name} on previously evaluated levels...\")\n",
    "    for i, level in enumerate(levels):\n",
    "        print(f\"Level {i + 1} - Complexity {(i+2)**2}:\")\n",
    "        for j, cfg in enumerate(level):\n",
    "            mean_reward = test_model(model, cfg)\n",
    "            print(f\"  Config {j + 1}: {mean_reward:.2f}\")\n",
    "        print()\n",
    "    print()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level Editor Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random level and visualize it\n",
    "random_cnf = random_config(8)\n",
    "print(\"random_cnf:\", random_cnf)\n",
    "\n",
    "print_level_from_config(random_cnf, solvable_only=True)\n",
    "'''\n",
    "# Edit the random level and visualize it\n",
    "edited_config = edit_config(random_cnf)\n",
    "print_level_from_config(edited_config)\n",
    "print(\"edited_config:\", edited_config)\n",
    "\n",
    "# Edit the random level and visualize it\n",
    "edited_config = edit_config(random_cnf)\n",
    "print_level_from_config(edited_config)\n",
    "print(\"edited_config:\", edited_config)\n",
    "\n",
    "# Edit the random level and visualize it\n",
    "edited_config = edit_config(random_cnf)\n",
    "print_level_from_config(edited_config)\n",
    "print(\"edited_config:\", edited_config)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bipedal Walker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://gymnasium.farama.org/environments/box2d/bipedal_walker/\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.utils import obs_as_tensor\n",
    "import random\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "class BipedalWalkerParamWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    A wrapper around BipedalWalker (or BipedalWalkerHardcore) \n",
    "    that sets custom parameters for terrain generation at reset time.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env_id=\"BipedalWalker-v3\", hardcore=False):\n",
    "        super().__init__(gym.make(env_id))\n",
    "        self.hardcore = hardcore\n",
    "\n",
    "        # The environment's internal parameters. \n",
    "        # We will override these at reset to control terrain generation.\n",
    "        self.config = {\n",
    "            \"seed_val\": None,\n",
    "            \"stump_height\": 0.0,\n",
    "            \"stair_height\": 0.0,\n",
    "            \"stair_steps\": 1,\n",
    "            \"roughness\": 0.0,\n",
    "            \"pit_gap\": 0.0,\n",
    "        }\n",
    "\n",
    "        # If we want hardcore version, you can do so with:\n",
    "        if self.hardcore:\n",
    "            self.env = gym.make(\"BipedalWalkerHardcore-v3\")\n",
    "\n",
    "    def set_config(self, config_dict):\n",
    "        \"\"\"Update environment parameters.\"\"\"\n",
    "        for k, v in config_dict.items():\n",
    "            self.config[k] = v\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Modify the environment's terrain parameters right before reset.\n",
    "        This monkey-patches internal Box2D variables, if needed,\n",
    "        or sets seeds to randomize terrain accordingly.\n",
    "        \"\"\"\n",
    "        # 1) Seed:\n",
    "        #if self.config[\"seed_val\"] is not None:\n",
    "        #    self.env.seed(self.config[\"seed_val\"])\n",
    "\n",
    "        # 2) Override terrain parameters in the underlying env\n",
    "        #    (We rely on the environment reading these at reset or having\n",
    "        #     references in the terrain generation code. Depending on \n",
    "        #     your exact BipedalWalker implementation, you might need \n",
    "        #     to modify the source or do partial overrides.)\n",
    "        self.env.unwrapped.stump_height = self.config[\"stump_height\"]\n",
    "        self.env.unwrapped.stair_height = self.config[\"stair_height\"]\n",
    "        self.env.unwrapped.stair_steps = int(self.config[\"stair_steps\"])\n",
    "        self.env.unwrapped.roughness = self.config[\"roughness\"]\n",
    "        self.env.unwrapped.pit_gap = self.config[\"pit_gap\"]\n",
    "\n",
    "        obs = self.env.reset(**kwargs)\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "    \n",
    "\n",
    "\n",
    "# The min and max values for each terrain parameter.\n",
    "# For simplicity, define them here, but you can store these in a table/dict.\n",
    "PARAM_BOUNDS = {\n",
    "    \"stump_height\":  (0.0, 5.0),\n",
    "    \"stair_height\":  (0.0, 5.0),\n",
    "    \"stair_steps\":   (1,   9),\n",
    "    \"roughness\":     (0.0, 10.0),\n",
    "    \"pit_gap\":       (0.0, 10.0)\n",
    "}\n",
    "\n",
    "def random_config(easy_init=False):\n",
    "    \"\"\"\n",
    "    Sample a random environment config. If easy_init=True,\n",
    "    you might restrict sampling to small values, so the agent\n",
    "    starts with simpler terrain.\n",
    "    \"\"\"\n",
    "    cfg = {}\n",
    "    \n",
    "    # Optionally set a random seed \n",
    "    cfg[\"seed_val\"] = random.randint(0, 1_000_000)\n",
    "\n",
    "    def sample_uniform(low, high):\n",
    "        return random.uniform(low, high)\n",
    "\n",
    "    if easy_init:\n",
    "        # Possibly narrower range for simpler (initial) levels\n",
    "        cfg[\"stump_height\"] = sample_uniform(0.0, 0.4)\n",
    "        cfg[\"stair_height\"] = sample_uniform(0.0, 0.4)\n",
    "        cfg[\"stair_steps\"]  = random.randint(1, 2)\n",
    "        cfg[\"roughness\"]    = sample_uniform(0.0, 0.6)\n",
    "        cfg[\"pit_gap\"]      = sample_uniform(0.0, 0.8)\n",
    "    else:\n",
    "        cfg[\"stump_height\"] = sample_uniform(*PARAM_BOUNDS[\"stump_height\"])\n",
    "        cfg[\"stair_height\"] = sample_uniform(*PARAM_BOUNDS[\"stair_height\"])\n",
    "        cfg[\"stair_steps\"]  = random.randint(*PARAM_BOUNDS[\"stair_steps\"])\n",
    "        cfg[\"roughness\"]    = sample_uniform(*PARAM_BOUNDS[\"roughness\"])\n",
    "        cfg[\"pit_gap\"]      = sample_uniform(*PARAM_BOUNDS[\"pit_gap\"])\n",
    "\n",
    "    return cfg\n",
    "\n",
    "def edit_config(old_cfg):\n",
    "    \"\"\"\n",
    "    Make a small 'mutation' to the old_cfg.\n",
    "    This can be random increments/decrements to each parameter, \n",
    "    or randomly choose one parameter to mutate.\n",
    "    \"\"\"\n",
    "    new_cfg = dict(old_cfg)\n",
    "    param_to_edit = random.choice([\"stump_height\", \"stair_height\", \"stair_steps\", \"roughness\", \"pit_gap\"])\n",
    "    # pick small delta \n",
    "    delta = random.uniform(0.1, 1.0)\n",
    "\n",
    "    # Add or subtract\n",
    "    sign = random.choice([-1, 1])\n",
    "    new_val = new_cfg[param_to_edit] + sign * delta\n",
    "    \n",
    "    # Clip to valid range\n",
    "    low, high = PARAM_BOUNDS[param_to_edit]\n",
    "    new_cfg[param_to_edit] = np.clip(new_val, low, high)\n",
    "\n",
    "    # Possibly update the seed too \n",
    "    new_cfg[\"seed_val\"] = random.randint(0, 1_000_000)\n",
    "    return new_cfg\n",
    "\n",
    "\n",
    "\n",
    "class LevelReplayBuffer:\n",
    "    \"\"\"\n",
    "    Stores (config, score). Score is e.g. a 'regret' approximation.\n",
    "    We keep the highest-score configs up to max_size.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_size=100):\n",
    "        self.max_size = max_size\n",
    "        self.data = []\n",
    "\n",
    "    def add(self, cfg, score):\n",
    "        self.data.append((cfg, score))\n",
    "        # keep only top K by score\n",
    "        self.data.sort(key=lambda x: x[1], reverse=True)\n",
    "        self.data = self.data[:self.max_size]\n",
    "\n",
    "    def sample(self):\n",
    "        if len(self.data) == 0:\n",
    "            return None\n",
    "        # Weighted sample by score or just pick the top \n",
    "        # For simplicity, pick randomly from top half:\n",
    "        half = len(self.data) // 2\n",
    "        idx = np.random.randint(0, max(1, half))\n",
    "        return self.data[idx][0]\n",
    "\n",
    "def estimate_regret(env, model, max_steps=1000, gamma=0.99, lam=0.95):\n",
    "    \"\"\"\n",
    "    Calculate regret using Generalized Advantage Estimation (GAE)\n",
    "    with Stable-Baselines3's PPO model.\n",
    "    \"\"\"\n",
    "    obs = env.reset()[0]\n",
    "    regrets = []\n",
    "    rewards = []\n",
    "    dones = []\n",
    "    values = []\n",
    "    \n",
    "\n",
    "    for t in range(max_steps):\n",
    "        # Add batch dimension to the observation tensor\n",
    "        obs_tensor = torch.as_tensor(obs).float().unsqueeze(0).to(device)\n",
    "        \n",
    "        # Use the model's policy to get the value and action.\n",
    "        # For actions, model.predict handles single observations well.\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        \n",
    "        # Compute the value from the policy.\n",
    "        value_t = model.policy.predict_values(obs_tensor).item()\n",
    "        values.append(value_t)\n",
    "        \n",
    "        # Perform the step in the environment\n",
    "        obs, reward, done, truncated, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "    # Add value of the terminal state (0 if done/truncated)\n",
    "    if done or truncated:\n",
    "        terminal_value = 0.0\n",
    "    else:\n",
    "        terminal_obs_tensor = torch.as_tensor(obs).float().unsqueeze(0).to(device)\n",
    "        terminal_value = model.policy.predict_values(terminal_obs_tensor).item()\n",
    "    values.append(terminal_value)\n",
    "\n",
    "    # Compute TD-errors and GAE-like regret score\n",
    "    for t in range(len(rewards)):\n",
    "        delta_t = rewards[t] + gamma * values[t + 1] * (1 - dones[t]) - values[t]\n",
    "        discounted_error = (gamma * lam) ** t * delta_t\n",
    "        regrets.append(max(0, discounted_error))\n",
    "\n",
    "    # Return the maximum positive regret score (or 0 if empty)\n",
    "    return max(regrets) if regrets else 0.0\n",
    "\n",
    "def run_accel_bipedal(\n",
    "    total_iterations=50,\n",
    "    steps_per_iteration=10000,\n",
    "    replay_prob=0.8,\n",
    "    regret_threshold=1.0,\n",
    "    max_buf_size=100,\n",
    "    easy_start=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Minimal ACCEL training loop on BipedalWalker.\n",
    "    \"\"\"\n",
    "    # 1) Create the environment wrapper + vectorize\n",
    "    def make_env():\n",
    "        env = BipedalWalkerParamWrapper(env_id=\"BipedalWalkerHardcore-v3\", hardcore=True)\n",
    "        return env\n",
    "    \n",
    "    # (Alternatively, do SubprocVecEnv if you want parallel CPU rollouts.)\n",
    "    vec_env = DummyVecEnv([make_env])\n",
    "\n",
    "    # 2) Initialize PPO\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\", \n",
    "        vec_env, \n",
    "        n_steps=2048,        # must be multiple of vec_env.num_envs\n",
    "        batch_size=64,\n",
    "        learning_rate=3e-4,\n",
    "        gamma=0.99,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    # 3) Create LevelReplayBuffer\n",
    "    level_buffer = LevelReplayBuffer(max_size=max_buf_size)\n",
    "\n",
    "    # 4) [Optional] Pre-fill buffer with some easy or random levels\n",
    "    for _ in range(10):  # pre-fill 10 levels\n",
    "        print(\"Pre-filling buffer...\")\n",
    "        cfg = random_config(easy_init=easy_start)\n",
    "        env = make_env()  # fresh environment\n",
    "        env.set_config(cfg)\n",
    "        # Train on it briefly to get a partial updated policy \n",
    "        model.set_env(DummyVecEnv([lambda: env]))\n",
    "        model.learn(total_timesteps=2000)\n",
    "\n",
    "        # Evaluate regret\n",
    "        rew_env = make_env()\n",
    "        rew_env.set_config(cfg)\n",
    "        regret = estimate_regret(rew_env, model)\n",
    "\n",
    "        if regret >= regret_threshold:\n",
    "            level_buffer.add(cfg, regret)\n",
    "        \n",
    "    \n",
    "    print(\"Buffer pre-filled. Starting main loop...\")\n",
    "\n",
    "    # 5) ACCEL main loop\n",
    "    for it in range(total_iterations):\n",
    "        # Decide: replay from buffer or create new\n",
    "        use_replay = (np.random.rand() < replay_prob) and (len(level_buffer.data) > 0)\n",
    "        if use_replay:\n",
    "            cfg = level_buffer.sample()\n",
    "            # Optionally edit the config to keep pushing frontier\n",
    "            cfg = edit_config(cfg)\n",
    "        else:\n",
    "            cfg = random_config(easy_init=False)\n",
    "\n",
    "        # Train on that config\n",
    "        env = make_env()\n",
    "        env.set_config(cfg)\n",
    "        model.set_env(DummyVecEnv([lambda: env]))\n",
    "        model.learn(total_timesteps=steps_per_iteration)\n",
    "\n",
    "        # Now measure regret\n",
    "        rew_env = make_env()\n",
    "        rew_env.set_config(cfg)\n",
    "        regret = estimate_regret(rew_env, model)\n",
    "\n",
    "        if regret >= regret_threshold:\n",
    "            level_buffer.add(cfg, regret)\n",
    "\n",
    "        print(f\"[Iteration {it+1}/{total_iterations}] => Config = {cfg}, Regret = {regret:.3f}, \"\n",
    "              f\"Buffer size = {len(level_buffer.data)}\")\n",
    "\n",
    "    return model, level_buffer\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trained_model, buffer_data = run_accel_bipedal(\n",
    "        total_iterations=250,\n",
    "        steps_per_iteration=1000,\n",
    "        replay_prob=0.8,\n",
    "        regret_threshold=1.0,\n",
    "        max_buf_size=100,\n",
    "        easy_start=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Evaluate final performance on some reference environment\n",
    "    test_env = gym.make(\"BipedalWalkerHardcore-v3\", render_mode='human')\n",
    "    mean_return = 0.0\n",
    "    N = 10\n",
    "    for _ in range(N):\n",
    "        obs = test_env.reset()[0]\n",
    "        done = False\n",
    "        episode_reward = 0.0\n",
    "        while not done:\n",
    "            action, _ = trained_model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, truncated, info = test_env.step(action)\n",
    "            episode_reward += reward\n",
    "            if truncated:\n",
    "                done = True\n",
    "        mean_return += episode_reward\n",
    "    mean_return /= N\n",
    "    print(f\"Avg Return on BipedalWalkerHardcore-v3 over {N} trials: {mean_return}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maze generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_maze(width, height):\n",
    "    maze = [[1 for _ in range(width)] for _ in range(height)]  # 1 for walls\n",
    "    stack = []\n",
    "    directions = [(0, 2), (0, -2), (2, 0), (-2, 0)]\n",
    "\n",
    "    def is_valid(x, y):\n",
    "        return 0 < x < height - 1 and 0 < y < width - 1 and maze[x][y] == 1\n",
    "\n",
    "    def carve(x, y):\n",
    "        maze[x][y] = 0\n",
    "        random.shuffle(directions)\n",
    "        for dx, dy in directions:\n",
    "            nx, ny = x + dx, y + dy\n",
    "            if is_valid(nx, ny):\n",
    "                maze[x + dx // 2][y + dy // 2] = 0\n",
    "                carve(nx, ny)\n",
    "\n",
    "    carve(1, 1)  # Start at (1, 1)\n",
    "\n",
    "    return maze\n",
    "\n",
    "# Display the maze\n",
    "def print_maze(maze):\n",
    "    for row in maze:\n",
    "        print(\"\".join(\"█\" if cell == 1 else \" \" for cell in row))\n",
    "\n",
    "maze = generate_maze(21, 21)\n",
    "print_maze(maze)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomMaze(MiniGridEnv):\n",
    "    \"\"\"\n",
    "    Simple MiniGrid environment that places random wall tiles\n",
    "    according to a config dict, returning only the 'image' observation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config=None, **kwargs):\n",
    "        if config is None:\n",
    "            config = {}\n",
    "        self.config = config\n",
    "\n",
    "        # Extract parameters from config\n",
    "        self.width = config.get(\"width\")\n",
    "        self.height = config.get(\"height\")\n",
    "        self.num_blocks = config.get(\"num_blocks\")\n",
    "        self.custom_seed = config.get(\"seed_val\")\n",
    "        \n",
    "        \n",
    "        # Create a random number generator with the custom seed\n",
    "        self.rng = np.random.default_rng(seed=self.custom_seed)\n",
    "\n",
    "        grid_size = max(self.width, self.height)\n",
    "\n",
    "        mission_space = MissionSpace(mission_func=lambda: \"get to the green goal square\")\n",
    "\n",
    "        super().__init__(\n",
    "            grid_size=grid_size,\n",
    "            max_steps=self.width * self.height * 2, # max_steps is typically 2x the grid size\n",
    "            see_through_walls=False,\n",
    "            agent_view_size=5,                      # Size of the agent's view square\n",
    "            mission_space=mission_space,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # Manually define our observation_space as a single Box (the image).\n",
    "        # By default, MiniGrid's image shape is (view_size, view_size, 3) if using partial obs,\n",
    "        # or (height, width, 3) if using full-grid observation. We'll do full-grid here:\n",
    "        # We'll define (self.height, self.width, 3) as the shape.\n",
    "        # In practice, \"image\" shape can vary if partial observations are used.\n",
    "        self.observation_space = Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=(self.agent_view_size, self.agent_view_size, 3),\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "\n",
    "    \n",
    "    def _gen_grid(self, width, height):\n",
    "        \"\"\"\n",
    "        Generate the grid layout for a new episode using the DFS Maze Generation Algorithm.\n",
    "        \"\"\"\n",
    "        # Create an empty grid of the \"true\" width x height from config\n",
    "        self.grid = Grid(self.width, self.height)\n",
    "        # Surround the grid with walls\n",
    "        self.grid.wall_rect(0, 0, self.width, self.height)\n",
    "\n",
    "        # Initialize the maze as walls\n",
    "        maze = [[1 for _ in range(self.width)] for _ in range(self.height)]\n",
    "\n",
    "        # Define directions for DFS\n",
    "        directions = [(0, 2), (0, -2), (2, 0), (-2, 0)]\n",
    "\n",
    "        def is_valid(x, y):\n",
    "            \"\"\"Check if a cell is valid for carving.\"\"\"\n",
    "            return 0 < x < self.height - 1 and 0 < y < self.width - 1 and maze[x][y] == 1\n",
    "\n",
    "        def carve(x, y):\n",
    "            \"\"\"Carve passages in the maze using DFS.\"\"\"\n",
    "            maze[x][y] = 0  # Mark the cell as part of the maze\n",
    "            self.grid.set(x, y, None)  # Clear the wall in the grid\n",
    "            self.rng.shuffle(directions)\n",
    "            for dx, dy in directions:\n",
    "                nx, ny = x + dx, y + dy\n",
    "                if is_valid(nx, ny):\n",
    "                    # Remove the wall between cells\n",
    "                    maze[x + dx // 2][y + dy // 2] = 0\n",
    "                    self.grid.set(x + dx // 2, y + dy // 2, None)\n",
    "                    carve(nx, ny)\n",
    "\n",
    "        # Start carving from the top-left corner\n",
    "        carve(1, 1)\n",
    "\n",
    "        # Place the goal object in a random position not occupied by a wall\n",
    "        while True:\n",
    "            r = self.rng.integers(1, self.height - 1)\n",
    "            c = self.rng.integers(1, self.width - 1)\n",
    "            if self.grid.get(c, r) is None:\n",
    "                self.put_obj(Goal(), c, r)\n",
    "                break\n",
    "\n",
    "        # Place the agent in a random position not occupied by a wall and not on the goal\n",
    "        while True:\n",
    "            r = self.rng.integers(1, self.height - 1)\n",
    "            c = self.rng.integers(1, self.width - 1)\n",
    "            if self.grid.get(c, r) is None:\n",
    "                self.place_agent(top=(c, r), rand_dir=True)\n",
    "                break\n",
    "    \n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Override reset to ensure we only return the 'image' array\n",
    "        instead of a dict with 'image' and 'mission'.\n",
    "        \"\"\"\n",
    "        obs, info = super().reset(**kwargs)\n",
    "        obs = self._convert_obs(obs)\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Same for step: override to convert the dict observation into an image only.\n",
    "        \"\"\"\n",
    "        obs, reward, done, truncated, info = super().step(action)\n",
    "        obs = self._convert_obs(obs)\n",
    "        return obs, reward, done, truncated, info\n",
    "\n",
    "    def _convert_obs(self, original_obs):\n",
    "        \"\"\"\n",
    "        original_obs is typically {'image':..., 'mission':...}.\n",
    "        We'll just return original_obs['image'] to get a Box(low=0,high=255) shape.\n",
    "        \"\"\"\n",
    "        return original_obs[\"image\"]\n",
    "        #return np.transpose(original_obs[\"image\"], (2, 0, 1))\n",
    "\n",
    "\n",
    "\n",
    "def print_maze_from_config(config):\n",
    "    env = MyCustomMaze(config, render_mode='rgb_array')\n",
    "    env.reset()\n",
    "    full_level_image = env.render()  # This should return an RGB image of the full grid\n",
    "\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(full_level_image)\n",
    "    plt.title(\"Maze Configuration: \" + str(config))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# Generate a random maze and visualize it\n",
    "random_maze = random_config(6)\n",
    "print_maze_from_config(random_maze)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD CODE, NOT VECORIZED, REDUNDANT CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# ====================================================\n",
    "# 4. Main ACCEL Loop\n",
    "# ====================================================\n",
    "\n",
    "def main_accel_demo(total_iterations, replay_prob, train_steps, level_buffer_size,\n",
    "                    initial_fill_size, grid_size):\n",
    "    \n",
    "    \n",
    "    # Create a level buffer to store generated levels and their scores\n",
    "    level_buffer = LevelBuffer(max_size=level_buffer_size)\n",
    "    iteration_regrets = []\n",
    "        \n",
    "    #Create a dummy environment to initialize the model\n",
    "    dummy_env = MyCustomGrid(random_config(grid_size))\n",
    "    vectorized_env = create_vectorized_env(dummy_env, n_envs=4)\n",
    "\n",
    "    # Initialize student model with PPO\n",
    "    print(\"Initializing student model PPO...\")\n",
    "    student_model = initialize_ppo(dummy_env)\n",
    "\n",
    "    skipped = 0\n",
    "\n",
    "    # Populate buffer with initial levels\n",
    "    print(f\"Populating buffer with {initial_fill_size} initial levels with regret != 0...\")\n",
    "    for _ in range(initial_fill_size + skipped):\n",
    "        cfg = random_config(grid_size)\n",
    "        regret = calculate_regret_gae(MyCustomGrid(cfg), student_model, max_steps=1000, gamma=0.99, lam=0.95)\n",
    "        \n",
    "        # Skip levels with 0 regret\n",
    "        if regret == 0:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        level_buffer.add(cfg, regret)\n",
    "        \n",
    "    \n",
    "    print(\"\\nDone. Number of skipped levels with zero regret:\", skipped)\n",
    "    \n",
    "    skipped = 0\n",
    "    iteration = 0\n",
    "    # Main ACCEL loop\n",
    "    print(\"\\nMain ACCEL loop...\")\n",
    "    while iteration < total_iterations + skipped:\n",
    "        print(f\"\\n=== ITERATION {iteration + 1}/{total_iterations} SKIPPED {skipped} ===\")\n",
    "        \n",
    "        iteration += 1\n",
    "        \n",
    "        # Decide whether to use replay or generate a new level\n",
    "        use_replay = np.random.rand() < replay_prob\n",
    "        \n",
    "        # Generates new random levels if you don't use replay\n",
    "        if not use_replay or len(level_buffer.data) == 0:\n",
    "            cfg = random_config(grid_size)\n",
    "            regret = calculate_regret_gae(MyCustomGrid(cfg), student_model, max_steps=100, gamma=0.99, lam=0.95)\n",
    "            \n",
    "            if regret == 0:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            level_buffer.add(cfg, regret)\n",
    "            print(f\"  Sampled new config, regret={regret:.3f}\")\n",
    "        else:\n",
    "            # Replays an existing layer, edits it, and evaluates the new layer\n",
    "            old_cfg = level_buffer.sample_config()\n",
    "            env = MyCustomGrid(old_cfg)\n",
    "            \n",
    "            student_model.set_env(env)\n",
    "            student_model.learn(total_timesteps=train_steps)\n",
    "\n",
    "            new_cfg = edit_config(old_cfg)\n",
    "            regret = calculate_regret_gae(MyCustomGrid(new_cfg), student_model, max_steps=100, gamma=0.99, lam=0.95)\n",
    "            \n",
    "            if regret == 0:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            level_buffer.add(new_cfg, regret)\n",
    "            print(f\"  Replayed + mutated config, regret={regret:.3f}\")\n",
    "        \n",
    "        iteration_regrets.append(regret)\n",
    "    \n",
    "    print(\"\\nDone. Number of skipped levels with zero regret:\", skipped)\n",
    "\n",
    "    # Visualize progress of the regret over iterations.\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(iteration_regrets, marker='o')\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Regret\")\n",
    "    plt.title(\"Regret Progress during ACCEL Training\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    \n",
    "    config = {\n",
    "        \"grid_size\": 8,\n",
    "        \n",
    "        \"total_iterations\": 64,\n",
    "        \"train_steps\": 1024,\n",
    "\n",
    "        \"replay_prob\": 0.7,           # Probability of replaying a level and editing it vs. generating a new one\n",
    "        \"level_buffer_size\": 128,     # Maximum number of levels to store in the buffer\n",
    "        \"initial_fill_size\": 64,      # Number of levels to pre-fill the buffer with\n",
    "        \"regret_threshold\": 0.0,      # Minimum regret threshold to consider a level for the buffer\n",
    "        \n",
    "        \"n_envs\": 8,                  # Number of parallel environments to use for training\n",
    "        \n",
    "        \"edit_levels\": True,          # Whether to edit levels during training i.e. ACCEL or PLR\n",
    "        \"easy_start\": True            # Whether to fill the buffer with easy levels first i.e. minimum number of blocks\n",
    "    }\n",
    "    \n",
    "    print(\"Running ACCEL with config:\")\n",
    "    print(config, \"\\n\")\n",
    "    \n",
    "    main_accel(**config)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_regret_gae_parallel(env_config, model, max_steps=1000, gamma=0.99, lam=0.95, n_envs=4):\n",
    "    '''\n",
    "    Roll out n_envs copies of MyCustomGrid(env_config) in parallel,\n",
    "    compute GAE-based 'regret' for each environment, and return the max.\n",
    "    '''\n",
    "\n",
    "    # Create vectorized env with n_envs copies\n",
    "    vec_env = create_vectorized_env(env_config, n_envs=n_envs)\n",
    "    obs_array = vec_env.reset()  # shape: (n_envs, height, width, 3)\n",
    "\n",
    "    # For each environment, we will store transitions to later compute GAE\n",
    "    # We'll keep them in lists, one per environment.\n",
    "    # Alternatively, we can store them in big arrays (n_envs, max_steps), etc.\n",
    "    rewards_list = [[] for _ in range(n_envs)]\n",
    "    values_list = [[] for _ in range(n_envs)]\n",
    "    dones_list = [[] for _ in range(n_envs)]\n",
    "\n",
    "    for t in range(max_steps):\n",
    "        # Model’s predict can handle multiple obs in a single forward pass\n",
    "        actions, _ = model.predict(obs_array, deterministic=True)\n",
    "        \n",
    "        # Also compute the values in one batch\n",
    "        # Convert obs_array to torch tensor\n",
    "        obs_tensor = torch.as_tensor(np.transpose(obs_array, (0, 3, 1, 2))).float().to(device)\n",
    "        with torch.no_grad():\n",
    "            # shape: (n_envs, 1)\n",
    "            value_t = model.policy.predict_values(obs_tensor).cpu().numpy().flatten()\n",
    "\n",
    "        # Step all envs in parallel\n",
    "        next_obs_array, rewards, dones, truncs = vec_env.step(actions)\n",
    "\n",
    "        # Store the results\n",
    "        for i in range(n_envs):\n",
    "            rewards_list[i].append(rewards[i])\n",
    "            values_list[i].append(value_t[i])\n",
    "            dones_list[i].append(bool(dones[i]) or bool(truncs[i]))\n",
    "\n",
    "        obs_array = next_obs_array\n",
    "\n",
    "        # If all envs are done or truncated, we can break early\n",
    "        if all(dones) or all(truncs):\n",
    "            break\n",
    "\n",
    "    # We also need the terminal value for each env\n",
    "    # (0 if done, otherwise model's value at final obs)\n",
    "    obs_tensor = torch.as_tensor(np.transpose(obs_array, (0, 3, 1, 2))).float().to(device)\n",
    "    with torch.no_grad():\n",
    "        final_values = model.policy.predict_values(obs_tensor).cpu().numpy().flatten()\n",
    "\n",
    "    # Now, compute GAE-based \"regret\" for each of the n_envs\n",
    "    regrets = []\n",
    "    for i in range(n_envs):\n",
    "        # If the env ended with done or truncated, terminal value = 0\n",
    "        if dones_list[i][-1]:\n",
    "            values_list[i].append(0.0)\n",
    "        else:\n",
    "            values_list[i].append(final_values[i])\n",
    "\n",
    "        # Compute delta_t and approximate GAE-like metric\n",
    "        env_rewards = rewards_list[i]\n",
    "        env_values = values_list[i]\n",
    "        env_dones = dones_list[i]\n",
    "\n",
    "        env_regrets = []\n",
    "        for t in range(len(env_rewards)):\n",
    "            delta_t = env_rewards[t] + gamma * env_values[t + 1] * (1 - env_dones[t]) - env_values[t]\n",
    "            # accumulate discounted error\n",
    "            discounted_error = (gamma * lam) ** t * delta_t\n",
    "            env_regrets.append(max(0, discounted_error))\n",
    "\n",
    "        # The environment's \"regret\" is the max of its positive GAE deltas\n",
    "        regrets.append(max(env_regrets) if env_regrets else 0.0)\n",
    "\n",
    "    # Return the maximum regret across the parallel envs\n",
    "    return max(regrets) if regrets else 0.0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    '''def _gen_grid(self, width, height):\n",
    "        \"\"\"\n",
    "        Generate the grid layout for a new episode.\n",
    "        We use self.width and self.height from config, even though the underlying\n",
    "        MiniGrid environment might use grid_size for some of its operations.\n",
    "        \"\"\"    \n",
    "        \n",
    "        # Create an empty grid of the \"true\" width x height from config\n",
    "        self.grid = Grid(self.width, self.height)\n",
    "        # Surround the grid with walls\n",
    "        self.grid.wall_rect(0, 0, self.width, self.height)\n",
    "        \n",
    "        # Place random walls inside using the custom seed. Only place a wall if the cell is empty.\n",
    "        for _ in range(self.num_blocks):\n",
    "            r = self.rng.integers(1, self.height - 1)\n",
    "            c = self.rng.integers(1, self.width - 1)\n",
    "            if self.grid.get(c, r) is None: #and (c, r) != self.config[\"start_pos\"] and (c, r) != self.config[\"goal_pos\"]:\n",
    "                self.put_obj(Wall(), c, r)\n",
    "        \n",
    "        # Place the goal object in a random position not occupied by any wall\n",
    "        \"\"\"if self.config[\"goal_pos\"] is None:\"\"\"\n",
    "        while True:\n",
    "            r = self.rng.integers(1, self.height - 1)\n",
    "            c = self.rng.integers(1, self.width - 1)\n",
    "            if self.grid.get(c, r) is None: # and (c, r) != self.config[\"start_pos\"]:\n",
    "                self.put_obj(Goal(), c, r)\n",
    "                self.config[\"goal_pos\"] = (c, r)\n",
    "                break\n",
    "        \"\"\"elif self.config[\"goal_pos\"] is not None:\n",
    "            c, r = self.config[\"goal_pos\"]\n",
    "            self.put_obj(Goal(), c, r)\"\"\"\n",
    "\n",
    "        # Place the agent in a random position not occupied by any wall and not on the goal\n",
    "        \n",
    "        \"\"\"if self.config[\"start_pos\"] is None:\"\"\"\n",
    "        while True:\n",
    "            r = self.rng.integers(1, self.height - 1)\n",
    "            c = self.rng.integers(1, self.width - 1)\n",
    "            if self.grid.get(c, r) is None: # and (c, r) != self.config[\"goal_pos\"]:\n",
    "                self.place_agent(top=(c, r), rand_dir=True)\n",
    "                self.config[\"start_pos\"] = (c, r)\n",
    "                break  \n",
    "        \"\"\"elif self.config[\"start_pos\"] is not None:\n",
    "            c, r = self.config[\"start_pos\"]\n",
    "            self.place_agent(top=(c, r), rand_dir=True)\"\"\"'''\n",
    "            \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "accel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
