{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACCEL - Adversarially Compounding Complexity by Editing Levels\n",
    "\n",
    "### 1.1 Unsupervised Environment Design (UED)\n",
    "UED addresses the question: **How can we generate a curriculum of environments (levels) that maximally challenge an agent so that it generalizes well?**\n",
    "\n",
    "- Levels are parameterized environments (e.g., different maze layouts, obstacle configurations, etc.).\n",
    "- A teacher (level-generator) creates or selects levels that push the agent (the student).\n",
    "- A regret-based objective is often used to identify the levels that the agent finds hardest.\n",
    "\n",
    "### 1.2 Regret & Minimax Regret\n",
    "- Regret is the difference between the return the optimal policy *could achieve* on a given level vs. what the current agent’s policy achieves.\n",
    "- Minimax Regret means the agent’s policy minimizes the worst-case regret across all levels. In practice, this encourages the agent to be robust to the most difficult levels.\n",
    "\n",
    "### 1.3 PLR (Prioritized Level Replay)\n",
    "PLR is a simpler form of UED that:\n",
    "\n",
    "- Randomly samples levels from a design space.\n",
    "- Scores each level by an approximate regret measure (e.g., large “TD-error” or “value loss” = “hard” level).\n",
    "- Curates a buffer of the highest-scoring (hardest) levels.\n",
    "- Trains the agent only (or primarily) on those curated levels.\n",
    "\n",
    "\n",
    "### 1.4 ACCEL (Adversarially Compounding Complexity by Editing Levels) \n",
    "ACCEL extends PLR by **evolving**  previously discovered levels rather than always sampling new random ones. Specifically: \n",
    "1. **Start**  with a generator that can randomly produce levels.\n",
    " \n",
    "2. **Keep**  a replay buffer $\\Lambda$ of highest-regret levels (like PLR).\n",
    " \n",
    "3. **Replay**  from $\\Lambda$ (train on those levels).\n",
    " \n",
    "4. **Edit (mutate)**  these replayed levels—small changes that typically make them *more* challenging.\n",
    " \n",
    "5. **Re-evaluate**  mutated levels for regret. If they’re also high-regret, add them to $\\Lambda$.\n",
    " \n",
    "6. **Iterate**  this process, thus “compounding complexity” because each level can build on previously discovered “frontier” levels.\n",
    "\n",
    "**Outcome** : Over time, the environment buffer $\\Lambda$ accumulates an increasingly difficult and diverse set of levels, pushing the agent’s capabilities further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example usage of gymnasium with minigrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import minigrid\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"MiniGrid-Empty-5x5-v0\", render_mode=\"human\")\n",
    "\n",
    "# Reset the environment to start a new episode\n",
    "obs, info = env.reset()\n",
    "\n",
    "# Render the initial state of the environment\n",
    "env.render()\n",
    "\n",
    "# Take num_steps random actions in the environment\n",
    "for _ in range(50):\n",
    "    # Sample a random action\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # Step through the environment with the chosen action\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    # Render the environment after each action\n",
    "    env.render()\n",
    "    \n",
    "    # Check if the episode is done\n",
    "    if terminated or truncated:\n",
    "        print(\"Episode finished\")\n",
    "        break\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example usage of gymnasium with CartPole-v1 and PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to ./ppo_cartpole_tensorboard/PPO_5\n",
      "Eval num_timesteps=2000, episode_reward=9.60 +/- 0.80\n",
      "Episode length: 9.60 +/- 0.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.6      |\n",
      "|    mean_reward     | 9.6      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asus\\miniconda3\\envs\\accel\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=9.20 +/- 0.75\n",
      "Episode length: 9.20 +/- 0.75\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.2      |\n",
      "|    mean_reward     | 9.2      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=10.00 +/- 0.63\n",
      "Episode length: 10.00 +/- 0.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10       |\n",
      "|    mean_reward     | 10       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=9.80 +/- 0.98\n",
      "Episode length: 9.80 +/- 0.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.8      |\n",
      "|    mean_reward     | 9.8      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.4     |\n",
      "|    ep_rew_mean     | 21.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 2241     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# Create the Gymnasium environment\n",
    "env_id = \"CartPole-v1\"\n",
    "env = gym.make(env_id)\n",
    "\n",
    "# Optionally, you can create a vectorized environment for parallel training\n",
    "# This can speed up training by using multiple environments simultaneously\n",
    "# Here, we create 4 parallel environments\n",
    "vec_env = make_vec_env(env_id, n_envs=4)\n",
    "\n",
    "# Initialize the PPO agent\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",          # Multi-layer Perceptron policy\n",
    "    vec_env,              # Vectorized environment\n",
    "    verbose=1,            # Verbosity level (0: no output, 1: info)\n",
    "    tensorboard_log=\"./ppo_cartpole_tensorboard/\"  # Path for TensorBoard logs\n",
    ")\n",
    "\n",
    "# Set up an evaluation callback to monitor the agent's performance\n",
    "eval_env = gym.make(env_id)\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path='./logs/',\n",
    "    log_path='./logs/',\n",
    "    eval_freq=500,        # Evaluate the agent every 500 steps\n",
    "    n_eval_episodes=5,    # Number of episodes for evaluation\n",
    "    deterministic=True,\n",
    "    render=False\n",
    ")\n",
    "\n",
    "# Train the agent for a total of 100,000 steps, use the eval_callback every 500 steps\n",
    "model.learn(total_timesteps=100, callback=eval_callback)\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"ppo_cartpole\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = 283.0\n",
      "Episode 2: Total Reward = 995.0\n",
      "Episode 3: Total Reward = 397.0\n",
      "Episode 4: Total Reward = 423.0\n",
      "Episode 5: Total Reward = 601.0\n",
      "Episode 6: Total Reward = 124.0\n",
      "Episode 7: Total Reward = 278.0\n",
      "Episode 8: Total Reward = 1015.0\n",
      "Episode 9: Total Reward = 171.0\n",
      "Episode 10: Total Reward = 132.0\n"
     ]
    }
   ],
   "source": [
    "# Load the environment\n",
    "env_id = \"CartPole-v1\"\n",
    "env = gym.make(env_id, render_mode=\"human\")\n",
    "\n",
    "# To demonstrate loading the model, you can reload it as follows:\n",
    "model = PPO.load(\"ppo_cartpole\", env=vec_env)\n",
    "\n",
    "# Evaluate the trained agent\n",
    "episodes = 10\n",
    "for episode in range(1, episodes + 1):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        # Optionally, render the environment (requires a display)\n",
    "        # env.render()\n",
    "    print(f\"Episode {episode}: Total Reward = {total_reward}\")\n",
    "\n",
    "# Close the environments\n",
    "env.close()\n",
    "eval_env.close()\n",
    "vec_env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input**:  \n",
    "- Level buffer size $K$, initial fill ratio $\\rho$, level generator\n",
    "\n",
    "**Initialize**:  \n",
    "1. Initialize policy $\\pi(\\phi)$\n",
    "2. Initialize level buffer $\\Lambda$\n",
    "3. Sample $K \\cdot \\rho$ initial levels to populate $\\Lambda$\n",
    "\n",
    "**while** not converged **do**  \n",
    "   1. Sample replay decision $d \\sim P_D(d)$  \n",
    "   2. **if** $d = 0$ **then**  \n",
    "      - Sample level $\\theta$ from level generator  \n",
    "      - Collect $\\pi$'s trajectory $\\tau$ on $\\theta$, with stop-gradient $\\phi^\\perp$  \n",
    "      - Compute regret score $S$ for $\\theta$ (Equation 5)  \n",
    "      - Update $\\Lambda$ with $\\theta$ if score $S$ meets threshold  \n",
    "   3. **else**  \n",
    "      - Sample a replay level, $\\theta \\sim \\Lambda$  \n",
    "      - Collect policy trajectory $\\tau$ on $\\theta$  \n",
    "      - Update $\\pi$ with rewards $R(\\tau)$  \n",
    "      - Edit $\\theta$ to produce $\\theta'$  \n",
    "      - Collect $\\pi$'s trajectory $\\tau$ on $\\theta'$, with stop-gradient $\\phi^\\perp$  \n",
    "      - Compute regret score $S'$ for $\\theta'$  \n",
    "      - Update $\\Lambda$ with $\\theta$ or $\\theta'$ if score $S$ or $S'$ meets threshold  \n",
    "      - (Optionally) Update Editor using score $S$  \n",
    "   4. **end**  \n",
    "**end**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACCEL CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import minigrid\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "\n",
    "K   = 100   # Maximum number of levels to store in the replay buffer (levels to train on)\n",
    "pho = 0.5   # Initial fill rate of the replay buffer (initial ratio of random levels to store)\n",
    "p   = 0.5   # Probability of requesting a new level to be generated\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LevelGenerator:\n",
    "    def __init__(self, level_buffer):\n",
    "        # Fill the replay buffer with K levels generated with the initial fill rate\n",
    "        for _ in range(int(K * pho)):\n",
    "            self.generate(level_buffer)\n",
    "\n",
    "    def generate(self, level_buffer):\n",
    "        if len(level_buffer) < K:\n",
    "            level_buffer.append(gym.make(\"MiniGrid-LavaGapS5-v0\", render_mode=\"human\"))\n",
    "        else:\n",
    "            print(\"Replay buffer is full\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    levels = [] # Replay buffer to store the levels\n",
    "\n",
    "    # Initialize the level generator\n",
    "    level_generator = LevelGenerator(levels)\n",
    "\n",
    "    # Initialize the agent\n",
    "    model = PPO(\"MlpPolicy\", levels[0], verbose=1)\n",
    "\n",
    "    # Train the agent\n",
    "    for _ in range(1000):\n",
    "        # With probability p, generate a new level and add it to the replay buffer\n",
    "        if np.random.rand() < p:\n",
    "            level_generator.generate()\n",
    "        else:\n",
    "            # Sample a level from the replay buffer\n",
    "            level = np.random.choice(levels)\n",
    "            obs, reward, done, info = level.reset(), 0, False, {}\n",
    "            while not done:\n",
    "                action, _ = model.predict(obs, deterministic=True)\n",
    "                obs, reward, done, info = level.step(action)\n",
    "                level.render()\n",
    "            print(\"Episode finished\")\n",
    "\n",
    "\n",
    "    # Close all the environments in the replay buffer\n",
    "    for env in level_generator.levels:\n",
    "        env.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW ACCEL CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "\n",
      "=== ITERATION 1/30 ===\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.2     |\n",
      "|    ep_rew_mean     | 0.564    |\n",
      "| time/              |          |\n",
      "|    fps             | 785      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 128      |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 31.5          |\n",
      "|    ep_rew_mean          | 0.396         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 617           |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 0             |\n",
      "|    total_timesteps      | 256           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.5050325e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.95         |\n",
      "|    explained_variance   | 0.0587        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0183        |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | -0.00136      |\n",
      "|    value_loss           | 0.0295        |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 33.8         |\n",
      "|    ep_rew_mean          | 0.346        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 585          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 0            |\n",
      "|    total_timesteps      | 384          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.975334e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.95        |\n",
      "|    explained_variance   | -0.0116      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0161       |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.001       |\n",
      "|    value_loss           | 0.0213       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36.6         |\n",
      "|    ep_rew_mean          | 0.292        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 554          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 0            |\n",
      "|    total_timesteps      | 512          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.293529e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.95        |\n",
      "|    explained_variance   | 0.0265       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00473      |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.000712    |\n",
      "|    value_loss           | 0.016        |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 38.2          |\n",
      "|    ep_rew_mean          | 0.255         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 390           |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 1             |\n",
      "|    total_timesteps      | 640           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.5107991e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.95         |\n",
      "|    explained_variance   | -0.174        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.000458      |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -0.000385     |\n",
      "|    value_loss           | 0.00158       |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 40.1         |\n",
      "|    ep_rew_mean          | 0.215        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 395          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 1            |\n",
      "|    total_timesteps      | 768          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.056981e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.95        |\n",
      "|    explained_variance   | -5.18        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00119     |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.000722    |\n",
      "|    value_loss           | 0.000196     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 41            |\n",
      "|    ep_rew_mean          | 0.194         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 409           |\n",
      "|    iterations           | 7             |\n",
      "|    time_elapsed         | 2             |\n",
      "|    total_timesteps      | 896           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.6851113e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.95         |\n",
      "|    explained_variance   | -10.4         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -0.00183      |\n",
      "|    n_updates            | 60            |\n",
      "|    policy_gradient_loss | -0.000368     |\n",
      "|    value_loss           | 0.000367      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 41.5          |\n",
      "|    ep_rew_mean          | 0.187         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 418           |\n",
      "|    iterations           | 8             |\n",
      "|    time_elapsed         | 2             |\n",
      "|    total_timesteps      | 1024          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00016808184 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.94         |\n",
      "|    explained_variance   | -0.0898       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -0.00235      |\n",
      "|    n_updates            | 70            |\n",
      "|    policy_gradient_loss | -0.00213      |\n",
      "|    value_loss           | 0.000257      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 41.3          |\n",
      "|    ep_rew_mean          | 0.189         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 428           |\n",
      "|    iterations           | 9             |\n",
      "|    time_elapsed         | 2             |\n",
      "|    total_timesteps      | 1152          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.3258926e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.94         |\n",
      "|    explained_variance   | 0.202         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.000237      |\n",
      "|    n_updates            | 80            |\n",
      "|    policy_gradient_loss | -0.000147     |\n",
      "|    value_loss           | 0.00507       |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 42.2         |\n",
      "|    ep_rew_mean          | 0.17         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 435          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 1280         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006655478 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.94        |\n",
      "|    explained_variance   | 0.0343       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.000371    |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00481     |\n",
      "|    value_loss           | 0.0139       |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 41.2       |\n",
      "|    ep_rew_mean          | 0.191      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 440        |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 1408       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00146282 |\n",
      "|    clip_fraction        | 0          |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.94      |\n",
      "|    explained_variance   | 0.537      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.000348   |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.00345   |\n",
      "|    value_loss           | 0.000302   |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 39.8         |\n",
      "|    ep_rew_mean          | 0.221        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 445          |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 1536         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011955369 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.94        |\n",
      "|    explained_variance   | 0.0111       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.000348    |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00323     |\n",
      "|    value_loss           | 0.0216       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 40           |\n",
      "|    ep_rew_mean          | 0.216        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 450          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 1664         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011157519 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.93        |\n",
      "|    explained_variance   | -0.0129      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0104       |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00424     |\n",
      "|    value_loss           | 0.0316       |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 40.5          |\n",
      "|    ep_rew_mean          | 0.207         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 453           |\n",
      "|    iterations           | 14            |\n",
      "|    time_elapsed         | 3             |\n",
      "|    total_timesteps      | 1792          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00035140198 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.94         |\n",
      "|    explained_variance   | 0.274         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -0.00108      |\n",
      "|    n_updates            | 130           |\n",
      "|    policy_gradient_loss | -0.000983     |\n",
      "|    value_loss           | 0.000987      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 40.1          |\n",
      "|    ep_rew_mean          | 0.214         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 453           |\n",
      "|    iterations           | 15            |\n",
      "|    time_elapsed         | 4             |\n",
      "|    total_timesteps      | 1920          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00085161347 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.94         |\n",
      "|    explained_variance   | -0.803        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -0.0118       |\n",
      "|    n_updates            | 140           |\n",
      "|    policy_gradient_loss | -0.00409      |\n",
      "|    value_loss           | 0.000664      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 40.7          |\n",
      "|    ep_rew_mean          | 0.204         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 451           |\n",
      "|    iterations           | 16            |\n",
      "|    time_elapsed         | 4             |\n",
      "|    total_timesteps      | 2048          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00040184706 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.93         |\n",
      "|    explained_variance   | 0.0746        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.000201      |\n",
      "|    n_updates            | 150           |\n",
      "|    policy_gradient_loss | -0.00184      |\n",
      "|    value_loss           | 0.0119        |\n",
      "-------------------------------------------\n",
      "  Replayed + mutated old_cfg -> new_cfg, difficulty=1.000\n",
      "\n",
      "=== ITERATION 2/30 ===\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 38       |\n",
      "|    ep_rew_mean     | 0.283    |\n",
      "| time/              |          |\n",
      "|    fps             | 851      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 128      |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 42.8          |\n",
      "|    ep_rew_mean          | 0.17          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 608           |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 0             |\n",
      "|    total_timesteps      | 256           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00029978342 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.94         |\n",
      "|    explained_variance   | 0.00808       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -0.000961     |\n",
      "|    n_updates            | 170           |\n",
      "|    policy_gradient_loss | -0.00153      |\n",
      "|    value_loss           | 0.0178        |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 45.5          |\n",
      "|    ep_rew_mean          | 0.106         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 580           |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 0             |\n",
      "|    total_timesteps      | 384           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00017720181 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.94         |\n",
      "|    explained_variance   | -1.87         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.66e-05      |\n",
      "|    n_updates            | 180           |\n",
      "|    policy_gradient_loss | -0.000918     |\n",
      "|    value_loss           | 0.000288      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 35            |\n",
      "|    ep_rew_mean          | 0.32          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 551           |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 0             |\n",
      "|    total_timesteps      | 512           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00021703355 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.94         |\n",
      "|    explained_variance   | -1.1          |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -0.00138      |\n",
      "|    n_updates            | 190           |\n",
      "|    policy_gradient_loss | -0.00134      |\n",
      "|    value_loss           | 0.000311      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 33.9         |\n",
      "|    ep_rew_mean          | 0.339        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 545          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 1            |\n",
      "|    total_timesteps      | 640          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011143261 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.94        |\n",
      "|    explained_variance   | 0.0841       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0154       |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00552     |\n",
      "|    value_loss           | 0.0548       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 36.2        |\n",
      "|    ep_rew_mean          | 0.291       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 541         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 768         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005132849 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.93       |\n",
      "|    explained_variance   | 0.0725      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.013      |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0091     |\n",
      "|    value_loss           | 0.0165      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 37.4         |\n",
      "|    ep_rew_mean          | 0.265        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 539          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 1            |\n",
      "|    total_timesteps      | 896          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039541298 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.9         |\n",
      "|    explained_variance   | -1.13        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00695     |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00263     |\n",
      "|    value_loss           | 0.00102      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 37.5        |\n",
      "|    ep_rew_mean          | 0.266       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 534         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 1024        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001957452 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.88       |\n",
      "|    explained_variance   | -1.17       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00496    |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00396    |\n",
      "|    value_loss           | 0.00105     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 35.7         |\n",
      "|    ep_rew_mean          | 0.301        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 528          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 1152         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020708935 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.87        |\n",
      "|    explained_variance   | -0.0114      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00674     |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00462     |\n",
      "|    value_loss           | 0.0147       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 34.4        |\n",
      "|    ep_rew_mean          | 0.329       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 513         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 1280        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009850111 |\n",
      "|    clip_fraction        | 0.00234     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.87       |\n",
      "|    explained_variance   | 0.104       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0335     |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0142     |\n",
      "|    value_loss           | 0.0318      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 31.1        |\n",
      "|    ep_rew_mean          | 0.396       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 501         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 1408        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015372744 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.76       |\n",
      "|    explained_variance   | 0.16        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0304     |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.00947    |\n",
      "|    value_loss           | 0.011       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.7        |\n",
      "|    ep_rew_mean          | 0.403       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 488         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 1536        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005837903 |\n",
      "|    clip_fraction        | 0.00547     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.72       |\n",
      "|    explained_variance   | 0.131       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0146     |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.00657    |\n",
      "|    value_loss           | 0.0227      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.5       |\n",
      "|    ep_rew_mean          | 0.408      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 479        |\n",
      "|    iterations           | 13         |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 1664       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02469407 |\n",
      "|    clip_fraction        | 0.265      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.55      |\n",
      "|    explained_variance   | 0.0847     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0206    |\n",
      "|    n_updates            | 280        |\n",
      "|    policy_gradient_loss | -0.0141    |\n",
      "|    value_loss           | 0.00739    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.2        |\n",
      "|    ep_rew_mean          | 0.414       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 477         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 1792        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003082255 |\n",
      "|    clip_fraction        | 0.00781     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | -0.159      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0013      |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.00711    |\n",
      "|    value_loss           | 0.00732     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 30.4        |\n",
      "|    ep_rew_mean          | 0.412       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 474         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 1920        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013214917 |\n",
      "|    clip_fraction        | 0.057       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | 0.101       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0068     |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0132     |\n",
      "|    value_loss           | 0.016       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 30.5       |\n",
      "|    ep_rew_mean          | 0.41       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 16         |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2048       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00480168 |\n",
      "|    clip_fraction        | 0.00469    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.49      |\n",
      "|    explained_variance   | 0.0601     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00499   |\n",
      "|    n_updates            | 310        |\n",
      "|    policy_gradient_loss | -0.0036    |\n",
      "|    value_loss           | 0.00815    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 255\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. score=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, config=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 255\u001b[0m     \u001b[43mmain_accel_demo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 240\u001b[0m, in \u001b[0;36mmain_accel_demo\u001b[1;34m(total_iterations, replay_prob, train_steps)\u001b[0m\n\u001b[0;32m    238\u001b[0m env \u001b[38;5;241m=\u001b[39m MyCustomGrid(old_cfg)\n\u001b[0;32m    239\u001b[0m model\u001b[38;5;241m.\u001b[39mset_env(env)\n\u001b[1;32m--> 240\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m new_cfg \u001b[38;5;241m=\u001b[39m edit_config(old_cfg)\n\u001b[0;32m    243\u001b[0m diff_score \u001b[38;5;241m=\u001b[39m measure_difficulty(new_cfg, model)\n",
      "File \u001b[1;32mc:\\Users\\Emanuele\\miniconda3\\envs\\accel\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Emanuele\\miniconda3\\envs\\accel\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:336\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mep_info_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    334\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dump_logs(iteration)\n\u001b[1;32m--> 336\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Emanuele\\miniconda3\\envs\\accel\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:278\u001b[0m, in \u001b[0;36mPPO.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;66;03m# Clip grad norm\u001b[39;00m\n\u001b[0;32m    277\u001b[0m     th\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_grad_norm)\n\u001b[1;32m--> 278\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_updates \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n",
      "File \u001b[1;32mc:\\Users\\Emanuele\\miniconda3\\envs\\accel\\Lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m             )\n\u001b[1;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Emanuele\\miniconda3\\envs\\accel\\Lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\Emanuele\\miniconda3\\envs\\accel\\Lib\\site-packages\\torch\\optim\\adam.py:223\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    211\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    213\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    214\u001b[0m         group,\n\u001b[0;32m    215\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    220\u001b[0m         state_steps,\n\u001b[0;32m    221\u001b[0m     )\n\u001b[1;32m--> 223\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\Emanuele\\miniconda3\\envs\\accel\\Lib\\site-packages\\torch\\optim\\optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Emanuele\\miniconda3\\envs\\accel\\Lib\\site-packages\\torch\\optim\\adam.py:784\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    782\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 784\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Emanuele\\miniconda3\\envs\\accel\\Lib\\site-packages\\torch\\optim\\adam.py:379\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m    378\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mlerp_(grad, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[1;32m--> 379\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[0;32m    382\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TODO: The PPO seems that does not work if you change the grid size\n",
    "# of the environment, maybe you have to re-initialize the model with \n",
    "# the new environment but I am not sure, in this way do we have to \n",
    "# re-train the model from scratch?\n",
    "\n",
    "# TODO: Check if the score/regret is calculated correctly, maybe the\n",
    "# paper has the formula to calculate the score/regret?\n",
    "\n",
    "# TODO: I don't have idea on what are PPO, RL and my life choices, I\n",
    "# have to read about them to understand.\n",
    "\n",
    "\n",
    "# TODO: I think that we have to read better the paper to understand\n",
    "# the difference between teacher and student, and how the student\n",
    "# learns from the teacher, bla bla\n",
    "\n",
    "# SWAG\n",
    "\n",
    "# ====================================================\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "import gymnasium.spaces as spaces\n",
    "\n",
    "from minigrid.core.mission import MissionSpace\n",
    "from minigrid.core.world_object import Goal, Wall\n",
    "from minigrid.minigrid_env import MiniGridEnv, Grid\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# ====================================================\n",
    "# 1. Custom MiniGrid Environment that returns only the image\n",
    "#    for SB3's PPO (which expects a Box space).\n",
    "# ====================================================\n",
    "class MyCustomGrid(MiniGridEnv):\n",
    "    \"\"\"\n",
    "    Simple MiniGrid environment that places random wall tiles\n",
    "    according to a config dict, returning only the 'image' observation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config=None, **kwargs):\n",
    "        if config is None:\n",
    "            config = {}\n",
    "        self.config = config\n",
    "\n",
    "        # Extract config\n",
    "        self.width = config.get(\"width\", 8)\n",
    "        self.height = config.get(\"height\", 8)\n",
    "        self.num_blocks = config.get(\"num_blocks\", 5)\n",
    "        self.custom_seed = config.get(\"seed_val\", None)\n",
    "        self.agent_start = config.get(\"agent_start\", None)\n",
    "\n",
    "        # For older MiniGrid, we pass 'grid_size' not 'width'/'height' to the parent.\n",
    "        # We'll pick one dimension to define 'grid_size'; but note we manually place walls to match 'width' and 'height' in _gen_grid.\n",
    "        # For simplicity, let's just do: grid_size = max(width, height)\n",
    "        grid_size = max(self.width, self.height)\n",
    "\n",
    "        mission_space = MissionSpace(mission_func=lambda: \"get to the green goal square\")\n",
    "\n",
    "        super().__init__(\n",
    "            grid_size=grid_size,\n",
    "            max_steps=self.width * self.height * 2,\n",
    "            see_through_walls=False,\n",
    "            agent_view_size=5,\n",
    "            mission_space=mission_space,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # Manually define our observation_space as a single Box (the image).\n",
    "        # By default, MiniGrid's image shape is (view_size, view_size, 3) if using partial obs,\n",
    "        # or (height, width, 3) if using full-grid observation. We'll do full-grid here:\n",
    "        # We'll define (self.height, self.width, 3) as the shape.\n",
    "        # In practice, \"image\" shape can vary if partial observations are used.\n",
    "        self.observation_space = Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=(self.height, self.width, 3),\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "\n",
    "    def _gen_grid(self, width, height):\n",
    "        \"\"\"\n",
    "        Generate the grid layout for a new episode.\n",
    "        We'll use self.width, self.height from config\n",
    "        but the underlying minigrid might store its own grid_size.\n",
    "        \"\"\"\n",
    "        # Create an empty grid of the \"true\" width x height from config\n",
    "        self.grid = Grid(self.width, self.height)\n",
    "        # Surround with walls\n",
    "        self.grid.wall_rect(0, 0, self.width, self.height)\n",
    "\n",
    "        # Place random walls inside\n",
    "        for _ in range(self.num_blocks):\n",
    "            r = self._rand_int(1, self.height - 1)\n",
    "            c = self._rand_int(1, self.width - 1)\n",
    "            self.put_obj(Wall(), c, r)\n",
    "\n",
    "        # Place the agent\n",
    "        if self.agent_start is not None:\n",
    "            ax, ay = self.agent_start\n",
    "            self.place_agent(top=(ax, ay), size=(1, 1), rand_dir=False)\n",
    "        else:\n",
    "            self.place_agent()\n",
    "\n",
    "        # Place a goal object\n",
    "        self.place_obj(Goal())\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Override reset to ensure we only return the 'image' array\n",
    "        instead of a dict with 'image' and 'mission'.\n",
    "        \"\"\"\n",
    "        obs, info = super().reset(**kwargs)\n",
    "        obs = self._convert_obs(obs)\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Same for step: override to convert the dict observation into an image only.\n",
    "        \"\"\"\n",
    "        obs, reward, done, truncated, info = super().step(action)\n",
    "        obs = self._convert_obs(obs)\n",
    "        return obs, reward, done, truncated, info\n",
    "\n",
    "    def _convert_obs(self, original_obs):\n",
    "        \"\"\"\n",
    "        original_obs is typically {'image':..., 'mission':...}.\n",
    "        We'll just return original_obs['image'] to get a Box(low=0,high=255) shape.\n",
    "        \"\"\"\n",
    "        return original_obs[\"image\"]\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# 2. Simple “level buffer” \n",
    "# ====================================================\n",
    "class LevelBuffer:\n",
    "    def __init__(self, max_size=50):\n",
    "        self.max_size = max_size\n",
    "        self.data = []  # will store (config_dict, score)\n",
    "\n",
    "    def add(self, config, score):\n",
    "        self.data.append((config, score))\n",
    "        if len(self.data) > self.max_size:\n",
    "            self.data.sort(key=lambda x: x[1], reverse=True)\n",
    "            self.data = self.data[: self.max_size]\n",
    "\n",
    "    def sample_config(self):\n",
    "        if len(self.data) == 0:\n",
    "            return None\n",
    "        scores = [item[1] for item in self.data]\n",
    "        total = sum(scores)\n",
    "        if total <= 1e-9:\n",
    "            # fallback to uniform\n",
    "            idx = np.random.randint(len(self.data))\n",
    "            return self.data[idx][0]\n",
    "        probs = [s / total for s in scores]\n",
    "        idx = np.random.choice(len(self.data), p=probs)\n",
    "        return self.data[idx][0]\n",
    "\n",
    "\n",
    "# 3. Utility: generate random config\n",
    "def random_config():\n",
    "    return {\n",
    "        \"width\": 5, # np.random.randint(5, 10)\n",
    "        \"height\": 5, # np.random.randint(5, 10)\n",
    "        \"num_blocks\": np.random.randint(0, 15),\n",
    "        \"seed_val\": np.random.randint(0, 999999),\n",
    "        # \"agent_start\": (x, y)\n",
    "    }\n",
    "\n",
    "\n",
    "# 4. Utility: mutate\n",
    "def edit_config(old_config):\n",
    "    new_config = dict(old_config)\n",
    "    # randomly tweak width/height\n",
    "    #if np.random.rand() < 0.5:\n",
    "    #    new_config[\"width\"] = max(5, old_config[\"width\"] + np.random.choice([-1, 1]))\n",
    "    #else:\n",
    "    #    new_config[\"height\"] = max(5, old_config[\"height\"] + np.random.choice([-1, 1]))\n",
    "\n",
    "    # tweak num_blocks\n",
    "    new_config[\"num_blocks\"] = max(0, old_config[\"num_blocks\"] + np.random.choice([-2, -1, 1, 2]))\n",
    "\n",
    "    # optionally change seed\n",
    "    new_config[\"seed_val\"] = np.random.randint(0, 999999)\n",
    "    return new_config\n",
    "\n",
    "\n",
    "# 5. Evaluate difficulty or regret\n",
    "def measure_difficulty(config, model, max_steps=200):\n",
    "    \"\"\"\n",
    "    Here, we do one rollout, define difficulty as 1 - total_reward.\n",
    "    In real ACCEL, you'd do something like \"positive value loss.\"\n",
    "    \"\"\"\n",
    "    env = MyCustomGrid(config)\n",
    "    obs, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    for _ in range(max_steps):\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done or truncated:\n",
    "            break\n",
    "    difficulty = max(0, 1.0 - total_reward)\n",
    "    return difficulty\n",
    "\n",
    "\n",
    "def main_accel_demo(total_iterations=30, replay_prob=0.7, train_steps=2000):\n",
    "    \"\"\"\n",
    "    - Create a dummy environment (which yields a Box obs).\n",
    "    - Create a PPO model\n",
    "    - Create a buffer\n",
    "    - Repeatedly sample or replay + mutate\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Create a dummy environment to initialize PPO\n",
    "    dummy_env = MyCustomGrid(config={\"width\": 5, \"height\": 5, \"num_blocks\": 1}, render_mode=\"human\")\n",
    "    dummy_env.reset()\n",
    "    \n",
    "    # Render the environment as an image\n",
    "    dummy_env.render()\n",
    "\n",
    "    # 2) Initialize PPO with MlpPolicy\n",
    "    model = PPO(\"MlpPolicy\", dummy_env, verbose=1, n_steps=128, batch_size=64)\n",
    "\n",
    "    # 3) Create the level buffer & fill it\n",
    "    level_buffer = LevelBuffer(max_size=50)\n",
    "    for _ in range(10):\n",
    "        cfg = random_config()\n",
    "        level_buffer.add(cfg, score=1.0)\n",
    "\n",
    "    # 4) Outer loop\n",
    "    for iteration in range(total_iterations):\n",
    "        print(f\"\\n=== ITERATION {iteration+1}/{total_iterations} ===\")\n",
    "        use_replay = np.random.rand() < replay_prob\n",
    "\n",
    "        if (not use_replay) or (len(level_buffer.data) == 0):\n",
    "            # sample new config\n",
    "            cfg = random_config()\n",
    "            diff_score = measure_difficulty(cfg, model)\n",
    "            level_buffer.add(cfg, diff_score)\n",
    "            print(f\"  Sampled new config, difficulty={diff_score:.3f}\")\n",
    "        else:\n",
    "            # replay from buffer, short train, mutate\n",
    "            old_cfg = level_buffer.sample_config()\n",
    "            env = MyCustomGrid(old_cfg)\n",
    "            model.set_env(env)\n",
    "            model.learn(total_timesteps=train_steps)\n",
    "\n",
    "            new_cfg = edit_config(old_cfg)\n",
    "            diff_score = measure_difficulty(new_cfg, model)\n",
    "            level_buffer.add(new_cfg, diff_score)\n",
    "            print(f\"  Replayed + mutated old_cfg -> new_cfg, difficulty={diff_score:.3f}\")\n",
    "\n",
    "    print(\"\\nDone. Final buffer size:\", len(level_buffer.data))\n",
    "    print(\"Top-5 hardest levels (config, score):\")\n",
    "    level_buffer.data.sort(key=lambda x: x[1], reverse=True)\n",
    "    for i, (cfg, sc) in enumerate(level_buffer.data[:5]):\n",
    "        print(f\"{i+1}. score={sc:.3f}, config={cfg}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_accel_demo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "accel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
