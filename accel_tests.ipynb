{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bipedal Walker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evalute_models(load_dim=8, grid_size=12, n_eval_episodes=10, num_levels_per_difficulty=30)\n",
    "\n",
    "\"\"\"\n",
    "models = {\n",
    "    \"DR\": model_dr,\n",
    "    \"PLR\": model_plr,\n",
    "    \"ACCEL\": model_accel,\n",
    "    \"ACCEL-EasyStart\": model_accel_easy\n",
    "}\n",
    "    \n",
    "# Test the models on previously evaluated levels\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Testing model {model_name} on previously evaluated levels...\")\n",
    "    for i, level in enumerate(levels):\n",
    "        print(f\"Level {i + 1} - Complexity {(i+2)**2}:\")\n",
    "        for j, cfg in enumerate(level):\n",
    "            mean_reward = test_model(model, cfg)\n",
    "            print(f\"  Config {j + 1}: {mean_reward:.2f}\")\n",
    "        print()\n",
    "    print()\n",
    "\"\"\"\n",
    "\n",
    "# Reference: https://gymnasium.farama.org/environments/box2d/bipedal_walker/\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.utils import obs_as_tensor\n",
    "import random\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "class BipedalWalkerParamWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    A wrapper around BipedalWalker (or BipedalWalkerHardcore) \n",
    "    that sets custom parameters for terrain generation at reset time.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env_id=\"BipedalWalker-v3\", hardcore=False):\n",
    "        super().__init__(gym.make(env_id))\n",
    "        self.hardcore = hardcore\n",
    "\n",
    "        # The environment's internal parameters. \n",
    "        # We will override these at reset to control terrain generation.\n",
    "        self.config = {\n",
    "            \"seed_val\": None,\n",
    "            \"stump_height\": 0.0,\n",
    "            \"stair_height\": 0.0,\n",
    "            \"stair_steps\": 1,\n",
    "            \"roughness\": 0.0,\n",
    "            \"pit_gap\": 0.0,\n",
    "        }\n",
    "\n",
    "        # If we want hardcore version, you can do so with:\n",
    "        if self.hardcore:\n",
    "            self.env = gym.make(\"BipedalWalkerHardcore-v3\")\n",
    "\n",
    "    def set_config(self, config_dict):\n",
    "        \"\"\"Update environment parameters.\"\"\"\n",
    "        for k, v in config_dict.items():\n",
    "            self.config[k] = v\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Modify the environment's terrain parameters right before reset.\n",
    "        This monkey-patches internal Box2D variables, if needed,\n",
    "        or sets seeds to randomize terrain accordingly.\n",
    "        \"\"\"\n",
    "        # 1) Seed:\n",
    "        #if self.config[\"seed_val\"] is not None:\n",
    "        #    self.env.seed(self.config[\"seed_val\"])\n",
    "\n",
    "        # 2) Override terrain parameters in the underlying env\n",
    "        #    (We rely on the environment reading these at reset or having\n",
    "        #     references in the terrain generation code. Depending on \n",
    "        #     your exact BipedalWalker implementation, you might need \n",
    "        #     to modify the source or do partial overrides.)\n",
    "        self.env.unwrapped.stump_height = self.config[\"stump_height\"]\n",
    "        self.env.unwrapped.stair_height = self.config[\"stair_height\"]\n",
    "        self.env.unwrapped.stair_steps = int(self.config[\"stair_steps\"])\n",
    "        self.env.unwrapped.roughness = self.config[\"roughness\"]\n",
    "        self.env.unwrapped.pit_gap = self.config[\"pit_gap\"]\n",
    "\n",
    "        obs = self.env.reset(**kwargs)\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "    \n",
    "\n",
    "\n",
    "# The min and max values for each terrain parameter.\n",
    "# For simplicity, define them here, but you can store these in a table/dict.\n",
    "PARAM_BOUNDS = {\n",
    "    \"stump_height\":  (0.0, 5.0),\n",
    "    \"stair_height\":  (0.0, 5.0),\n",
    "    \"stair_steps\":   (1,   9),\n",
    "    \"roughness\":     (0.0, 10.0),\n",
    "    \"pit_gap\":       (0.0, 10.0)\n",
    "}\n",
    "\n",
    "def random_config(easy_init=False):\n",
    "    \"\"\"\n",
    "    Sample a random environment config. If easy_init=True,\n",
    "    you might restrict sampling to small values, so the agent\n",
    "    starts with simpler terrain.\n",
    "    \"\"\"\n",
    "    cfg = {}\n",
    "    \n",
    "    # Optionally set a random seed \n",
    "    cfg[\"seed_val\"] = random.randint(0, 1_000_000)\n",
    "\n",
    "    def sample_uniform(low, high):\n",
    "        return random.uniform(low, high)\n",
    "\n",
    "    if easy_init:\n",
    "        # Possibly narrower range for simpler (initial) levels\n",
    "        cfg[\"stump_height\"] = sample_uniform(0.0, 0.4)\n",
    "        cfg[\"stair_height\"] = sample_uniform(0.0, 0.4)\n",
    "        cfg[\"stair_steps\"]  = random.randint(1, 2)\n",
    "        cfg[\"roughness\"]    = sample_uniform(0.0, 0.6)\n",
    "        cfg[\"pit_gap\"]      = sample_uniform(0.0, 0.8)\n",
    "    else:\n",
    "        cfg[\"stump_height\"] = sample_uniform(*PARAM_BOUNDS[\"stump_height\"])\n",
    "        cfg[\"stair_height\"] = sample_uniform(*PARAM_BOUNDS[\"stair_height\"])\n",
    "        cfg[\"stair_steps\"]  = random.randint(*PARAM_BOUNDS[\"stair_steps\"])\n",
    "        cfg[\"roughness\"]    = sample_uniform(*PARAM_BOUNDS[\"roughness\"])\n",
    "        cfg[\"pit_gap\"]      = sample_uniform(*PARAM_BOUNDS[\"pit_gap\"])\n",
    "\n",
    "    return cfg\n",
    "\n",
    "def edit_config(old_cfg):\n",
    "    \"\"\"\n",
    "    Make a small 'mutation' to the old_cfg.\n",
    "    This can be random increments/decrements to each parameter, \n",
    "    or randomly choose one parameter to mutate.\n",
    "    \"\"\"\n",
    "    new_cfg = dict(old_cfg)\n",
    "    param_to_edit = random.choice([\"stump_height\", \"stair_height\", \"stair_steps\", \"roughness\", \"pit_gap\"])\n",
    "    # pick small delta \n",
    "    delta = random.uniform(0.1, 1.0)\n",
    "\n",
    "    # Add or subtract\n",
    "    sign = random.choice([-1, 1])\n",
    "    new_val = new_cfg[param_to_edit] + sign * delta\n",
    "    \n",
    "    # Clip to valid range\n",
    "    low, high = PARAM_BOUNDS[param_to_edit]\n",
    "    new_cfg[param_to_edit] = np.clip(new_val, low, high)\n",
    "\n",
    "    # Possibly update the seed too \n",
    "    new_cfg[\"seed_val\"] = random.randint(0, 1_000_000)\n",
    "    return new_cfg\n",
    "\n",
    "\n",
    "\n",
    "class LevelReplayBuffer:\n",
    "    \"\"\"\n",
    "    Stores (config, score). Score is e.g. a 'regret' approximation.\n",
    "    We keep the highest-score configs up to max_size.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_size=100):\n",
    "        self.max_size = max_size\n",
    "        self.data = []\n",
    "\n",
    "    def add(self, cfg, score):\n",
    "        self.data.append((cfg, score))\n",
    "        # keep only top K by score\n",
    "        self.data.sort(key=lambda x: x[1], reverse=True)\n",
    "        self.data = self.data[:self.max_size]\n",
    "\n",
    "    def sample(self):\n",
    "        if len(self.data) == 0:\n",
    "            return None\n",
    "        # Weighted sample by score or just pick the top \n",
    "        # For simplicity, pick randomly from top half:\n",
    "        half = len(self.data) // 2\n",
    "        idx = np.random.randint(0, max(1, half))\n",
    "        return self.data[idx][0]\n",
    "\n",
    "def estimate_regret(env, model, max_steps=1000, gamma=0.99, lam=0.95):\n",
    "    \"\"\"\n",
    "    Calculate regret using Generalized Advantage Estimation (GAE)\n",
    "    with Stable-Baselines3's PPO model.\n",
    "    \"\"\"\n",
    "    obs = env.reset()[0]\n",
    "    regrets = []\n",
    "    rewards = []\n",
    "    dones = []\n",
    "    values = []\n",
    "    \n",
    "\n",
    "    for t in range(max_steps):\n",
    "        # Add batch dimension to the observation tensor\n",
    "        obs_tensor = torch.as_tensor(obs).float().unsqueeze(0).to(device)\n",
    "        \n",
    "        # Use the model's policy to get the value and action.\n",
    "        # For actions, model.predict handles single observations well.\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        \n",
    "        # Compute the value from the policy.\n",
    "        value_t = model.policy.predict_values(obs_tensor).item()\n",
    "        values.append(value_t)\n",
    "        \n",
    "        # Perform the step in the environment\n",
    "        obs, reward, done, truncated, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "    # Add value of the terminal state (0 if done/truncated)\n",
    "    if done or truncated:\n",
    "        terminal_value = 0.0\n",
    "    else:\n",
    "        terminal_obs_tensor = torch.as_tensor(obs).float().unsqueeze(0).to(device)\n",
    "        terminal_value = model.policy.predict_values(terminal_obs_tensor).item()\n",
    "    values.append(terminal_value)\n",
    "\n",
    "    # Compute TD-errors and GAE-like regret score\n",
    "    for t in range(len(rewards)):\n",
    "        delta_t = rewards[t] + gamma * values[t + 1] * (1 - dones[t]) - values[t]\n",
    "        discounted_error = (gamma * lam) ** t * delta_t\n",
    "        regrets.append(max(0, discounted_error))\n",
    "\n",
    "    # Return the maximum positive regret score (or 0 if empty)\n",
    "    return max(regrets) if regrets else 0.0\n",
    "\n",
    "def run_accel_bipedal(\n",
    "    total_iterations=50,\n",
    "    steps_per_iteration=10000,\n",
    "    replay_prob=0.8,\n",
    "    regret_threshold=1.0,\n",
    "    max_buf_size=100,\n",
    "    easy_start=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Minimal ACCEL training loop on BipedalWalker.\n",
    "    \"\"\"\n",
    "    # 1) Create the environment wrapper + vectorize\n",
    "    def make_env():\n",
    "        env = BipedalWalkerParamWrapper(env_id=\"BipedalWalkerHardcore-v3\", hardcore=True)\n",
    "        return env\n",
    "    \n",
    "    # (Alternatively, do SubprocVecEnv if you want parallel CPU rollouts.)\n",
    "    vec_env = DummyVecEnv([make_env])\n",
    "\n",
    "    # 2) Initialize PPO\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\", \n",
    "        vec_env, \n",
    "        n_steps=2048,        # must be multiple of vec_env.num_envs\n",
    "        batch_size=64,\n",
    "        learning_rate=3e-4,\n",
    "        gamma=0.99,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    # 3) Create LevelReplayBuffer\n",
    "    level_buffer = LevelReplayBuffer(max_size=max_buf_size)\n",
    "\n",
    "    # 4) [Optional] Pre-fill buffer with some easy or random levels\n",
    "    for _ in range(10):  # pre-fill 10 levels\n",
    "        print(\"Pre-filling buffer...\")\n",
    "        cfg = random_config(easy_init=easy_start)\n",
    "        env = make_env()  # fresh environment\n",
    "        env.set_config(cfg)\n",
    "        # Train on it briefly to get a partial updated policy \n",
    "        model.set_env(DummyVecEnv([lambda: env]))\n",
    "        model.learn(total_timesteps=2000)\n",
    "\n",
    "        # Evaluate regret\n",
    "        rew_env = make_env()\n",
    "        rew_env.set_config(cfg)\n",
    "        regret = estimate_regret(rew_env, model)\n",
    "\n",
    "        if regret >= regret_threshold:\n",
    "            level_buffer.add(cfg, regret)\n",
    "        \n",
    "    \n",
    "    print(\"Buffer pre-filled. Starting main loop...\")\n",
    "\n",
    "    # 5) ACCEL main loop\n",
    "    for it in range(total_iterations):\n",
    "        # Decide: replay from buffer or create new\n",
    "        use_replay = (np.random.rand() < replay_prob) and (len(level_buffer.data) > 0)\n",
    "        if use_replay:\n",
    "            cfg = level_buffer.sample()\n",
    "            # Optionally edit the config to keep pushing frontier\n",
    "            cfg = edit_config(cfg)\n",
    "        else:\n",
    "            cfg = random_config(easy_init=False)\n",
    "\n",
    "        # Train on that config\n",
    "        env = make_env()\n",
    "        env.set_config(cfg)\n",
    "        model.set_env(DummyVecEnv([lambda: env]))\n",
    "        model.learn(total_timesteps=steps_per_iteration)\n",
    "\n",
    "        # Now measure regret\n",
    "        rew_env = make_env()\n",
    "        rew_env.set_config(cfg)\n",
    "        regret = estimate_regret(rew_env, model)\n",
    "\n",
    "        if regret >= regret_threshold:\n",
    "            level_buffer.add(cfg, regret)\n",
    "\n",
    "        print(f\"[Iteration {it+1}/{total_iterations}] => Config = {cfg}, Regret = {regret:.3f}, \"\n",
    "              f\"Buffer size = {len(level_buffer.data)}\")\n",
    "\n",
    "    return model, level_buffer\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trained_model, buffer_data = run_accel_bipedal(\n",
    "        total_iterations=250,\n",
    "        steps_per_iteration=1000,\n",
    "        replay_prob=0.8,\n",
    "        regret_threshold=1.0,\n",
    "        max_buf_size=100,\n",
    "        easy_start=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Evaluate final performance on some reference environment\n",
    "    test_env = gym.make(\"BipedalWalkerHardcore-v3\", render_mode='human')\n",
    "    mean_return = 0.0\n",
    "    N = 10\n",
    "    for _ in range(N):\n",
    "        obs = test_env.reset()[0]\n",
    "        done = False\n",
    "        episode_reward = 0.0\n",
    "        while not done:\n",
    "            action, _ = trained_model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, truncated, info = test_env.step(action)\n",
    "            episode_reward += reward\n",
    "            if truncated:\n",
    "                done = True\n",
    "        mean_return += episode_reward\n",
    "    mean_return /= N\n",
    "    print(f\"Avg Return on BipedalWalkerHardcore-v3 over {N} trials: {mean_return}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maze generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomMaze(MiniGridEnv):\n",
    "    \"\"\"\n",
    "    Simple MiniGrid environment that places random wall tiles\n",
    "    according to a config dict, returning only the 'image' observation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config=None, **kwargs):\n",
    "        if config is None:\n",
    "            config = {}\n",
    "        self.config = config\n",
    "\n",
    "        # Extract parameters from config\n",
    "        self.width = config.get(\"width\")\n",
    "        self.height = config.get(\"height\")\n",
    "        self.num_blocks = config.get(\"num_blocks\")\n",
    "        self.custom_seed = config.get(\"seed_val\")\n",
    "        \n",
    "        \n",
    "        # Create a random number generator with the custom seed\n",
    "        self.rng = np.random.default_rng(seed=self.custom_seed)\n",
    "\n",
    "        grid_size = max(self.width, self.height)\n",
    "\n",
    "        mission_space = MissionSpace(mission_func=lambda: \"get to the green goal square\")\n",
    "\n",
    "        super().__init__(\n",
    "            grid_size=grid_size,\n",
    "            max_steps=self.width * self.height * 2, # max_steps is typically 2x the grid size\n",
    "            see_through_walls=False,\n",
    "            agent_view_size=5,                      # Size of the agent's view square\n",
    "            mission_space=mission_space,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # Manually define our observation_space as a single Box (the image).\n",
    "        # By default, MiniGrid's image shape is (view_size, view_size, 3) if using partial obs,\n",
    "        # or (height, width, 3) if using full-grid observation. We'll do full-grid here:\n",
    "        # We'll define (self.height, self.width, 3) as the shape.\n",
    "        # In practice, \"image\" shape can vary if partial observations are used.\n",
    "        self.observation_space = Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=(self.agent_view_size, self.agent_view_size, 3),\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "\n",
    "    \n",
    "    def _gen_grid(self, width, height):\n",
    "        \"\"\"\n",
    "        Generate the grid layout for a new episode using the DFS Maze Generation Algorithm.\n",
    "        \"\"\"\n",
    "        # Create an empty grid of the \"true\" width x height from config\n",
    "        self.grid = Grid(self.width, self.height)\n",
    "        # Surround the grid with walls\n",
    "        self.grid.wall_rect(0, 0, self.width, self.height)\n",
    "\n",
    "        # Initialize the maze as walls\n",
    "        maze = [[1 for _ in range(self.width)] for _ in range(self.height)]\n",
    "\n",
    "        # Define directions for DFS\n",
    "        directions = [(0, 2), (0, -2), (2, 0), (-2, 0)]\n",
    "\n",
    "        def is_valid(x, y):\n",
    "            \"\"\"Check if a cell is valid for carving.\"\"\"\n",
    "            return 0 < x < self.height - 1 and 0 < y < self.width - 1 and maze[x][y] == 1\n",
    "\n",
    "        def carve(x, y):\n",
    "            \"\"\"Carve passages in the maze using DFS.\"\"\"\n",
    "            maze[x][y] = 0  # Mark the cell as part of the maze\n",
    "            self.grid.set(x, y, None)  # Clear the wall in the grid\n",
    "            self.rng.shuffle(directions)\n",
    "            for dx, dy in directions:\n",
    "                nx, ny = x + dx, y + dy\n",
    "                if is_valid(nx, ny):\n",
    "                    # Remove the wall between cells\n",
    "                    maze[x + dx // 2][y + dy // 2] = 0\n",
    "                    self.grid.set(x + dx // 2, y + dy // 2, None)\n",
    "                    carve(nx, ny)\n",
    "\n",
    "        # Start carving from the top-left corner\n",
    "        carve(1, 1)\n",
    "\n",
    "        # Place the goal object in a random position not occupied by a wall\n",
    "        while True:\n",
    "            r = self.rng.integers(1, self.height - 1)\n",
    "            c = self.rng.integers(1, self.width - 1)\n",
    "            if self.grid.get(c, r) is None:\n",
    "                self.put_obj(Goal(), c, r)\n",
    "                break\n",
    "\n",
    "        # Place the agent in a random position not occupied by a wall and not on the goal\n",
    "        while True:\n",
    "            r = self.rng.integers(1, self.height - 1)\n",
    "            c = self.rng.integers(1, self.width - 1)\n",
    "            if self.grid.get(c, r) is None:\n",
    "                self.place_agent(top=(c, r), rand_dir=True)\n",
    "                break\n",
    "    \n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Override reset to ensure we only return the 'image' array\n",
    "        instead of a dict with 'image' and 'mission'.\n",
    "        \"\"\"\n",
    "        obs, info = super().reset(**kwargs)\n",
    "        obs = self._convert_obs(obs)\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Same for step: override to convert the dict observation into an image only.\n",
    "        \"\"\"\n",
    "        obs, reward, done, truncated, info = super().step(action)\n",
    "        obs = self._convert_obs(obs)\n",
    "        return obs, reward, done, truncated, info\n",
    "\n",
    "    def _convert_obs(self, original_obs):\n",
    "        \"\"\"\n",
    "        original_obs is typically {'image':..., 'mission':...}.\n",
    "        We'll just return original_obs['image'] to get a Box(low=0,high=255) shape.\n",
    "        \"\"\"\n",
    "        return original_obs[\"image\"]\n",
    "        #return np.transpose(original_obs[\"image\"], (2, 0, 1))\n",
    "\n",
    "\n",
    "\n",
    "def print_maze_from_config(config):\n",
    "    env = MyCustomMaze(config, render_mode='rgb_array')\n",
    "    env.reset()\n",
    "    full_level_image = env.render()  # This should return an RGB image of the full grid\n",
    "\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(full_level_image)\n",
    "    plt.title(\"Maze Configuration: \" + str(config))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# Generate a random maze and visualize it\n",
    "random_maze = random_config(6)\n",
    "print_maze_from_config(random_maze)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD CODE, NOT VECORIZED, REDUNDANT CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "# ====================================================\n",
    "# 4. Main ACCEL Loop\n",
    "# ====================================================\n",
    "\n",
    "def main_accel_demo(total_iterations, replay_prob, train_steps, level_buffer_size,\n",
    "                    initial_fill_size, grid_size):\n",
    "    \n",
    "    \n",
    "    # Create a level buffer to store generated levels and their scores\n",
    "    level_buffer = LevelBuffer(max_size=level_buffer_size)\n",
    "    iteration_regrets = []\n",
    "        \n",
    "    #Create a dummy environment to initialize the model\n",
    "    dummy_env = MyCustomGrid(random_config(grid_size))\n",
    "    vectorized_env = create_vectorized_env(dummy_env, n_envs=4)\n",
    "\n",
    "    # Initialize student model with PPO\n",
    "    print(\"Initializing student model PPO...\")\n",
    "    student_model = initialize_ppo(dummy_env)\n",
    "\n",
    "    skipped = 0\n",
    "\n",
    "    # Populate buffer with initial levels\n",
    "    print(f\"Populating buffer with {initial_fill_size} initial levels with regret != 0...\")\n",
    "    for _ in range(initial_fill_size + skipped):\n",
    "        cfg = random_config(grid_size)\n",
    "        regret = calculate_regret_gae(MyCustomGrid(cfg), student_model, max_steps=1000, gamma=0.99, lam=0.95)\n",
    "        \n",
    "        # Skip levels with 0 regret\n",
    "        if regret == 0:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        level_buffer.add(cfg, regret)\n",
    "        \n",
    "    \n",
    "    print(\"\\nDone. Number of skipped levels with zero regret:\", skipped)\n",
    "    \n",
    "    skipped = 0\n",
    "    iteration = 0\n",
    "    # Main ACCEL loop\n",
    "    print(\"\\nMain ACCEL loop...\")\n",
    "    while iteration < total_iterations + skipped:\n",
    "        print(f\"\\n=== ITERATION {iteration + 1}/{total_iterations} SKIPPED {skipped} ===\")\n",
    "        \n",
    "        iteration += 1\n",
    "        \n",
    "        # Decide whether to use replay or generate a new level\n",
    "        use_replay = np.random.rand() < replay_prob\n",
    "        \n",
    "        # Generates new random levels if you don't use replay\n",
    "        if not use_replay or len(level_buffer.data) == 0:\n",
    "            cfg = random_config(grid_size)\n",
    "            regret = calculate_regret_gae(MyCustomGrid(cfg), student_model, max_steps=100, gamma=0.99, lam=0.95)\n",
    "            \n",
    "            if regret == 0:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            level_buffer.add(cfg, regret)\n",
    "            print(f\"  Sampled new config, regret={regret:.3f}\")\n",
    "        else:\n",
    "            # Replays an existing layer, edits it, and evaluates the new layer\n",
    "            old_cfg = level_buffer.sample_config()\n",
    "            env = MyCustomGrid(old_cfg)\n",
    "            \n",
    "            student_model.set_env(env)\n",
    "            student_model.learn(total_timesteps=train_steps)\n",
    "\n",
    "            new_cfg = edit_config(old_cfg)\n",
    "            regret = calculate_regret_gae(MyCustomGrid(new_cfg), student_model, max_steps=100, gamma=0.99, lam=0.95)\n",
    "            \n",
    "            if regret == 0:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            level_buffer.add(new_cfg, regret)\n",
    "            print(f\"  Replayed + mutated config, regret={regret:.3f}\")\n",
    "        \n",
    "        iteration_regrets.append(regret)\n",
    "    \n",
    "    print(\"\\nDone. Number of skipped levels with zero regret:\", skipped)\n",
    "\n",
    "    # Visualize progress of the regret over iterations.\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(iteration_regrets, marker='o')\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Regret\")\n",
    "    plt.title(\"Regret Progress during ACCEL Training\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    \n",
    "    config = {\n",
    "        \"grid_size\": 8,\n",
    "        \n",
    "        \"total_iterations\": 64,\n",
    "        \"train_steps\": 1024,\n",
    "\n",
    "        \"replay_prob\": 0.7,           # Probability of replaying a level and editing it vs. generating a new one\n",
    "        \"level_buffer_size\": 128,     # Maximum number of levels to store in the buffer\n",
    "        \"initial_fill_size\": 64,      # Number of levels to pre-fill the buffer with\n",
    "        \"regret_threshold\": 0.0,      # Minimum regret threshold to consider a level for the buffer\n",
    "        \n",
    "        \"n_envs\": 8,                  # Number of parallel environments to use for training\n",
    "        \n",
    "        \"edit_levels\": True,          # Whether to edit levels during training i.e. ACCEL or PLR\n",
    "        \"easy_start\": True            # Whether to fill the buffer with easy levels first i.e. minimum number of blocks\n",
    "    }\n",
    "    \n",
    "    print(\"Running ACCEL with config:\")\n",
    "    print(config, \"\\n\")\n",
    "    \n",
    "    main_accel(**config)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_regret_gae_parallel(env_config, model, max_steps=1000, gamma=0.99, lam=0.95, n_envs=4):\n",
    "    '''\n",
    "    Roll out n_envs copies of MyCustomGrid(env_config) in parallel,\n",
    "    compute GAE-based 'regret' for each environment, and return the max.\n",
    "    '''\n",
    "\n",
    "    # Create vectorized env with n_envs copies\n",
    "    vec_env = create_vectorized_env(env_config, n_envs=n_envs)\n",
    "    obs_array = vec_env.reset()  # shape: (n_envs, height, width, 3)\n",
    "\n",
    "    # For each environment, we will store transitions to later compute GAE\n",
    "    # We'll keep them in lists, one per environment.\n",
    "    # Alternatively, we can store them in big arrays (n_envs, max_steps), etc.\n",
    "    rewards_list = [[] for _ in range(n_envs)]\n",
    "    values_list = [[] for _ in range(n_envs)]\n",
    "    dones_list = [[] for _ in range(n_envs)]\n",
    "\n",
    "    for t in range(max_steps):\n",
    "        # Model’s predict can handle multiple obs in a single forward pass\n",
    "        actions, _ = model.predict(obs_array, deterministic=True)\n",
    "        \n",
    "        # Also compute the values in one batch\n",
    "        # Convert obs_array to torch tensor\n",
    "        obs_tensor = torch.as_tensor(np.transpose(obs_array, (0, 3, 1, 2))).float().to(device)\n",
    "        with torch.no_grad():\n",
    "            # shape: (n_envs, 1)\n",
    "            value_t = model.policy.predict_values(obs_tensor).cpu().numpy().flatten()\n",
    "\n",
    "        # Step all envs in parallel\n",
    "        next_obs_array, rewards, dones, truncs = vec_env.step(actions)\n",
    "\n",
    "        # Store the results\n",
    "        for i in range(n_envs):\n",
    "            rewards_list[i].append(rewards[i])\n",
    "            values_list[i].append(value_t[i])\n",
    "            dones_list[i].append(bool(dones[i]) or bool(truncs[i]))\n",
    "\n",
    "        obs_array = next_obs_array\n",
    "\n",
    "        # If all envs are done or truncated, we can break early\n",
    "        if all(dones) or all(truncs):\n",
    "            break\n",
    "\n",
    "    # We also need the terminal value for each env\n",
    "    # (0 if done, otherwise model's value at final obs)\n",
    "    obs_tensor = torch.as_tensor(np.transpose(obs_array, (0, 3, 1, 2))).float().to(device)\n",
    "    with torch.no_grad():\n",
    "        final_values = model.policy.predict_values(obs_tensor).cpu().numpy().flatten()\n",
    "\n",
    "    # Now, compute GAE-based \"regret\" for each of the n_envs\n",
    "    regrets = []\n",
    "    for i in range(n_envs):\n",
    "        # If the env ended with done or truncated, terminal value = 0\n",
    "        if dones_list[i][-1]:\n",
    "            values_list[i].append(0.0)\n",
    "        else:\n",
    "            values_list[i].append(final_values[i])\n",
    "\n",
    "        # Compute delta_t and approximate GAE-like metric\n",
    "        env_rewards = rewards_list[i]\n",
    "        env_values = values_list[i]\n",
    "        env_dones = dones_list[i]\n",
    "\n",
    "        env_regrets = []\n",
    "        for t in range(len(env_rewards)):\n",
    "            delta_t = env_rewards[t] + gamma * env_values[t + 1] * (1 - env_dones[t]) - env_values[t]\n",
    "            # accumulate discounted error\n",
    "            discounted_error = (gamma * lam) ** t * delta_t\n",
    "            env_regrets.append(max(0, discounted_error))\n",
    "\n",
    "        # The environment's \"regret\" is the max of its positive GAE deltas\n",
    "        regrets.append(max(env_regrets) if env_regrets else 0.0)\n",
    "\n",
    "    # Return the maximum regret across the parallel envs\n",
    "    return max(regrets) if regrets else 0.0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    def _gen_grid(self, width, height):\n",
    "        '''\n",
    "        Generate the grid layout for a new episode.\n",
    "        We use self.width and self.height from config, even though the underlying\n",
    "        MiniGrid environment might use grid_size for some of its operations.\n",
    "        '''    \n",
    "        \n",
    "        # Create an empty grid of the \"true\" width x height from config\n",
    "        self.grid = Grid(self.width, self.height)\n",
    "        # Surround the grid with walls\n",
    "        self.grid.wall_rect(0, 0, self.width, self.height)\n",
    "        \n",
    "        # Place random walls inside using the custom seed. Only place a wall if the cell is empty.\n",
    "        for _ in range(self.num_blocks):\n",
    "            r = self.rng.integers(1, self.height - 1)\n",
    "            c = self.rng.integers(1, self.width - 1)\n",
    "            if self.grid.get(c, r) is None: #and (c, r) != self.config[\"start_pos\"] and (c, r) != self.config[\"goal_pos\"]:\n",
    "                self.put_obj(Wall(), c, r)\n",
    "        \n",
    "        # Place the goal object in a random position not occupied by any wall\n",
    "        \"\"\"if self.config[\"goal_pos\"] is None:\"\"\"\n",
    "        while True:\n",
    "            r = self.rng.integers(1, self.height - 1)\n",
    "            c = self.rng.integers(1, self.width - 1)\n",
    "            if self.grid.get(c, r) is None: # and (c, r) != self.config[\"start_pos\"]:\n",
    "                self.put_obj(Goal(), c, r)\n",
    "                self.config[\"goal_pos\"] = (c, r)\n",
    "                break\n",
    "        \"\"\"elif self.config[\"goal_pos\"] is not None:\n",
    "            c, r = self.config[\"goal_pos\"]\n",
    "            self.put_obj(Goal(), c, r)\"\"\"\n",
    "\n",
    "        # Place the agent in a random position not occupied by any wall and not on the goal\n",
    "        \n",
    "        \"\"\"if self.config[\"start_pos\"] is None:\"\"\"\n",
    "        while True:\n",
    "            r = self.rng.integers(1, self.height - 1)\n",
    "            c = self.rng.integers(1, self.width - 1)\n",
    "            if self.grid.get(c, r) is None: # and (c, r) != self.config[\"goal_pos\"]:\n",
    "                self.place_agent(top=(c, r), rand_dir=True)\n",
    "                self.config[\"start_pos\"] = (c, r)\n",
    "                break  \n",
    "        \"\"\"elif self.config[\"start_pos\"] is not None:\n",
    "            c, r = self.config[\"start_pos\"]\n",
    "            self.place_agent(top=(c, r), rand_dir=True)\"\"\"'''\n",
    "            \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_accel(total_iterations, replay_prob, train_steps, level_buffer_size,\n",
    "               initial_fill_size, grid_size, n_envs, edit_levels, regret_threshold,\n",
    "               easy_start, domain_randomization, name):\n",
    "    \n",
    "    # Initialize Weights and Biases\n",
    "    wandb.init(project=\"accel\", config=config, mode=\"disabled\")\n",
    "    \n",
    "    # Create a level buffer, a personal class to store levels and scores\n",
    "    level_buffer = LevelBuffer(max_size=level_buffer_size)\n",
    "    \n",
    "    # Generate a random configuration {width, height, num_blocks, start_pos, goal_pos}\n",
    "    dummy_config = random_config(grid_size)\n",
    "    \n",
    "    # Create a vectorized environment, so a wrapper for MyCustomGrid that allows interconnection \n",
    "    # between gymnasium and stable-baselines3 to train the model in a vectorized way, since we\n",
    "    # are using DummyVecEnv, it is not true parallelism\n",
    "    vectorized_env = create_vectorized_env(dummy_config, n_envs=n_envs)\n",
    "\n",
    "    # Initialize PPO with vectorized environment\n",
    "    print(\"Initializing student model PPO...\")\n",
    "    student_model = initialize_ppo(vectorized_env)\n",
    "            \n",
    "        \n",
    "\n",
    "    # ====================================================\n",
    "    # Initial buffer fill\n",
    "    # ====================================================\n",
    "    if not domain_randomization:\n",
    "        print(f\"Populating buffer with {initial_fill_size} initial levels with regret > {regret_threshold}...\")\n",
    "        while len(level_buffer.data) < initial_fill_size:\n",
    "            \n",
    "            if easy_start:\n",
    "                cfg = random_config(grid_size, num_blocks=2)\n",
    "            else:\n",
    "                cfg = random_config(grid_size)\n",
    "            \n",
    "            #for monitor in vectorized_env.envs:\n",
    "            #    monitor.env.update_config(cfg)\n",
    "            \n",
    "            vectorized_env.env_method(\"update_config\", cfg)\n",
    "            \n",
    "            student_model.learn(total_timesteps=100)\n",
    "            \n",
    "            regret = calculate_regret_gae(MyCustomGrid(cfg), student_model, max_steps=1000, gamma=0.99, lam=0.95)\n",
    "\n",
    "            # Skip levels with low regret\n",
    "            if regret < regret_threshold: continue\n",
    "\n",
    "            level_buffer.add(cfg, regret)\n",
    "\n",
    "    # ====================================================\n",
    "    # Main ACCEL loop\n",
    "    # ====================================================\n",
    "    \n",
    "    iteration_regrets = []\n",
    "    iteration, skipped = 0, 0\n",
    "    \n",
    "    print(\"\\nMain training loop...\")\n",
    "    while iteration < total_iterations + skipped:\n",
    "        \n",
    "        print(f\"\\n=== ITERATION {iteration + 1}/{total_iterations + skipped} SKIPPED: {skipped} ===\")\n",
    "        \n",
    "        iteration += 1\n",
    "        \n",
    "        if domain_randomization:\n",
    "            cfg = random_config(grid_size)\n",
    "            vectorized_env.env_method(\"update_config\", cfg)\n",
    "            student_model.learn(total_timesteps=train_steps)\n",
    "            regret = calculate_regret_gae(MyCustomGrid(cfg), student_model, max_steps=1000, gamma=0.99, lam=0.95)\n",
    "            print(f\"Regret for current level: {regret}, buffer size: {len(level_buffer.data)}\")\n",
    "            iteration_regrets.append(regret)\n",
    "            \n",
    "            # if regret is below threshold, skip\n",
    "            if regret <= regret_threshold:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        # Decide whether to replay or generate a new level\n",
    "        use_replay = np.random.rand() < replay_prob\n",
    "\n",
    "        if not use_replay or len(level_buffer.data) == 0:\n",
    "            # Create a new random level\n",
    "            cfg = random_config(grid_size)\n",
    "            print(\"Generated new random level:\", cfg)\n",
    "        else:\n",
    "            # Sample a level from the buffer\n",
    "            cfg = level_buffer.sample_config()\n",
    "            print(\"Sampled level from buffer:\", cfg)\n",
    "            \n",
    "        # Update the vectorized environment with the selected config and train the model\n",
    "        #for monitor in vectorized_env.envs:\n",
    "        #    monitor.env.update_config(cfg)\n",
    "        \n",
    "        vectorized_env.env_method(\"update_config\", cfg)\n",
    "        \n",
    "        student_model.learn(total_timesteps=train_steps)\n",
    "        \n",
    "        wandb.log({\n",
    "            \"iteration\": iteration,\n",
    "            \"regret_score\": regret,\n",
    "            \"regret_threshold\": regret_threshold,\n",
    "            \"buffer_size\": len(level_buffer.data),\n",
    "            \"value_loss\": student_model.logger.name_to_value[\"train/value_loss\"],\n",
    "            \"entropy_loss\": student_model.logger.name_to_value[\"train/entropy_loss\"],\n",
    "            \"policy_loss\": student_model.logger.name_to_value[\"train/policy_loss\"],\n",
    "        })\n",
    "\n",
    "        if use_replay and edit_levels:\n",
    "            # Edit the level and calculate regret\n",
    "            cfg = edit_config(cfg)\n",
    "            print(\"Edited level to:\", cfg)\n",
    "\n",
    "        regret = calculate_regret_gae(MyCustomGrid(cfg), student_model, max_steps=1000, gamma=0.99, lam=0.95)\n",
    "        \n",
    "        if regret <= regret_threshold:\n",
    "            print(f\"Regret for current level is {regret:.5f} <= threshold {regret_threshold:.5f}. Skipping...\")\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        print(f\"Regret for current level: {regret}, buffer size: {len(level_buffer.data)}\")\n",
    "        level_buffer.add(cfg, regret)\n",
    "        iteration_regrets.append(regret)\n",
    "        \n",
    "        # Increase the regret threshold slightly\n",
    "        regret_threshold += 0.0001\n",
    "        \n",
    "    \n",
    "    # Plot and display the progress\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(iteration_regrets, marker='o')\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Regret\")\n",
    "    plt.title(\"Regret Progress during Training\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig(f\"regret_progress_{name}_{grid_size}x{grid_size}.png\")\n",
    "    \n",
    "    \n",
    "    print(\"\\nDone. Final buffer size:\", len(level_buffer.data))\n",
    "    print(\"Top-5 hardest levels (config, regret):\")\n",
    "    level_buffer.data.sort(key=lambda x: x[1], reverse=True)\n",
    "    for i, (cfg, sc) in enumerate(level_buffer.data[:5]):\n",
    "        print(f\"{i + 1}. regret={sc:.5f}, config={cfg}\")\n",
    "        #print_level_from_config(cfg)\n",
    "        \n",
    "    print(\"Top-5 easiest levels (config, regret):\")\n",
    "    level_buffer.data.sort(key=lambda x: x[1])\n",
    "    for i, (cfg, sc) in enumerate(level_buffer.data[:5]):\n",
    "        print(f\"{i + 1}. regret={sc:.5f}, config={cfg}\")\n",
    "        #print_level_from_config(cfg)\n",
    "    \n",
    "    wandb.finish()\n",
    "    \n",
    "    # Save the model\n",
    "    student_model.save(f\"models/{name}\")\n",
    "\n",
    "    return student_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ====================================================\n",
    "# 3. Utility Functions\n",
    "# ====================================================\n",
    "\n",
    "# Calculate regret using Generalized Advantage Estimation (GAE) with Stable-Baselines3's PPO model.\n",
    "# PLR approximates regret using a score function such as the positive value loss.\n",
    "def calculate_regret_gae(env, model, max_steps, gamma, lam):\n",
    "    \"\"\"\n",
    "    Calculate regret using Generalized Advantage Estimation (GAE)\n",
    "    with Stable-Baselines3's PPO model.\n",
    "    \"\"\"\n",
    "    obs, _ = env.reset()\n",
    "    regrets = []\n",
    "    rewards = []\n",
    "    dones = []\n",
    "    values = []\n",
    "\n",
    "    for t in range(max_steps):\n",
    "        # Add batch dimension to the observation tensor\n",
    "        obs_tensor = torch.as_tensor(obs).float().unsqueeze(0).to(device)\n",
    "        \n",
    "        # Use the model's policy to get the value and action.\n",
    "        # For actions, model.predict handles single observations well.\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        \n",
    "        # Compute the value from the policy.\n",
    "        value_t = model.policy.predict_values(obs_tensor).item()\n",
    "        values.append(value_t)\n",
    "        \n",
    "        # Perform the step in the environment\n",
    "        obs, reward, done, truncated, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "    # Add value of the terminal state (0 if done/truncated)\n",
    "    if done or truncated:\n",
    "        terminal_value = 0.0\n",
    "    else:\n",
    "        terminal_obs_tensor = torch.as_tensor(obs).float().unsqueeze(0).to(device)\n",
    "        terminal_value = model.policy.predict_values(terminal_obs_tensor).item()\n",
    "    values.append(terminal_value)\n",
    "\n",
    "    # Compute TD-errors and GAE-like regret score\n",
    "    for t in range(len(rewards)):\n",
    "        delta_t = rewards[t] + gamma * values[t + 1] * (1 - dones[t]) - values[t]\n",
    "        discounted_error = (gamma * lam) ** t * delta_t\n",
    "        regrets.append(max(0, discounted_error))\n",
    "\n",
    "    # Return the maximum positive regret score (or 0 if empty)\n",
    "    return max(regrets) if regrets else 0.0\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "def edit_config(old_config, difficulty_level=1):\n",
    "\n",
    "    width, height = old_config[\"width\"], old_config[\"height\"]\n",
    "    total_cells = width * height\n",
    "\n",
    "    # Define a baseline max number of blocks\n",
    "    max_blocks = int(0.6 * total_cells)  # Ensure we don't overcrowd (max 60% coverage)\n",
    "    \n",
    "    # Calculate the new number of blocks using a logarithmic scale\n",
    "    base_growth = int(np.log2(total_cells) * difficulty_level)\n",
    "    \n",
    "    # Introduce some randomness while keeping it within a reasonable range\n",
    "    growth_factor = np.random.randint(base_growth // 2, base_growth + 1)\n",
    "    \n",
    "    # Compute the new block count\n",
    "    new_number_blocks = old_config[\"num_blocks\"] + growth_factor\n",
    "    \n",
    "    # Ensure it's within the allowed range\n",
    "    new_config = dict(old_config)\n",
    "    new_config[\"num_blocks\"] = max(1, min(new_number_blocks, max_blocks))  \n",
    "    \n",
    "    # Mark as edited\n",
    "    new_config[\"edited\"] = True\n",
    "\n",
    "    return new_config\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
